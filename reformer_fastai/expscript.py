# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10c_experiment.synthetic-script.ipynb (unless otherwise specified).

__all__ = ['bs', 'sl', 'n_epochs', 'train_sz', 'valid_sz', 'n_hashes', 'bucket_size', 'vocab_sz', 'd_model', 'n_layers',
           'n_heads', 'max_seq_len', 'causal', 'use_lsh', 'get_dataloaders', 'get_model', 'get_learner',
           'exp_run_synthetic']

# Cell
from fastcore.all import *
from fastai.basics import *

from .reformer import LSHLM
from .data import TwinSequence, MaskTargCallback
from .metrics import MaskedAccuracy

# Cell

# Dataloaders
bs=32
sl=64
n_epochs=1
train_sz=500
valid_sz=100

# Model
n_hashes=4
bucket_size=32
vocab_sz=128
d_model=256
n_layers=1
n_heads=4
max_seq_len=64
causal=True
use_lsh=True

# Cell
def get_dataloaders(bs:int=32, sl:int=64, n_epochs:int=1, train_sz:int=500, valid_sz:int=100):
    dls = DataLoaders.from_dsets(TwinSequence(sl, train_sz), TwinSequence(sl, valid_sz), bs=bs, shuffle=False, device='cuda')
    return dls

# Cell
def get_model(vocab_sz:int=128, d_model:int=256, n_layers:int=1, n_heads:int=4,
              max_seq_len:int=64, bucket_size:int=32, n_hashes:int=4, causal:bool=True, use_lsh:bool=True):
    model = LSHLM(vocab_sz=vocab_sz, d_model=d_model, n_layers=n_layers, n_heads=n_heads, max_seq_len=max_seq_len,
              bucket_size=bucket_size, n_hashes=n_hashes, causal=True, use_lsh=True)
    return model

# Cell
def get_learner(dls, model):
    learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(ignore_index=-100),
                metrics=MaskedAccuracy(), cbs=[MaskTargCallback()])
    return learn

# Cell
@call_parse
def exp_run_synthetic(bs:Param(help="Batch size", type=int, default=bs),
         sl:Param(help="Seqlence length", type=int, default=sl),
         n_epochs:Param(help="Number of epochs", type=int, default=n_epochs),
         train_sz:Param(help="TwinSequence train size", type=int, default=train_sz),
         valid_sz:Param(help="TwinSequence valid size", type=int, default=valid_sz),
         n_hashes:Param(help="Number of LSH Attention hashes", type=int, default=n_hashes),
         bucket_size:Param(help="LSH Attention bucket size", type=int, default=bucket_size),
         vocab_sz:Param(help="Vocab size", type=int, default=vocab_sz),
         d_model:Param(help="Model dimension", type=int, default=d_model),
         n_layers:Param(help="Number of model layers", type=int, default=n_layers),
         n_heads:Param(help="Number of attention heads", type=int, default=n_heads),
         max_seq_len:Param(help="Max sequence length for model embedding", type=int, default=max_seq_len),
         causal:Param(help="Use causal masking", type=int, default=causal),
         use_lsh:Param(help="Use LSH Attention", type=int, default=use_lsh),
        ):
    print("I'm working!")
    print(f'bs : {bs}')

    dls = get_dataloaders(bs=bs, sl=sl, n_epochs=n_epochs, train_sz=train_sz, valid_sz=valid_sz)

    model = get_model(vocab_sz=vocab_sz, d_model=d_model, n_layers=n_layers, n_heads=n_heads,
              max_seq_len=max_seq_len, bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, use_lsh=use_lsh)

    learn = get_learner(dls, model)

#     learn.fit_one_cycle(n_epochs, lr)