# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/21_experiment-configs.ipynb (unless otherwise specified).

__all__ = ['ConfigBase', 'SyntheticConfig', 'TransformerLMConfigEnwik8', 'ReversibleLMConfigEnwik8']

# Cell
from fastcore.all import *
from fastai.basics import *

from .all import *

import json

# Cell
def _dummy(): return

# Cell
class ConfigBase:
    "Base class for Configs"
    _d:dict = None
    _model = _dummy

    @delegates(_model)
    def __init__(self, verbouse=False, warn=True, **kwargs):
        self.validate()
        for k,v in kwargs.items():
            if k in self._d:
                self._d[k]=v
                if verbouse: print(f'Setting {k} = {v}')
            elif warn: print(f'Parameter {k} is not accepted by LSHLM. Skipped')

    def validate(self):
        assert exists(self._d), "_d missing. You might want to provide defaults for config"
        assert self._model is not _dummy, "_model missing. Provide a model class"

    def __repr__(self):
        s = f"{self._model.__name__} config \n" + '-'*20
        s += ''.join(f'\n{k:16}{v}' for k,v in self._d.items())
        return s

    def dict(self): return self._d

    def save(self, fn, add_tstmp=False):
        if add_tstmp:
            tstmp = time.strftime("_%d_%m_%Y_%H:%M", time.gmtime())
            fn += tstmp
        with open(f'{fn}.json', 'w') as f:
            json.dump(self.dict(), f)

    @classmethod
    def from_file(cls, fn):
        with open(f'{fn}.json') as f:
            d = json.load(f)
        return cls(d)


# Cell
class SyntheticConfig(ConfigBase):
    """
    Config for Synthetic Experiment.
    See https://arampacha.github.io/reformer_fastai/experiment.synthetic-task.html for details
    """
    _model = LSHLM
    _d = {
        'vocab_sz':128,
        'd_model':256,
        'n_layers':1,
        'n_heads':4,
        'd_ff':256,
        'attn_dropout':0.0,
        'ff_dropout':0.0,
        'emb_dropout':0.0,
        'tie_weights':True,
        'causal':True,
        'pos_enc':'absolute',
        'max_seq_len':1024,
        'axial_shape':None,
        'axial_emb_dims':None,
        'pad_idx':None,
        'prenorm':False,
        'attn_bias':False,
        'bucket_size':64,
        'use_lsh':True,
        'n_hashes':4,
        'seed':123,
    }


# Cell
class TransformerLMConfigEnwik8(ConfigBase):
    """
    Config for enwik8 Experiment.
    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-baseline.html for details
    """
    _model = TransformerLM
    _d = {
        'vocab_sz':256,
        'd_model':1024,
        'n_layers':3,
        'n_heads':8,
        'd_ff':4096,
        'attn_dropout':0.1,
        'ff_dropout':0.1,
        'emb_dropout':0.1,
        'tie_weights':True,
        'causal':True,
        'pos_enc':'axial',
        'max_seq_len':2048,
        'axial_shape':(64,32),
        'axial_emb_dims':None,
        'pad_idx':None,
        'prenorm':False,
        'attn_bias':False,
        'shared_qk':False,
    }


# Cell
class ReversibleLMConfigEnwik8(ConfigBase):
    """
    Config for enwik8 Experiment.
    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-reversible.html for details
    """
    _model = TransformerLM
    _d = {
        'vocab_sz':256,
        'd_model':1024,
        'n_layers':3,
        'n_heads':8,
        'd_ff':4096,
        'attn_dropout':0.1,
        'ff_dropout':0.1,
        'emb_dropout':0.1,
        'tie_weights':True,
        'causal':True,
        'pos_enc':'axial',
        'max_seq_len':2048,
        'axial_shape':(64,32),
        'axial_emb_dims':None,
        'pad_idx':None,
        'prenorm':False,
        'attn_bias':False,
        'rev_thres':0,
    }
