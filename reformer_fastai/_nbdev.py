# AUTOGENERATED BY NBDEV! DO NOT EDIT!

__all__ = ["index", "modules", "custom_doc_links", "git_url"]

index = {"exists": "00_core.ipynb",
         "default": "00_core.ipynb",
         "expand_dim1": "00_core.ipynb",
         "max_neg_value": "00_core.ipynb",
         "top_p_filter": "00_core.ipynb",
         "top_k_filter": "00_core.ipynb",
         "cache_method_decorator": "00_core.ipynb",
         "look_one_back": "00_core.ipynb",
         "chunked_sum": "00_core.ipynb",
         "sort_key_val": "00_core.ipynb",
         "batched_index_select": "00_core.ipynb",
         "Residual": "00_core.ipynb",
         "PostNorm": "00_core.ipynb",
         "PreNorm": "00_core.ipynb",
         "FeedForward": "00_core.ipynb",
         "AbsolutePositionalEmbedding": "00_core.ipynb",
         "FixedPositionalEmbedding": "00_core.ipynb",
         "TransformerEmbedding": "00_core.ipynb",
         "do_cuda_timing": "00_core.ipynb",
         "model_performance": "00_core.ipynb",
         "total_params": "00_core.ipynb",
         "MASK_VAL": "01_attention.ipynb",
         "SELF_ATTN_MASK_VAL": "01_attention.ipynb",
         "AttnInProj": "01_attention.ipynb",
         "ScaledDotProdAttention": "01_attention.ipynb",
         "Attention": "01_attention.ipynb",
         "AdditiveInProj": "01_attention.ipynb",
         "AdditiveAttention": "01_attention.ipynb",
         "LSHAttention": "01_attention.ipynb",
         "ReformerAttention": "01_attention.ipynb",
         "TransformerEncoderBlock": "02_transformer.ipynb",
         "TransformerEncoder": "02_transformer.ipynb",
         "TransformerDecoderBlock": "02_transformer.ipynb",
         "TransformerDecoderBlockV2": "02_transformer.ipynb",
         "TransformerDecoder": "02_transformer.ipynb",
         "TransformerLM": "02_transformer.ipynb",
         "TransformerEncDec": "02_transformer.ipynb",
         "Chunk": "03_reformer.ipynb",
         "ChunkedFeedForward": "03_reformer.ipynb",
         "Deterministic": "03_reformer.ipynb",
         "ReversibleBlock": "03_reformer.ipynb",
         "IrreversibleBlock": "03_reformer.ipynb",
         "ReversibleSequence": "03_reformer.ipynb",
         "ReformerEncoder": "03_reformer.ipynb",
         "ReformerDecoder": "03_reformer.ipynb",
         "ReformerLM": "03_reformer.ipynb",
         "ReformerEncDec": "03_reformer.ipynb"}

modules = ["core.py",
           "attention.py",
           "transformer.py",
           "reformer.py"]

doc_url = "https://arampacha.github.io/reformer_fastai/"

git_url = "https://github.com/arampacha/reformer_fastai/tree/master/"

def custom_doc_links(name): return None
