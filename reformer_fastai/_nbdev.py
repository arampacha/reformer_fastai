# AUTOGENERATED BY NBDEV! DO NOT EDIT!

__all__ = ["index", "modules", "custom_doc_links", "git_url"]

index = {"exists": "00_core.ipynb",
         "default": "00_core.ipynb",
         "expand_dim1": "00_core.ipynb",
         "max_neg_value": "00_core.ipynb",
         "setattr_on": "00_core.ipynb",
         "top_p_filter": "00_core.ipynb",
         "top_k_filter": "00_core.ipynb",
         "cache_method_decorator": "00_core.ipynb",
         "look_one_back": "00_core.ipynb",
         "chunked_sum": "00_core.ipynb",
         "sort_key_val": "00_core.ipynb",
         "batched_index_select": "00_core.ipynb",
         "do_cuda_timing": "00_core.ipynb",
         "model_performance": "00_core.ipynb",
         "total_params": "00_core.ipynb",
         "CombineInputOutputCallback": "00_core.ipynb",
         "RemoveEOSCallback": "00_core.ipynb",
         "LossTargetShiftCallback": "00_core.ipynb",
         "PadBatchCallback": "00_core.ipynb",
         "AddEOSID": "00_core.ipynb",
         "LabelSmoothingCrossEntropy": "00_core.ipynb",
         "LabelSmoothingCrossEntropyFlat": "00_core.ipynb",
         "Learner.distrib_ctx": "00_core.ipynb",
         "Residual": "01_layers.ipynb",
         "PostNorm": "01_layers.ipynb",
         "PreNorm": "01_layers.ipynb",
         "FeedForward": "01_layers.ipynb",
         "get_axial_shape": "01_layers.ipynb",
         "get_axial_dims": "01_layers.ipynb",
         "AbsolutePositionalEmbedding": "01_layers.ipynb",
         "FixedPositionalEmbedding": "01_layers.ipynb",
         "TransformerEmbedding": "01_layers.ipynb",
         "MASK_VAL": "02_attention.ipynb",
         "SELF_ATTN_MASK_VAL": "02_attention.ipynb",
         "AttnInProj": "02_attention.ipynb",
         "AttnInProjV2": "02_attention.ipynb",
         "SharedQKAttnInProj": "02_attention.ipynb",
         "ScaledDotProdAttention": "02_attention.ipynb",
         "Attention": "02_attention.ipynb",
         "MemEfficientAttention": "02_attention.ipynb",
         "ChunkedDotProdAttention": "02_attention.ipynb",
         "ChunkedAttention": "02_attention.ipynb",
         "AdditiveInProj": "02_attention.ipynb",
         "AdditiveAttention": "02_attention.ipynb",
         "LSHAttention": "02_attention.ipynb",
         "LSHSelfAttention": "02_attention.ipynb",
         "ReformerAttention": "02_attention.ipynb",
         "ReformerAttentionV2": "02_attention.ipynb",
         "LMMixin": "03_transformer.ipynb",
         "EncDecMixin": "03_transformer.ipynb",
         "TransformerEncoderBlock": "03_transformer.ipynb",
         "TransformerEncoder": "03_transformer.ipynb",
         "TransformerDecoderBlock": "03_transformer.ipynb",
         "TransformerDecoderBlockV2": "03_transformer.ipynb",
         "TransformerDecoder": "03_transformer.ipynb",
         "TransformerLM": "03_transformer.ipynb",
         "transformer_lm_splits": "03_transformer.ipynb",
         "Transformer": "03_transformer.ipynb",
         "transformer_splits": "03_transformer.ipynb",
         "LowMemEncoderBlock": "03_transformer.ipynb",
         "LowMemEncoder": "03_transformer.ipynb",
         "ChunkedTransformerLM": "03_transformer.ipynb",
         "from_config": "04_reformer.ipynb",
         "MODELS": "04_reformer.ipynb",
         "Chunk": "04_reformer.ipynb",
         "ChunkedFeedForward": "04_reformer.ipynb",
         "Deterministic": "04_reformer.ipynb",
         "ReversibleBlock": "04_reformer.ipynb",
         "IrreversibleBlock": "04_reformer.ipynb",
         "ReversibleSequence": "04_reformer.ipynb",
         "ReversibleEncoder": "04_reformer.ipynb",
         "ReversibleDecoder": "04_reformer.ipynb",
         "ReversibleLM": "04_reformer.ipynb",
         "ReversibleTransformer": "04_reformer.ipynb",
         "LSHEncoderBlock": "04_reformer.ipynb",
         "LSHEncoder": "04_reformer.ipynb",
         "LSHLM": "04_reformer.ipynb",
         "ReformerEncoder": "04_reformer.ipynb",
         "ReformerLM": "04_reformer.ipynb",
         "reformer_lm_splits": "04_reformer.ipynb",
         "ByteTextTokenizer": "05_tokenizers.ipynb",
         "SubwordTextEncoder": "05_tokenizers.ipynb",
         "read_lines": "06_data.ipynb",
         "convert_data_to_seq_length": "06_data.ipynb",
         "read_and_prepare_data": "06_data.ipynb",
         "TwinSequence": "06_data.ipynb",
         "MaskTargCallback": "06_data.ipynb",
         "DeterministicTwinSequence": "06_data.ipynb",
         "MaskedAccuracy": "07_metrics.ipynb",
         "BPC": "07_metrics.ipynb",
         "bpc": "07_metrics.ipynb",
         "Adafactor": "08_optimizers.ipynb",
         "adafactor": "08_optimizers.ipynb",
         "Learner.gather_args": "09_tracking.ipynb",
         "download_enwik8_data": "20_experiment-script.ipynb",
         "download_wmt14_data": "20_experiment-script.ipynb",
         "get_twin_sequence_dataloaders": "20_experiment-script.ipynb",
         "get_enwik8_dataloader": "20_experiment-script.ipynb",
         "get_wmt14_dataloader": "20_experiment-script.ipynb",
         "get_synthetic_learner": "20_experiment-script.ipynb",
         "get_lm_learner": "20_experiment-script.ipynb",
         "get_reformerlm_learner": "20_experiment-script.ipynb",
         "get_seq2seq_learner": "20_experiment-script.ipynb",
         "init_wandb": "20_experiment-script.ipynb",
         "run_exp": "20_experiment-script.ipynb",
         "update_sig": "21_experiment-configs.ipynb",
         "ConfigBase": "21_experiment-configs.ipynb",
         "SyntheticConfig": "21_experiment-configs.ipynb",
         "TransformerLMConfigEnwik8": "21_experiment-configs.ipynb",
         "ReversibleLMConfigEnwik8": "21_experiment-configs.ipynb",
         "NHashesConfig": "21_experiment-configs.ipynb",
         "NLayersConfig": "21_experiment-configs.ipynb",
         "ReversibleTransformerConfigWMT": "21_experiment-configs.ipynb",
         "TransformerConfigWMT": "21_experiment-configs.ipynb"}

modules = ["core.py",
           "layers.py",
           "attention.py",
           "transformer.py",
           "reformer.py",
           "tokenizers.py",
           "data.py",
           "metrics.py",
           "optimizers.py",
           "tracking.py",
           "expscript.py",
           "configs.py"]

doc_url = "https://arampacha.github.io/reformer_fastai/"

git_url = "https://github.com/arampacha/reformer_fastai/tree/master/"

def custom_doc_links(name): return None
