# AUTOGENERATED BY NBDEV! DO NOT EDIT!

__all__ = ["index", "modules", "custom_doc_links", "git_url"]

index = {"exists": "00_core.ipynb",
         "default": "00_core.ipynb",
         "expand_dim1": "00_core.ipynb",
         "max_neg_value": "00_core.ipynb",
         "top_p_filter": "00_core.ipynb",
         "top_k_filter": "00_core.ipynb",
         "cache_method_decorator": "00_core.ipynb",
         "look_one_back": "00_core.ipynb",
         "chunked_sum": "00_core.ipynb",
         "sort_key_val": "00_core.ipynb",
         "batched_index_select": "00_core.ipynb",
         "do_cuda_timing": "00_core.ipynb",
         "model_performance": "00_core.ipynb",
         "total_params": "00_core.ipynb",
         "Residual": "01_layers.ipynb",
         "PostNorm": "01_layers.ipynb",
         "PreNorm": "01_layers.ipynb",
         "FeedForward": "01_layers.ipynb",
         "AbsolutePositionalEmbedding": "01_layers.ipynb",
         "FixedPositionalEmbedding": "01_layers.ipynb",
         "TransformerEmbedding": "01_layers.ipynb",
         "MASK_VAL": "02_attention.ipynb",
         "SELF_ATTN_MASK_VAL": "02_attention.ipynb",
         "AttnInProj": "02_attention.ipynb",
         "AttnInProjV2": "02_attention.ipynb",
         "SharedQKAttnInProj": "02_attention.ipynb",
         "ScaledDotProdAttention": "02_attention.ipynb",
         "Attention": "02_attention.ipynb",
         "AdditiveInProj": "02_attention.ipynb",
         "AdditiveAttention": "02_attention.ipynb",
         "LSHAttention": "02_attention.ipynb",
         "LSHSelfAttention": "02_attention.ipynb",
         "ReformerAttention": "02_attention.ipynb",
         "ReformerAttentionV2": "02_attention.ipynb",
         "TransformerEncoderBlock": "03_transformer.ipynb",
         "TransformerEncoder": "03_transformer.ipynb",
         "TransformerDecoderBlock": "03_transformer.ipynb",
         "TransformerDecoderBlockV2": "03_transformer.ipynb",
         "TransformerDecoder": "03_transformer.ipynb",
         "TransformerLM": "03_transformer.ipynb",
         "TransformerEncDec": "03_transformer.ipynb",
         "Chunk": "04_reformer.ipynb",
         "ChunkedFeedForward": "04_reformer.ipynb",
         "Deterministic": "04_reformer.ipynb",
         "ReversibleBlock": "04_reformer.ipynb",
         "IrreversibleBlock": "04_reformer.ipynb",
         "ReversibleSequence": "04_reformer.ipynb",
         "ReformerEncoder": "04_reformer.ipynb",
         "ReformerDecoder": "04_reformer.ipynb",
         "ReformerLM": "04_reformer.ipynb",
         "ReformerEncDec": "04_reformer.ipynb",
         "ByteTextTokenizer": "05_tokenizers.ipynb",
         "SubwordTextEncoder": "05_tokenizers.ipynb",
         "read_lines": "06_data.ipynb",
         "convert_data_to_seq_length": "06_data.ipynb",
         "read_and_prepare_data": "06_data.ipynb"}

modules = ["core.py",
           "layers.py",
           "attention.py",
           "transformer.py",
           "reformer.py",
           "tokenizers.py",
           "data.py"]

doc_url = "https://arampacha.github.io/reformer_fastai/"

git_url = "https://github.com/arampacha/reformer_fastai/tree/master/"

def custom_doc_links(name): return None
