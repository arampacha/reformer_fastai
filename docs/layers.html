---

title: Layers


keywords: fastai
sidebar: home_sidebar

summary: "Contains basic layers used both in baseline and reformer"
description: "Contains basic layers used both in baseline and reformer"
nb_path: "nbs/01_layers.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01_layers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Layer-Wrappers">Layer Wrappers<a class="anchor-link" href="#Layer-Wrappers"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>based on <a href="https://github.com/lucidrains/all-normalization-transformer/blob/master/all_normalization_transformer/all_normalization_transformer.py">https://github.com/lucidrains/all-normalization-transformer/blob/master/all_normalization_transformer/all_normalization_transformer.py</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Residual" class="doc_header"><code>class</code> <code>Residual</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L29" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Residual</code>(<strong><code>sublayer</code></strong>:<code>Module</code>) :: <code>Module</code></p>
</blockquote>
<p>Add skip-connection: out = x + sublayer(x)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PostNorm" class="doc_header"><code>class</code> <code>PostNorm</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L36" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PostNorm</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>sublayer</code></strong>:<code>Module</code>) :: <code>Module</code></p>
</blockquote>
<p>Adds LayerNorm after sublayer</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PreNorm" class="doc_header"><code>class</code> <code>PreNorm</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L47" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PreNorm</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>sublayer</code></strong>:<code>Module</code>) :: <code>Module</code></p>
</blockquote>
<p>Adds LayerNorm before sublayer</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Positional-FeedForward">Positional FeedForward<a class="anchor-link" href="#Positional-FeedForward"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FeedForward" class="doc_header"><code>class</code> <code>FeedForward</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L58" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FeedForward</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>d_ff</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Simple positional feed-forward module with GELU activation function.
If d_ff is None defaults to 4*d_model</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">ff</span>  <span class="o">=</span> <span class="n">Residual</span><span class="p">(</span><span class="n">PreNorm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">d</span><span class="p">)))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Embedding-Layer">Embedding Layer<a class="anchor-link" href="#Embedding-Layer"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_axial_shape" class="doc_header"><code>get_axial_shape</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L78" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_axial_shape</code>(<strong><code>x</code></strong>)</p>
</blockquote>
<p>Simple heuristic to suggest axial_shape givem max_seq_len (2 factors)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_axial_dims" class="doc_header"><code>get_axial_dims</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L82" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_axial_dims</code>(<strong><code>d_emb</code></strong>, <strong><code>n</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AbsolutePositionalEmbedding" class="doc_header"><code>class</code> <code>AbsolutePositionalEmbedding</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L88" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AbsolutePositionalEmbedding</code>(<strong><code>d_emb</code></strong>:<code>int</code>, <strong><code>max_seq_len</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<p>Learnable absolute positional encodings</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FixedPositionalEmbedding" class="doc_header"><code>class</code> <code>FixedPositionalEmbedding</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L98" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FixedPositionalEmbedding</code>(<strong><code>d_emb</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<p>Fixed positional encodings</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TransformerEmbedding" class="doc_header"><code>class</code> <code>TransformerEmbedding</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/layers.py#L111" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TransformerEmbedding</code>(<strong><code>emb_sz</code></strong>:<code>int</code>, <strong><code>d_emb</code></strong>:<code>int</code>, <strong><code>max_seq_len</code></strong>:<code>int</code>=<em><code>512</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>pos_enc</code></strong>:<code>str</code>=<em><code>'absolute'</code></em>, <strong><code>axial_shape</code></strong>:<code>Tuple</code>=<em><code>None</code></em>, <strong><code>axial_emb_dims</code></strong>:<code>Tuple</code>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Combines token embedings with positional encodings
pos_enc: str from {'absolute', 'fixed', 'axial'}</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fix_emb</span> <span class="o">=</span> <span class="n">TransformerEmbedding</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">pos_enc</span><span class="o">=</span><span class="s1">&#39;fixed&#39;</span><span class="p">)</span>
<span class="n">abs_emb</span> <span class="o">=</span> <span class="n">TransformerEmbedding</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">pos_enc</span><span class="o">=</span><span class="s1">&#39;absolute&#39;</span><span class="p">)</span>
<span class="n">axl_emb</span> <span class="o">=</span> <span class="n">TransformerEmbedding</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">pos_enc</span><span class="o">=</span><span class="s1">&#39;axial&#39;</span><span class="p">,</span> <span class="n">axial_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">16</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total number of parameters in embedding layer&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Fixed:    </span><span class="si">{</span><span class="n">total_params</span><span class="p">(</span><span class="n">fix_emb</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Absolute: </span><span class="si">{</span><span class="n">total_params</span><span class="p">(</span><span class="n">abs_emb</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Axial:    </span><span class="si">{</span><span class="n">total_params</span><span class="p">(</span><span class="n">axl_emb</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Total number of parameters in embedding layer
Fixed:    16384
Absolute: 49152
Axial:    17920
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

