---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/06_data.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/06_data.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data-helpers">Data helpers<a class="anchor-link" href="#Data-helpers"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="read_lines" class="doc_header"><code>read_lines</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L15" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>read_lines</code>(<strong><code>path</code></strong>)</p>
</blockquote>
<p>Tokenizes a text file.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="convert_data_to_seq_length" class="doc_header"><code>convert_data_to_seq_length</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L27" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>convert_data_to_seq_length</code>(<strong><code>df</code></strong>, <strong><code>seq_length</code></strong>=<em><code>65536</code></em>)</p>
</blockquote>
<p>Take a dataframe text data and convert it to a dataframe with the same columns where
every data sample is of numericalized token length of seq_length, except for the last example which is the remainder.
(less than but closest to the value given)
:param df: a pandas dataframe with columns [tokenized, lens] consisting of the numericalized tokens of text and their respective lengths
:param seq_length: the numericalized token sequence length to split the data into
:return: the new dataframe with split data samples</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="read_and_prepare_data" class="doc_header"><code>read_and_prepare_data</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L59" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>read_and_prepare_data</code>(<strong><code>data_path</code></strong>, <strong><code>seq_length</code></strong>=<em><code>0</code></em>)</p>
</blockquote>
<p>Read the data from file, and prepare the dataframe.
This does not include splitting into train and validation sets.
:param data_path: relative path to the raw data
:param seq_length: sequence length to split data into, default is don't change data sample length
:return: the dataframe after preparations</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">read_lines</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tokenizes a text file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>  <span class="c1"># + [&#39;&lt;eos&gt;&#39;])</span>
    <span class="k">return</span> <span class="n">lines</span>


<span class="k">def</span> <span class="nf">convert_data_to_seq_length</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">16</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Take a dataframe text data and convert it to a dataframe with the same columns where</span>
<span class="sd">    every data sample is of numericalized token length of seq_length, except for the last example which is the remainder.</span>
<span class="sd">    (less than but closest to the value given)</span>
<span class="sd">    :param df: a pandas dataframe with columns [tokenized, lens] consisting of the numericalized tokens of text and their respective lengths</span>
<span class="sd">    :param seq_length: the numericalized token sequence length to split the data into</span>
<span class="sd">    :return: the new dataframe with split data samples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concat_data</span> <span class="o">=</span> <span class="n">to_concat</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">]))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">,</span> <span class="s1">&#39;lens&#39;</span><span class="p">])</span>
    <span class="n">n_seqs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">concat_data</span><span class="p">)</span><span class="o">//</span><span class="n">seq_length</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_seqs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Splitting data&quot;</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">n_seqs</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">concat_data</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">seq_length</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">seq_length</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s1">&#39;tokenized&#39;</span><span class="p">:</span> <span class="n">sample</span><span class="p">,</span>
                <span class="s1">&#39;lens&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span>
            <span class="p">},</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Add last data sample which is the remainder</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">concat_data</span><span class="p">[</span><span class="n">n_seqs</span><span class="o">*</span><span class="n">seq_length</span><span class="p">:]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s1">&#39;tokenized&#39;</span><span class="p">:</span> <span class="n">sample</span><span class="p">,</span>
            <span class="s1">&#39;lens&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span>
        <span class="p">},</span>
        <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">read_and_prepare_data</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Read the data from file, and prepare the dataframe.</span>
<span class="sd">    This does not include splitting into train and validation sets.</span>
<span class="sd">    :param data_path: relative path to the raw data</span>
<span class="sd">    :param seq_length: sequence length to split data into, default is don&#39;t change data sample length</span>
<span class="sd">    :return: the dataframe after preparations</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reading data from path...&quot;</span><span class="p">)</span>
    <span class="c1"># Read the data from file</span>
    <span class="n">enwik8</span> <span class="o">=</span> <span class="n">read_lines</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">enwik8</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done!&quot;</span><span class="p">)</span>
    
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># this is so the printing of the progress bar is not weird</span>
    <span class="c1"># Initialize the BTT</span>
    <span class="n">btt</span> <span class="o">=</span> <span class="n">ByteTextTokenizer</span><span class="p">(</span><span class="n">is_lm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_eos</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Modify dataset for training</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Tokenizing data&quot;</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">progress_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">btt</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    
    <span class="c1"># By default we won&#39;t change the data sample length</span>
    <span class="k">if</span> <span class="n">seq_length</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence length has been added, splitting data to samples with sequence length &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">seq_length</span><span class="p">))</span>
        <span class="c1"># Convert data samples according to sequence length</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">convert_data_to_seq_length</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done!&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;lens&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;lens_cum_sum&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">lens</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;hello world!&#39;</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">test_text</span><span class="p">]})</span>
<span class="n">btt</span> <span class="o">=</span> <span class="n">ByteTextTokenizer</span><span class="p">(</span><span class="n">is_lm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_eos</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenized_test_text</span> <span class="o">=</span> <span class="n">btt</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>

<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;tokenizing data&quot;</span><span class="p">)</span>
<span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">progress_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">btt</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Split the df into a divisable length (2)</span>
<span class="n">converted_test_df</span> <span class="o">=</span> <span class="n">convert_data_to_seq_length</span><span class="p">(</span><span class="n">test_df</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">converted_test_df</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_text</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span>

<span class="c1"># Split the df into a non-divisable length (5)</span>
<span class="n">converted_test_df</span> <span class="o">=</span> <span class="n">convert_data_to_seq_length</span><span class="p">(</span><span class="n">test_df</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">converted_test_df</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_text</span><span class="p">)</span><span class="o">//</span><span class="mi">5</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>tokenizing data: 100%|██████████| 1/1 [00:00&lt;00:00, 1047.01it/s]
Splitting data: 100%|██████████| 7/7 [00:00&lt;00:00, 245.79it/s]
Splitting data: 100%|██████████| 2/2 [00:00&lt;00:00, 206.42it/s]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>LMTensorText([  2, 107, 104, 111, 111, 114,  35, 122, 114, 117, 111, 103,  36,   1])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

