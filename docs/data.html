---

title: Data utils


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/06_data.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/06_data.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="read_lines" class="doc_header"><code>read_lines</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L16" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>read_lines</code>(<strong><code>path</code></strong>)</p>
</blockquote>
<p>Tokenizes a text file.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="convert_data_to_seq_length" class="doc_header"><code>convert_data_to_seq_length</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L28" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>convert_data_to_seq_length</code>(<strong><code>df</code></strong>, <strong><code>seq_length</code></strong>=<em><code>65536</code></em>)</p>
</blockquote>
<p>Take a dataframe text data and convert it to a dataframe with the same columns where
every data sample is of numericalized token length of seq_length, except for the last example which is the remainder.
(less than but closest to the value given)
:param df: a pandas dataframe with columns [tokenized, lens] consisting of the numericalized tokens of text and their respective lengths
:param seq_length: the numericalized token sequence length to split the data into
:return: the new dataframe with split data samples</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="read_and_prepare_data" class="doc_header"><code>read_and_prepare_data</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L60" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>read_and_prepare_data</code>(<strong><code>data_path</code></strong>, <strong><code>seq_length</code></strong>=<em><code>0</code></em>)</p>
</blockquote>
<p>Read the data from file, and prepare the dataframe.
This does not include splitting into train and validation sets.
:param data_path: relative path to the raw data
:param seq_length: sequence length to split data into, default is don't change data sample length
:return: the dataframe after preparations</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">read_lines</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tokenizes a text file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>  <span class="c1"># + [&#39;&lt;eos&gt;&#39;])</span>
    <span class="k">return</span> <span class="n">lines</span>


<span class="k">def</span> <span class="nf">convert_data_to_seq_length</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">16</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Take a dataframe text data and convert it to a dataframe with the same columns where</span>
<span class="sd">    every data sample is of numericalized token length of seq_length, except for the last example which is the remainder.</span>
<span class="sd">    (less than but closest to the value given)</span>
<span class="sd">    :param df: a pandas dataframe with columns [tokenized, lens] consisting of the numericalized tokens of text and their respective lengths</span>
<span class="sd">    :param seq_length: the numericalized token sequence length to split the data into</span>
<span class="sd">    :return: the new dataframe with split data samples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concat_data</span> <span class="o">=</span> <span class="n">to_concat</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">]))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">,</span> <span class="s1">&#39;lens&#39;</span><span class="p">])</span>
    <span class="n">n_seqs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">concat_data</span><span class="p">)</span><span class="o">//</span><span class="n">seq_length</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_seqs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Splitting data&quot;</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">n_seqs</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">concat_data</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">seq_length</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">seq_length</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s1">&#39;tokenized&#39;</span><span class="p">:</span> <span class="n">sample</span><span class="p">,</span>
                <span class="s1">&#39;lens&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span>
            <span class="p">},</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Add last data sample which is the remainder</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">concat_data</span><span class="p">[</span><span class="n">n_seqs</span><span class="o">*</span><span class="n">seq_length</span><span class="p">:]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s1">&#39;tokenized&#39;</span><span class="p">:</span> <span class="n">sample</span><span class="p">,</span>
            <span class="s1">&#39;lens&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span>
        <span class="p">},</span>
        <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">read_and_prepare_data</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Read the data from file, and prepare the dataframe.</span>
<span class="sd">    This does not include splitting into train and validation sets.</span>
<span class="sd">    :param data_path: relative path to the raw data</span>
<span class="sd">    :param seq_length: sequence length to split data into, default is don&#39;t change data sample length</span>
<span class="sd">    :return: the dataframe after preparations</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reading data from path...&quot;</span><span class="p">)</span>
    <span class="c1"># Read the data from file</span>
    <span class="n">enwik8</span> <span class="o">=</span> <span class="n">read_lines</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">enwik8</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done!&quot;</span><span class="p">)</span>
    
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># this is so the printing of the progress bar is not weird</span>
    <span class="c1"># Initialize the BTT</span>
    <span class="n">btt</span> <span class="o">=</span> <span class="n">ByteTextTokenizer</span><span class="p">(</span><span class="n">is_lm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_eos</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Modify dataset for training</span>
    <span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Tokenizing data&quot;</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">progress_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">btt</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    
    <span class="c1"># By default we won&#39;t change the data sample length</span>
    <span class="k">if</span> <span class="n">seq_length</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence length has been added, splitting data to samples with sequence length &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">seq_length</span><span class="p">))</span>
        <span class="c1"># Convert data samples according to sequence length</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">convert_data_to_seq_length</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done!&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;lens&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;lens_cum_sum&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">lens</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;hello world!&#39;</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">test_text</span><span class="p">]})</span>
<span class="n">btt</span> <span class="o">=</span> <span class="n">ByteTextTokenizer</span><span class="p">(</span><span class="n">is_lm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_eos</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenized_test_text</span> <span class="o">=</span> <span class="n">btt</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>

<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;tokenizing data&quot;</span><span class="p">)</span>
<span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">progress_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">btt</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Split the df into a divisable length (2)</span>
<span class="n">converted_test_df</span> <span class="o">=</span> <span class="n">convert_data_to_seq_length</span><span class="p">(</span><span class="n">test_df</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">converted_test_df</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_text</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span>

<span class="c1"># Split the df into a non-divisable length (5)</span>
<span class="n">converted_test_df</span> <span class="o">=</span> <span class="n">convert_data_to_seq_length</span><span class="p">(</span><span class="n">test_df</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">converted_test_df</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_text</span><span class="p">)</span><span class="o">//</span><span class="mi">5</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>tokenizing data: 100%|██████████| 1/1 [00:00&lt;00:00, 1088.86it/s]
Splitting data: 100%|██████████| 7/7 [00:00&lt;00:00, 268.23it/s]
Splitting data: 100%|██████████| 2/2 [00:00&lt;00:00, 146.19it/s]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;tokenized&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>LMTensorText([  2, 107, 104, 111, 111, 114,  35, 122, 114, 117, 111, 103,  36,   1])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Synthetic-task">Synthetic task<a class="anchor-link" href="#Synthetic-task"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Data utils for the synthetic task of the reformer paper. We want to create sequences of the form 0w0w, where w is a sequence of integeres between 1-127 of some lenght: eg. 08470847.
We create items on the fly instead of all items up front.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TwinSequence" class="doc_header"><code>class</code> <code>TwinSequence</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L96" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TwinSequence</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>Dataset</code></p>
</blockquote>
<p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<p>.. note::
  :class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
  sampler that yields integral indices.  To make it work with a map-style
  dataset with non-integral indices/keys, a custom sampler must be provided.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">=</span><span class="n">DataLoaders</span><span class="o">.</span><span class="n">from_dsets</span><span class="p">(</span><span class="n">TwinSequence</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">bs</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">yb</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([6, 10]), torch.Size([6, 10]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inp</span><span class="p">,</span> <span class="n">targ</span> <span class="o">=</span> <span class="n">xb</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">yb</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">inp</span><span class="p">,</span> <span class="n">targ</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>([0, 39, 60, 35, 107, 0, 39, 60, 35, 107],
 [39, 60, 35, 107, 0, 39, 60, 35, 107, 0])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">inp</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">targ</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the synthetic task we also have to mask the <strong>first half</strong> of the targets. The first part is just random integers, so it's impossible to learn anything from it. We set the tokens in the first part to a special index, -100, and later tell our lossfunction to ignore items with this value. This means that the only task the model can learn is to copy the first part of the input sequence. If we didn't mask the first part, it would be penalized for poor performance in the first part, and would try to find a compromise.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MaskTargCallback" class="doc_header"><code>class</code> <code>MaskTargCallback</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/data.py#L111" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MaskTargCallback</code>(<strong><code>after_create</code></strong>=<em><code>None</code></em>, <strong><code>before_fit</code></strong>=<em><code>None</code></em>, <strong><code>before_epoch</code></strong>=<em><code>None</code></em>, <strong><code>before_train</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_pred</code></strong>=<em><code>None</code></em>, <strong><code>after_loss</code></strong>=<em><code>None</code></em>, <strong><code>before_backward</code></strong>=<em><code>None</code></em>, <strong><code>before_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_step</code></strong>=<em><code>None</code></em>, <strong><code>after_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_train</code></strong>=<em><code>None</code></em>, <strong><code>after_train</code></strong>=<em><code>None</code></em>, <strong><code>before_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_fit</code></strong>=<em><code>None</code></em>, <strong><code>after_fit</code></strong>=<em><code>None</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <code>Learner</code> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

