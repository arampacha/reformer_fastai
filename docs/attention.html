---

title: Attention Modules 


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/02_attention.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02_attention.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-Projection">Attention Projection<a class="anchor-link" href="#Attention-Projection"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AttnInProj" class="doc_header"><code>class</code> <code>AttnInProj</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L31" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AttnInProj</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">AttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q2</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">k2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">context</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
<span class="n">q2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AttnInProjV2" class="doc_header"><code>class</code> <code>AttnInProjV2</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L44" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AttnInProjV2</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">AttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q2</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">k2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">context</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
<span class="n">q2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Shared-Query-Key-Attention-Projection">Shared Query-Key Attention Projection<a class="anchor-link" href="#Shared-Query-Key-Attention-Projection"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SharedQKAttnInProj" class="doc_header"><code>class</code> <code>SharedQKAttnInProj</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L56" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SharedQKAttnInProj</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">shared_proj</span> <span class="o">=</span> <span class="n">SharedQKAttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">shared_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">q1</span> <span class="ow">is</span> <span class="n">k1</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scaled-Dot-Product-Attention">Scaled Dot Product Attention<a class="anchor-link" href="#Scaled-Dot-Product-Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ScaledDotProdAttention" class="doc_header"><code>class</code> <code>ScaledDotProdAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L69" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ScaledDotProdAttention</code>(<strong><code>d_model</code></strong>, <strong><code>n_heads</code></strong>, <strong><code>causal</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>shared_qk</code></strong>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes scaled dot-product attnetion given q, k, v</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Scaled dot-product attention is calculated as:</p>
<p>{% raw %}
$$\textbf {Attention}(Q,K,V) = \textbf {softmax}({QK^T\over\sqrt d_k})V $$
{% endraw %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn_func</span> <span class="o">=</span> <span class="n">ScaledDotProdAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">sl</span><span class="p">,</span><span class="n">d</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn_func</span> <span class="o">=</span> <span class="n">ScaledDotProdAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">shared_qk</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">sl</span><span class="p">,</span><span class="n">d</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">attn_func</span> <span class="o">=</span> <span class="n">ScaledDotProdAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">shared_qk</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-container">Attention container<a class="anchor-link" href="#Attention-container"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Attention" class="doc_header"><code>class</code> <code>Attention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L108" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Attention</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>shared_qk</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Standard attention module using scaled dot-product attention</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">shared_qk</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Causal masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="c1"># all elements in first half are equal despite second half is defferent</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="n">e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of input</span>
<span class="n">mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Context masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of context</span>
<span class="n">context_mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">context_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="n">context2</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">context2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span><span class="p">),</span> <span class="n">e_msg</span>
<span class="c1"># all output values are different for different context</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span> <span class="o">==</span> <span class="n">out2</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">csl</span> <span class="o">=</span> <span class="n">sl</span> <span class="o">+</span> <span class="mi">16</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">csl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">5</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">csl</span><span class="p">)</span>
<span class="n">context_mask</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">10</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span><span class="p">,</span> <span class="n">context_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="n">context_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">store_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attention</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">attention</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">csl</span><span class="p">)</span>
<span class="c1"># zeros for masked keys and &quot;don&#39;t cares&quot; for masked queries</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Additive-Attention">Additive Attention<a class="anchor-link" href="#Additive-Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdditiveInProj" class="doc_header"><code>class</code> <code>AdditiveInProj</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L161" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdditiveInProj</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">AdditiveInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q2</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">k2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">context</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">d</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
<span class="n">q2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 240, 64]), torch.Size([4, 240, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdditiveAttention" class="doc_header"><code>class</code> <code>AdditiveAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L177" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdditiveAttention</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>shared_qk</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/reformer_fastai/attention.html#Attention"><code>Attention</code></a></p>
</blockquote>
<p>Additive attention module</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Causal masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="c1"># all elements in first half are equal despite second half is defferent</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="n">e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of input</span>
<span class="n">mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Context masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of context</span>
<span class="n">context_mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">context_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="n">context2</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">context2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span><span class="p">),</span> <span class="n">e_msg</span>
<span class="c1"># all output values are different for different context</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span> <span class="o">==</span> <span class="n">out2</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LSH-Attention">LSH Attention<a class="anchor-link" href="#LSH-Attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>LSH attention from Reformer: <a href="https://arxiv.org/abs/2001.04451">The Efficient Transformer</a>. Based on <a href="https://github.com/lucidrains/reformer-pytorch/">lucidrains/reformer-pytorch</a>, but simpliefied and refactored. Uses shared keys and queries, but requires both to be passed as input (even though they are identical).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LSHAttention" class="doc_header"><code>class</code> <code>LSHAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L212" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LSHAttention</code>(<strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>bucket_size</code></strong>=<em><code>64</code></em>, <strong><code>n_hashes</code></strong>=<em><code>8</code></em>, <strong><code>causal</code></strong>=<em><code>False</code></em>, <strong><code>allow_duplicate_attention</code></strong>=<em><code>False</code></em>, <strong><code>attend_across_buckets</code></strong>=<em><code>False</code></em>, <strong><code>drop_for_hash_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>return_attn</code></strong>=<em><code>False</code></em>, <strong><code>random_state</code></strong>=<em><code>None</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>LSH attention module:</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Test LSH-attention layer. Note: <a href="/reformer_fastai/experiment-script.html#d_model"><code>d_model</code></a> is infered from input. Assumes shared key and query, but accepts both as input.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">shared_proj</span> <span class="o">=</span> <span class="n">SharedQKAttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">shared_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">lsh_attn</span> <span class="o">=</span> <span class="n">LSHAttention</span><span class="p">()</span>
<span class="n">out</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lsh_attn</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lsh_attn</span> <span class="o">=</span> <span class="n">LSHAttention</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">lsh_attn1</span> <span class="o">=</span> <span class="n">LSHAttention</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">lsh_attn</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">lsh_attn1</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LSH-self-attention">LSH-self-attention<a class="anchor-link" href="#LSH-self-attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Performs multihead <a href="/reformer_fastai/attention.html#LSHAttention"><code>LSHAttention</code></a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LSHSelfAttention" class="doc_header"><code>class</code> <code>LSHSelfAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L437" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LSHSelfAttention</code>(<strong><code>d_model</code></strong>, <strong><code>n_heads</code></strong>=<em><code>8</code></em>, <strong><code>bucket_size</code></strong>=<em><code>64</code></em>, <strong><code>n_hashes</code></strong>=<em><code>8</code></em>, <strong><code>causal</code></strong>=<em><code>False</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>attend_across_buckets</code></strong>=<em><code>False</code></em>, <strong><code>allow_duplicate_attention</code></strong>=<em><code>False</code></em>, <strong><code>return_attn</code></strong>=<em><code>False</code></em>, <strong><code>random_state</code></strong>=<em><code>None</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>dropout_hash</code></strong>=<em><code>0.0</code></em>, <strong><code>out_dropout</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">LSHSelfAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">out_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dropout_hash</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Testing-causal-masking">Testing causal masking<a class="anchor-link" href="#Testing-causal-masking"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that unlike the testing for the standard transformer, we can't draw new vectors for our change input since this would impact the clustering of the vectors in the LSH-algorithm. If we instead scale by a constant factor, the angular based clustering is not affected, even though the values have changed.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Causal masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">LSHSelfAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span><span class="o">*</span><span class="mi">2</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="n">e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Testing-masking">Testing masking<a class="anchor-link" href="#Testing-masking"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">LSHSelfAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of input</span>
<span class="n">mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span><span class="o">*</span><span class="mi">2</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Testing-context-masking">Testing context masking<a class="anchor-link" href="#Testing-context-masking"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Context masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">LSHSelfAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Passing in context=x should not alter the result, as compared to no context:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out0</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out0</span><span class="p">,</span> <span class="n">out1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mask second half of context</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">context</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>                        <span class="c1"># cloning x for context</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">context_mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="n">context2</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">context2</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">context2</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">*</span><span class="mi">2</span>   <span class="c1"># scaling to not affect clustering, relevant here?</span>
<span class="c1">#context2[:, sl//2:, :] = torch.randn(bs, sl//2, d)  # new random data</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[ 0.0452, -0.0576, -0.0456,  ..., -0.1130, -0.0580, -0.1802],
         [ 0.1083, -0.0677,  0.0850,  ..., -0.0548, -0.0367, -0.1796],
         [-0.0294, -0.1177,  0.0340,  ..., -0.0477, -0.0325, -0.0656],
         ...,
         [-0.0314, -0.0388, -0.0134,  ..., -0.0803, -0.1505, -0.0360],
         [ 0.0812, -0.0492,  0.0248,  ..., -0.0232, -0.0728, -0.1378],
         [-0.0238, -0.1127, -0.0312,  ...,  0.0057, -0.0981, -0.0497]],
        grad_fn=&lt;SelectBackward&gt;),
 tensor([[ 0.0408, -0.0534, -0.0392,  ..., -0.1195, -0.0529, -0.1826],
         [ 0.1115, -0.0670,  0.0916,  ..., -0.0603, -0.0332, -0.1739],
         [-0.0294, -0.1200,  0.0364,  ..., -0.0530, -0.0293, -0.0682],
         ...,
         [-0.0325, -0.0378, -0.0093,  ..., -0.0855, -0.1461, -0.0382],
         [ 0.0833, -0.0506,  0.0322,  ..., -0.0283, -0.0702, -0.1345],
         [-0.0238, -0.1127, -0.0312,  ...,  0.0057, -0.0981, -0.0497]],
        grad_fn=&lt;SelectBackward&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">out1</span><span class="o">==</span><span class="n">out2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(256)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#assert all_equal(out1, out2), e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">)</span>
<span class="c1">#assert not (out1 == out2).any()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reformer-Attention">Reformer Attention<a class="anchor-link" href="#Reformer-Attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Reformer attention calculates multihead attention with shared keys and queries, and allows switching between full <a href="/reformer_fastai/attention.html#Attention"><code>Attention</code></a> or <a href="/reformer_fastai/attention.html#LSHAttention"><code>LSHAttention</code></a> at creation, but not during inference or training.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ReformerAttention" class="doc_header"><code>class</code> <code>ReformerAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L509" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ReformerAttention</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>lsh_attention</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>n_hashes</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>bucket_size</code></strong>:<code>int</code>=<em><code>64</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Reformer attention container.</p>
<p>Switch between FullSharedQKAttention and LSHAttention.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn_lsh</span> <span class="o">=</span> <span class="n">ReformerAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">lsh_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_lsh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn_full</span> <span class="o">=</span> <span class="n">ReformerAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">lsh_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_full</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The state dicts of full and lsh attention are identical:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">attn_lsh</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;attn.in_proj.to_qk.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.in_proj.to_v.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.out_proj.weight&#39;, torch.Size([512, 512]))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">attn_full</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;attn.in_proj.to_qk.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.in_proj.to_v.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.out_proj.weight&#39;, torch.Size([512, 512]))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ReformerAttentionV2" class="doc_header"><code>class</code> <code>ReformerAttentionV2</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L562" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ReformerAttentionV2</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>attn_mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>use_lsh</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>n_hashes</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>bucket_size</code></strong>:<code>int</code>=<em><code>64</code></em>, <strong><code>random_state</code></strong>:<code>int</code>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Reformer attention container. Take on making it switchable on the fly.</p>
<p>Switch between FullSharedQKAttention and LSHAttention.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ReformerAttentionV2 containes both <a href="/reformer_fastai/attention.html#LSHAttention"><code>LSHAttention</code></a> and <a href="/reformer_fastai/attention.html#ScaledDotProdAttention"><code>ScaledDotProdAttention</code></a> and which one to use is determined by <code>self.lsh_attention</code> flag.</p>
<p>Proposed TODOs:</p>
<ul>
<li>[x] rename <code>self.lsh_attention</code> to <code>self.use_lsh</code> to avoid confusion with <code>self.lsh_attn</code> which is a module</li>
<li>[x] synchronize mask naming across all Attention modules: input_mask-&gt;attn_mask; minor renaming in LSH modules to make it consistent with <a href="/reformer_fastai/attention.html#Attention"><code>Attention</code></a></li>
<li>[x] add masking support to ReformerAttentionV2</li>
<li>[ ] add masking tests</li>
<li>[ ] synchronize <code>store_attention</code> functionality</li>
<li>[ ] test switchable attention module with synthetic task</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">ReformerAttentionV2</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">use_lsh</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 256])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span><span class="o">.</span><span class="n">use_lsh</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 256])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>ReformerAttentionV2(
  (in_proj): SharedQKAttnInProj(
    (to_qk): Linear(in_features=256, out_features=256, bias=False)
    (to_v): Linear(in_features=256, out_features=256, bias=False)
  )
  (lsh_attn): LSHAttention(
    (dropout): Dropout(p=0.1, inplace=False)
    (dropout_for_hash): Dropout(p=0.0, inplace=False)
  )
  (full_attn): ScaledDotProdAttention(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (out_proj): Linear(in_features=256, out_features=256, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>State dict remanes unchanged</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">attn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;in_proj.to_qk.weight&#39;, torch.Size([256, 256])),
 (&#39;in_proj.to_v.weight&#39;, torch.Size([256, 256])),
 (&#39;out_proj.weight&#39;, torch.Size([256, 256]))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

