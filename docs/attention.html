---

title: Attention Modules 


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/02_attention.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02_attention.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-Projection">Attention Projection<a class="anchor-link" href="#Attention-Projection"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AttnInProj" class="doc_header"><code>class</code> <code>AttnInProj</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L31" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AttnInProj</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">AttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q2</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">k2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">context</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
<span class="n">q2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AttnInProjV2" class="doc_header"><code>class</code> <code>AttnInProjV2</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L44" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AttnInProjV2</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">AttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q2</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">k2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">context</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
<span class="n">q2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Shared-Query-Key-Attention-Projection">Shared Query-Key Attention Projection<a class="anchor-link" href="#Shared-Query-Key-Attention-Projection"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SharedQKAttnInProj" class="doc_header"><code>class</code> <code>SharedQKAttnInProj</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L56" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SharedQKAttnInProj</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">shared_proj</span> <span class="o">=</span> <span class="n">SharedQKAttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">shared_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">q1</span> <span class="ow">is</span> <span class="n">k1</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scaled-Dot-Product-Attention">Scaled Dot Product Attention<a class="anchor-link" href="#Scaled-Dot-Product-Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ScaledDotProdAttention" class="doc_header"><code>class</code> <code>ScaledDotProdAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L69" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ScaledDotProdAttention</code>(<strong><code>d_model</code></strong>, <strong><code>n_heads</code></strong>, <strong><code>causal</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>shared_qk</code></strong>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes scaled dot-product attnetion given q, k, v</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Scaled dot-product attention is calculated as:</p>
<p>{% raw %}
$$\textbf {Attention}(Q,K,V) = \textbf {softmax}({QK^T\over\sqrt d_k})V $$
{% endraw %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn_func</span> <span class="o">=</span> <span class="n">ScaledDotProdAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn_func</span> <span class="o">=</span> <span class="n">ScaledDotProdAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">shared_qk</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">attn_func</span> <span class="o">=</span> <span class="n">ScaledDotProdAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">shared_qk</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-container">Attention container<a class="anchor-link" href="#Attention-container"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Attention" class="doc_header"><code>class</code> <code>Attention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L127" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Attention</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>shared_qk</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Standard attention module using scaled dot-product attention</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">shared_qk</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Causal masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="c1"># all elements in first half are equal despite second half is defferent</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="n">e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of input</span>
<span class="n">mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Context masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of context</span>
<span class="n">context_mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">context_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="n">context2</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">context2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span><span class="p">),</span> <span class="n">e_msg</span>
<span class="c1"># all output values are different for different context</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span> <span class="o">==</span> <span class="n">out2</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">csl</span> <span class="o">=</span> <span class="n">sl</span> <span class="o">+</span> <span class="mi">16</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">csl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">5</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">csl</span><span class="p">)</span>
<span class="n">context_mask</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">10</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span><span class="p">,</span> <span class="n">context_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="n">context_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">store_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attention</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">attention</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">csl</span><span class="p">)</span>
<span class="c1"># zeros for masked keys and &quot;don&#39;t cares&quot; for masked queries</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Additive-Attention">Additive Attention<a class="anchor-link" href="#Additive-Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdditiveInProj" class="doc_header"><code>class</code> <code>AdditiveInProj</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L180" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdditiveInProj</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Computes q, k, v from input x and [optional] context</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">AdditiveInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q1</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">k1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">q1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q2</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">k2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">v2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">context</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">d</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
<span class="n">q2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([4, 128, 64]), torch.Size([4, 240, 64]), torch.Size([4, 240, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdditiveAttention" class="doc_header"><code>class</code> <code>AdditiveAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L196" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdditiveAttention</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>shared_qk</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/reformer_fastai/attention.html#Attention"><code>Attention</code></a></p>
</blockquote>
<p>Additive attention module</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Causal masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="c1"># all elements in first half are equal despite second half is defferent</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="n">e_msg</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">AdditiveAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of input</span>
<span class="n">mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">x2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">e_msg</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">out2</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">e_msg</span> <span class="o">=</span> <span class="s2">&quot;Context masking error&quot;</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">)</span>
<span class="c1"># mask out second half of context</span>
<span class="n">context_mask</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context_mask</span> <span class="o">=</span> <span class="n">context_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="n">context2</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">context2</span><span class="p">[:,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">,</span> <span class="n">context_mask</span><span class="o">=</span><span class="n">context_mask</span><span class="p">)</span>
<span class="c1"># all elements are equal, masked values do not effect result</span>
<span class="k">assert</span> <span class="n">all_equal</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span><span class="p">),</span> <span class="n">e_msg</span>
<span class="c1"># all output values are different for different context</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">context2</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out1</span> <span class="o">==</span> <span class="n">out2</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LSH-Attention">LSH Attention<a class="anchor-link" href="#LSH-Attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>LSH attention from Reformer: <a href="https://arxiv.org/abs/2001.04451">The Efficient Transformer</a>. Based on <a href="https://github.com/lucidrains/reformer-pytorch/">lucidrains/reformer-pytorch</a>, but simpliefied and refactored. Uses shared keys and queries, but requires both to be passed as input (even though they are identical).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LSHAttention" class="doc_header"><code>class</code> <code>LSHAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L231" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LSHAttention</code>(<strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>bucket_size</code></strong>=<em><code>64</code></em>, <strong><code>n_hashes</code></strong>=<em><code>8</code></em>, <strong><code>causal</code></strong>=<em><code>False</code></em>, <strong><code>allow_duplicate_attention</code></strong>=<em><code>False</code></em>, <strong><code>attend_across_buckets</code></strong>=<em><code>False</code></em>, <strong><code>drop_for_hash_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>return_attn</code></strong>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>LSH attention module:</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Test LSH-attention layer. Note: <code>d_model</code> is infered from input. Assumes shared key and query, but accepts both as input.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">shared_proj</span> <span class="o">=</span> <span class="n">SharedQKAttnInProj</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">shared_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">lsh_attn</span> <span class="o">=</span> <span class="n">LSHAttention</span><span class="p">()</span>
<span class="n">out</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lsh_attn</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LSH-self-attention">LSH-self-attention<a class="anchor-link" href="#LSH-self-attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Performs multihead <a href="/reformer_fastai/attention.html#LSHAttention"><code>LSHAttention</code></a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LSHSelfAttention" class="doc_header"><code>class</code> <code>LSHSelfAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L452" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LSHSelfAttention</code>(<strong><code>d_model</code></strong>, <strong><code>n_heads</code></strong>=<em><code>8</code></em>, <strong><code>bucket_size</code></strong>=<em><code>64</code></em>, <strong><code>n_hashes</code></strong>=<em><code>8</code></em>, <strong><code>causal</code></strong>=<em><code>False</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>attend_across_buckets</code></strong>=<em><code>False</code></em>, <strong><code>allow_duplicate_attention</code></strong>=<em><code>False</code></em>, <strong><code>return_attn</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>post_attn_dropout</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">LSHSelfAttention</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reformer-Attention">Reformer Attention<a class="anchor-link" href="#Reformer-Attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Reformer attention calculates multihead attention with shared keys and queries, and allows switching between full <a href="/reformer_fastai/attention.html#Attention"><code>Attention</code></a> or <a href="/reformer_fastai/attention.html#LSHAttention"><code>LSHAttention</code></a> at creation, but not during inference or training.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ReformerAttention" class="doc_header"><code>class</code> <code>ReformerAttention</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L512" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ReformerAttention</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>lsh_attention</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>n_hashes</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>bucket_size</code></strong>:<code>int</code>=<em><code>64</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Reformer attention container.</p>
<p>Switch between FullSharedQKAttention and LSHAttention.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn_lsh</span> <span class="o">=</span> <span class="n">ReformerAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">lsh_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_lsh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn_full</span> <span class="o">=</span> <span class="n">ReformerAttention</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">lsh_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn_full</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The state dicts of full and lsh attention are identical:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">attn_lsh</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;attn.in_proj.to_qk.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.in_proj.to_v.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.out_proj.weight&#39;, torch.Size([512, 512]))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">attn_full</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;attn.in_proj.to_qk.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.in_proj.to_v.weight&#39;, torch.Size([512, 512])),
 (&#39;attn.out_proj.weight&#39;, torch.Size([512, 512]))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ReformerAttentionV2" class="doc_header"><code>class</code> <code>ReformerAttentionV2</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/attention.py#L565" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ReformerAttentionV2</code>(<strong><code>d_model</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>causal</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>mask</code></strong>:<code>Tensor</code>=<em><code>None</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>out_dropout</code></strong>:<code>float</code>=<em><code>None</code></em>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>store_attention</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>lsh_attention</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>n_hashes</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>bucket_size</code></strong>:<code>int</code>=<em><code>64</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Reformer attention container. Take on making it switchable on the fly.</p>
<p>Switch between FullSharedQKAttention and LSHAttention.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ReformerAttentionV2 containes both <a href="/reformer_fastai/attention.html#LSHAttention"><code>LSHAttention</code></a> and <a href="/reformer_fastai/attention.html#ScaledDotProdAttention"><code>ScaledDotProdAttention</code></a> and which one to use is determined by <code>self.lsh_attention</code> flag.</p>
<p>Proposed TODOs:</p>
<ul>
<li>[ ] rename <code>self.lsh_attention</code> to <code>self.use_lsh</code> to avoid confusion with <code>self.lsh_attn</code> which is a module</li>
<li>[ ] synchronize mask naming across all Attention modules</li>
<li>[ ] add masking support to ReformerAttentionV2</li>
<li>[ ] synchronize <code>store_attention</code> functionality</li>
<li>[ ] test switchable attention module with synthetic task</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sl</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">ReformerAttentionV2</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">lsh_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span><span class="o">.</span><span class="n">lsh_attention</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([4, 128, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>ReformerAttentionV2(
  (in_proj): SharedQKAttnInProj(
    (to_qk): Linear(in_features=512, out_features=512, bias=False)
    (to_v): Linear(in_features=512, out_features=512, bias=False)
  )
  (lsh_attn): LSHAttention(
    (dropout): Dropout(p=0.1, inplace=False)
    (dropout_for_hash): Dropout(p=0.0, inplace=False)
  )
  (full_attn): ScaledDotProdAttention(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (out_proj): Linear(in_features=512, out_features=512, bias=False)
  (dropout): Dropout(p=0.1, inplace=False)
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>State dict remanes unchanged</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">attn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;in_proj.to_qk.weight&#39;, torch.Size([512, 512])),
 (&#39;in_proj.to_v.weight&#39;, torch.Size([512, 512])),
 (&#39;out_proj.weight&#39;, torch.Size([512, 512]))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

