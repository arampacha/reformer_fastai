---

title: Reproducibility Summary


keywords: fastai
sidebar: home_sidebar

summary: "Summary page from our Reformer Reproducibility Report"
description: "Summary page from our Reformer Reproducibility Report"
nb_path: "nbs/reproducibility.report_1_reproducibility_summary.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/reproducibility.report_1_reproducibility_summary.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scope-of-Reproducibility">Scope of Reproducibility<a class="anchor-link" href="#Scope-of-Reproducibility"> </a></h2><p><em>State the main claim(s) of the original paper you are trying to reproduce (typically the main claim(s) of the paper). This is meant to place the work in context, and to tell a reader the objective of the reproduction.</em></p>
<p>The Reformer paper introduced a model, "Reformer", based on LSH Attention and Reversible Residual Layers, that is claimed to perform on par with Transformer models while:</p>
<ul>
<li>a) being much more memory-efficient</li>
<li>b) much faster on long sequences</li>
</ul>
<p>The scope of this reproducibility effort is to verify these claims of memory efficiency and speed on longer sequences. This verification was carried out via experiments for only NLP datasets used is the paper; a synthetic dataset, enwik8 and WMT. The computer vision experiments were not verified due to a lack of available compute for this community effort</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Methodology">Methodology<a class="anchor-link" href="#Methodology"> </a></h2><p><em>Briefly describe what you did and which resources you used.  For example, did you use author’s code?  Did youre-implement parts of the pipeline? You can also use this space to list the hardware used, and the total budget (e.g. GPUhours) for the experiments.</em></p>
<p>We chose to verify the authors' claims by focusing on the NLP-related experiments outlined in the paper. Our experiments replicated the LSH attention, Reversible Residual Layers, Shared Query-Key Attention NLP experiments with a custom syntheric dataset from the paper as well as the WMT [REF] and enwik8 [REF] datasets.</p>
<p>For code development, our approach was to implement the original Transformer model <a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani REF</a> from scratch, which them provided a solid foundation from which we could modify individual model components to test the techniques described in the paper. We referenced code from the authors Trax repository <a href="https://github.com/google/trax/tree/master/trax/models/reformer">Trax repository REF</a>, pytorch implementations of the Reformer paper <a href="https://github.com/lucidrains/reformer-pytorch">@lucidrains REF</a>, <a href="https://github.com/huggingface/transformers">@huggingface Transformer REF</a> as well as other Transformer archtecture implementations <a href="https://github.com/pytorch/fairseq/">FairSeq REF</a>, <a href="https://github.com/lucidrains/local-attention">@lucidtrains local-attention REF</a> and resources <a href="https://jalammar.github.io/illustrated-transformer/">Illustraed Transformer REF</a>, <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Annotated Transformer REF</a>.  During our experiment design we also referenced the authors' code to replicate the data loading and tokenization methods used and ported them into the fastai framework.</p>
<p>The fastai <a href="https://www.mdpi.com/2078-2489/11/2/108/htm">fastai REF</a> framework was used for dataloading and model traing, experiment tracking was carried out with Weights and Biases <a href="see bibtext reference below">wandb REF</a> and nbdev literate programming environment <a href="https://nbdev.fast.ai/">nbdev REF</a> was used for all development work.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>(Weights and Biases BibTeX Reference)</strong></p>
<blockquote><p>@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={<a href="https://www.wandb.com/},author">https://www.wandb.com/},author</a> = {Biewald, Lukas},
}</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results">Results<a class="anchor-link" href="#Results"> </a></h2><p><em>Start with your overall conclusion — where did your results reproduce the original paper, and where did your resultsdiffer? Be specific and use precise language, e.g. "we reproduced the accuracy to within 1% of reported value, whichsupports the paper’s conclusion that it outperforms the baselines". Getting exactly the same number is in most casesinfeasible, so you’ll need to use your judgement to decide if your results support the original claim of the paper.</em></p>
<p>OUR RESULTS</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-was-easy">What was easy<a class="anchor-link" href="#What-was-easy"> </a></h2><p><em>Describe which parts of your reproduction study were easy. For example, was it easy to run the author’s code, or easy to re-implement their method based on the description in the paper? The goal of this section is to summarize to a reader which parts of the original paper they could easily apply to their problem.</em></p>
<p>EASY - NOT MUCH :D</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-was-difficult">What was difficult<a class="anchor-link" href="#What-was-difficult"> </a></h2><p><em>Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhapsthe data was not available and you couldn’t verify some experiments, or the author’s code was broken and had to bedebugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn’t verify them.The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to re-use, orrequire a significant amount of work and resources to verify</em></p>
<p>While there were Reformer code resources available, when it came to implementation details this paper was quite challenging due to many design decisions not being fully documented in the paper. Examples include X,Y, Z.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Communication-with-original-authors">Communication with original authors<a class="anchor-link" href="#Communication-with-original-authors"> </a></h2><p>We emailed the authors a number of times to verify the metric used for language modelling (BPC), the optimizer used for the Machine Translation experiments</p>

</div>
</div>
</div>
</div>
 

