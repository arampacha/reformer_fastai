---

title: LSH exploration


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/50_exploration.LSH.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/50_exploration.LSH.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">pdb</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>LSH is an algorithm for clustering of high dimensional data. There are several ways of implementing the algorithm. We'll look at random projections and random rotations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LSH-clustering">LSH clustering<a class="anchor-link" href="#LSH-clustering"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Random-projections">Random projections<a class="anchor-link" href="#Random-projections"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://www.youtube.com/watch?v=i4H0kjxrias&amp;t=1s">Yannick</a> explains LSH with random projections, and the same with this <a href="https://www.pragmatic.ml/reformer-deep-dive/">blog post</a>. That means that in a 2D case we can envision lines drawn at random, and points grouped depending on if they point in a similar direction or not. This method is probabilistic as points that are close can end up in different buckets by chance, but they will have a high probability of beeing grouped to gether. Illustration from the blog:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/Random_projections.png" alt="Random_projections.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll demonstrate random projections for the 2 dimensional case. First we fix some points in the 2d plane. We'll do everything deterministically to begin with:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                   <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-.</span><span class="mi">5</span><span class="p">]])</span>
<span class="n">fig</span> <span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">]):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">();</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we make a vector assumed to pass thru the origin. We'll make it a positive unit vector so we can think of it pointing up and to the right. We can manually project this vector from say x = [-1, 1] and display it as a line:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dot product between vectors a and b is the lenght of a projected onto b multiplied with the length of b. So if two vectors point in the same direction their dot product is positive and vice versa. If vectors are perpendicular the dot product is 0. The doproduct is defined by summing up the pairwise products of two vectors. Note that we can think of our points as vectors from the origin. Taking the dot product of the red point and u, we get as expected a postivite dotproduct:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1.9</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can calculate the dotproduct for all our points by matrix multiplication. The red vector points along u, the blue opposite, and the green on is perpendicular:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">points</span><span class="nd">@u</span><span class="o">.</span><span class="n">T</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([ 1.9, -1.9,  0. ])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can generalize this in a function by adding randomness, and only keeping the sign of the dotproduct.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">rand_proj</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 1 projection in 2-dim</span>
    <span class="n">dots</span> <span class="o">=</span> <span class="n">points</span><span class="nd">@u</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">dots</span><span class="p">)</span>

<span class="n">rand_proj</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[-1.],
       [ 1.],
       [ 1.]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p><strong>In this case we have two buckets, -1 and 1</strong></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can also repeat the bucketing process n times to get a more stable estimate. Each run will produce a different results. Eg. the first column below represents a new hash bucket for the red point.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">rand_proj</span><span class="p">(</span><span class="n">points</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[array([ 1., -1.,  1.]),
 array([ 1., -1.,  1.]),
 array([-1.,  1., -1.]),
 array([-1.,  1.,  1.]),
 array([ 1., -1.,  1.])]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can also generalize this to produce more than two buckets. The number of buckets is 2x the number of random projections:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">rand_proj</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">n_projections</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_projections</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">)</span>
    <span class="n">dots</span> <span class="o">=</span> <span class="n">points</span><span class="nd">@u</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">dots</span><span class="p">)</span>

<span class="n">rand_proj</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">n_projections</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[-1.,  1.],
       [ 1., -1.],
       [-1.,  1.]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this case we have 4 buckets: [-1,-1], [-1,1], [1,-1], [1,1]. Which could be further combined into a single id [0,1,2,3]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="random-rotations">random rotations<a class="anchor-link" href="#random-rotations"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The paper instead opts for an angular interpretation of LSH:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/Random_rotations.png" alt="Random_rotations.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the 2d case each point is projected onto a unit sphere, and then rotated randomly. Bucketing will depend on which of the sectors it ends up in. The algorithm is decribed as:</p>
<p><em>To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = arg max([xR; −xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors.</em></p>
<p>The blog has an implementation to compute a single hash wich follows these steps.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">rand_rotations</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_buckets</span><span class="p">):</span>
    <span class="n">random_rotations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_buckets</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">rotated_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">random_rotations</span><span class="p">)</span>
    <span class="n">rotated_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">rotated_vectors</span><span class="p">,</span> <span class="o">-</span><span class="n">rotated_vectors</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rotated_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rand_rotations</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_buckets</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([3, 1, 2])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This has the nice property of directly giving us the hash-bucket id instead of our list above. The next step will be to scale the algorithm to do several rounds. One could simply loop it, but it will be more effectient to add an extra dimension to our matrices. We will also need to take care of batch and attention head dimensions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Incorporation-of-batches-and-multiple-hashing-rounds">Incorporation of batches and multiple hashing rounds<a class="anchor-link" href="#Incorporation-of-batches-and-multiple-hashing-rounds"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">rearrange</span><span class="p">,</span> <span class="n">repeat</span><span class="p">,</span> <span class="n">reduce</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code for the LSH algorithm used for the paper can be found in the <a href="https://github.com/google/trax/blob/6b4ee9f7473934d15f7b872000be43b1a984bd81/trax/layers/research/efficient_attention.py#L56">trax library</a>. Lucidrains also has a stripped down <a href="https://github.com/lucidrains/reformer-pytorch/blob/5eb10786dee3fcdb0092a3d8cc415665f0bbdb14/reformer_pytorch/reformer_pytorch.py#L207">version</a>. We'll base our algorithm on lucidrains, but simplify even further to make the algorithm as clear as possible:</p>
<ul>
<li>we'll assume rehashing each round as in trax library</li>
<li>no dropout on the vectors to be hashed.</li>
<li>won't pay attention to device at the moment</li>
<li>assume correct dytpes passed in</li>
<li>similar number of rotations per head</li>
</ul>
<p>That means we have to:</p>
<ol>
<li>Keep track of the various dimension</li>
<li>perform the random_rotations part of the algorithm (as above)</li>
<li>Structure the output depending on number of rounds and buckets</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">hash_vectors</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="n">n_buckets</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

    <span class="c1"># 1. account for the input shapes. vecs = [bs, sl, hidden_dim]</span>
    <span class="k">assert</span> <span class="n">n_buckets</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">vecs</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">rotations_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">n_buckets</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> 

    <span class="c1"># 2. get the dotproduct, cat and argmax like in the section above</span>

    <span class="n">random_rotations</span> <span class="o">=</span> <span class="n">repeat</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">rotations_shape</span><span class="p">),</span>  <span class="c1">#repeat rotations accross the batch dimension</span>
                              <span class="s1">&#39;h nr nb -&gt; bs h nr nb&#39;</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>                               
    
    <span class="n">rotated_vecs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bsh,bhrn-&gt;brsn&#39;</span><span class="p">,</span> 
                                <span class="n">vecs</span><span class="p">,</span>               <span class="c1"># [bs, sl, hidden_dim]</span>
                                <span class="n">random_rotations</span><span class="p">)</span>   <span class="c1"># [bs, hidden_dim, n_rounds, n_buckets//2]</span>
                                                    <span class="c1"># rotated vecs: [bs, n_rounds, sl, n_buckets//2]</span>
        
    <span class="n">rotated_vecs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">rotated_vecs</span><span class="p">,</span> <span class="o">-</span><span class="n">rotated_vecs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [bs, n_rounds, sl, n_buckets]</span>
    <span class="n">buckets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rotated_vecs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                    <span class="c1"># [bs, n_rounds, sl] </span>

    <span class="c1"># 3. Next we add offsets so that bucket numbers from different hashing rounds don&#39;t overlap.</span>

    <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">)</span>                               <span class="c1"># list of [0,1,2,..n_rounds-1]</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">offsets</span> <span class="o">*</span> <span class="n">n_buckets</span><span class="p">,</span> <span class="s1">&#39;(r)-&gt; (1)(r)(1)&#39;</span><span class="p">)</span>    <span class="c1"># [1, n_rounds, 1]</span>
    <span class="n">buckets</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">buckets</span><span class="o">+</span><span class="n">offsets</span><span class="p">,</span> <span class="s1">&#39;bs r sl -&gt; bs (r sl)&#39;</span><span class="p">)</span>   <span class="c1"># [bs, (n_rounds*sl)]</span>
    <span class="k">return</span> <span class="n">buckets</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's pass our trusty old points in, but first convert to tensors, and create a batch dimension first.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;b d -&gt; b () d&#39;</span><span class="p">)</span>
<span class="n">hash_vectors</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">n_buckets</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">hash_vectors</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">n_buckets</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[0],
         [1],
         [0]]),
 tensor([[1, 2, 4, 7, 8],
         [0, 3, 5, 6, 9],
         [1, 2, 4, 6, 9]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the multiround case, the result from each hashing round is stacked along the 1 dimension, and an offset is added so each one has a unique index, from [0 to n_rounds * n_hases -1], [0-9] in this case.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the transformer setting the q and k matrix shapes will be: [bs, sl, hidden_dim]</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">hash_vectors</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">n_buckets</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">out</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 512]), tensor(0), tensor(3))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">hash_vectors</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">n_buckets</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">out</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 1536]), tensor(0), tensor(11))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Main-steps-of-LSH-attention">Main steps of LSH attention<a class="anchor-link" href="#Main-steps-of-LSH-attention"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next parts we need to add are:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/LSH_main-steps.png" alt="LSH_main-steps.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>sort first for bucket id, next for position in original sequence</li>
<li>chunk buckets to some given size</li>
<li>concatenate chunk with previous chunk to allow for inbetween attention</li>
<li>calculate attention and output per (concatenated) chunk</li>
<li>unsort everything and return</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's set up some data to test - with a bs of 1 for sake of simplicity</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">buckets</span> <span class="o">=</span> <span class="n">hash_vectors</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n_buckets</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([1, 1, 1, 2, 1, 3, 2, 1, 3, 0])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The number of vectors in each bucket:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">buckets</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0, 1, 2, 3]), tensor([139, 122, 127, 124]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have 512 vectors of dim 128 to sort according to bucket:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([512, 128])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sorting-tensors">Sorting tensors<a class="anchor-link" href="#Sorting-tensors"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tmp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">tmp</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8],
        [ 9, 10, 11],
        [12, 13, 14]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can sort along a specified axis, and get the index of the sorted item in the original tensor</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">v</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">v</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 2,  1,  0],
        [ 5,  4,  3],
        [ 8,  7,  6],
        [11, 10,  9],
        [14, 13, 12]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">i</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[2, 1, 0],
        [2, 1, 0],
        [2, 1, 0],
        [2, 1, 0],
        [2, 1, 0]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And use the indices to get our original order back:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">i</span><span class="p">[:,</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0, 1, 2],
        [0, 1, 2],
        [0, 1, 2],
        [0, 1, 2],
        [0, 1, 2]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But how do we sort by some other index? We can slice according to the indices of the sorted buckets:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reorder</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">tmp</span><span class="p">[:,</span><span class="n">reorder</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 2,  0,  1],
        [ 5,  3,  4],
        [ 8,  6,  7],
        [11,  9, 10],
        [14, 12, 13]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's get indexes of the sorted buckets tensor and use that to reorder k:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">v</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="n">buckets</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">v</span><span class="p">[:,:</span><span class="mi">5</span><span class="p">],</span> <span class="n">v</span><span class="p">[:,</span><span class="o">-</span><span class="mi">5</span><span class="p">:],</span> <span class="n">i</span><span class="p">[:,:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[0, 0, 0, 0, 0]]),
 tensor([[3, 3, 3, 3, 3]]),
 tensor([[351, 366, 364, 114, 363]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And finally reorder our k. Since we have a batch dimension (0) we have sorted along dim(1) - the rows.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k</span><span class="p">[:,</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">],:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-0.5602, -0.2878, -1.1196,  ..., -1.2602,  1.1406, -0.4469],
         [ 0.5765,  0.1284,  0.0911,  ..., -0.2538, -0.7697, -1.6130],
         [ 0.2006, -0.7220, -1.3837,  ..., -1.2253,  0.5768,  0.6116],
         ...,
         [-0.0983,  0.5041, -1.4759,  ..., -0.6982, -0.0050,  1.1906],
         [-1.5745,  0.4858, -1.5506,  ..., -0.8055, -0.6155, -2.4515],
         [-0.4779, -1.0148,  1.4694,  ..., -1.2689,  0.2453, -0.0928]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can verify that the first item above is the same as the index i[:,0] of our k:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k</span><span class="p">[:,</span><span class="n">i</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-0.5602, -0.2878, -1.1196, -0.1774, -0.3821, -0.5390, -0.8756,
          -0.2887, -1.0551, -0.4596]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also have to maintain order with respect to the orginial sequence order. So every k in bucket 0 have to be resorted according to it's original place in the sequenze.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sort-chunks">Sort chunks<a class="anchor-link" href="#Sort-chunks"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Chunking to equal size is straight forward. Remove the batch dimension and chunk along the sequence dimension:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">chunks</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">),</span> <span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([1, 512, 128]), 4, torch.Size([128, 128]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="calculate-attention">calculate attention<a class="anchor-link" href="#calculate-attention"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can get the attention from a chunk (assuming k=v) the normal way, ignoring attention heads and batch dimension for the moment. We also must make sure that a chunk can attend to the prvious chunk. For chunk 1 this means concatenation with chunk 0, before attention is calculated:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k0</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">chunks</span>
<span class="n">k0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([128, 128]), torch.Size([128, 128]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k_01</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">k0</span><span class="p">,</span><span class="n">k1</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
<span class="n">k_01</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([256, 128])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">k_01</span><span class="nd">@k_01</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">attn</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([256, 256])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attn</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="caveats">caveats<a class="anchor-link" href="#caveats"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are some important steps we still need to account for. From the paper:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/paper_1.png" alt="paper_1.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/paper_2.png" alt="paper_2.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That means we still have to:</p>
<ul>
<li>set h(kj) = h(qj)</li>
<li>vectors in each chunk have to be sorted according to order in the original sequence</li>
<li>mask out attention to not attend to it's own position (except when no other targets exist)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implement-LSH-attention-layer">Implement LSH-attention layer<a class="anchor-link" href="#Implement-LSH-attention-layer"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have a rough idea of the implementation steps, so let's see if we can find a minimal solution based on the implementation by <a href="https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py">lucidrains</a>. Once again will try to strip away as much as possible to leave a minimal solution for clarity:</p>
<ul>
<li>no dropout </li>
<li>no padding or casusal masking (only masking relevant to LSH)</li>
<li>don't return attn matrix</li>
<li>don't account for duplicate attention (unsure about this)</li>
<li>don't detach tensors where we don't need gradients</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="helpers">helpers<a class="anchor-link" href="#helpers"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From: <a href="https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py">https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># boundaries might occur in the middle of a sequence of items from the</span>
<span class="c1"># same bucket, so this increases the chances of attending to relevant items.</span>
<span class="k">def</span> <span class="nf">look_one_back</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_extra</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="o">...</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x_extra</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sort_key_val</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">values</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="n">t2</span> <span class="o">=</span> <span class="n">t2</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">t2</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">batched_index_select</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
    <span class="n">last_dim</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">values</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">last_dim</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">max_neg_value</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">chunked_sum</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="o">*</span><span class="n">orig_size</span><span class="p">,</span> <span class="n">last_dim</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">last_dim</span><span class="p">)</span>
    <span class="n">summed_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">tensor</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">summed_tensors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">orig_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">TOKEN_SELF_ATTN_VALUE</span> <span class="o">=</span> <span class="o">-</span><span class="mf">5e4</span> <span class="c1"># carefully set for half precision to work</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Forward,-step-by-step">Forward, step by step<a class="anchor-link" href="#Forward,-step-by-step"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's step through the forward() of the LSHAttention layer (below), to see if we can makes sense of it. We start with random test data. We assume <code>q=k</code>, and pass along <code>v</code> as well:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span>
<span class="n">qk</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">dim</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<span class="n">qk</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 512, 256]), torch.Size([64, 512, 256]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="hashing">hashing<a class="anchor-link" href="#hashing"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Grouping is done by <code>hash_vector()</code>. We'll test 6 rounds with 16 buckets.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_buckets</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span>
<span class="n">buckets</span> <span class="o">=</span> <span class="n">hash_vectors</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="n">n_buckets</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">)</span>
<span class="n">buckets</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 3072])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 5,  0,  5, 14,  2,  4,  0,  1,  5,  3, 14,  9, 10,  0,  3,  4,  0, 15,
        12,  6])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that our bucket ids are unique within their hash group. I.e. the first and second hash group has non overlaping bucket ids:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">sl</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(),</span> <span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sl</span><span class="p">:</span><span class="mi">2</span><span class="o">*</span><span class="n">sl</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]),
 tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The ticker is an index into the original sequence id:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ticker</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span> <span class="o">*</span> <span class="n">sl</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">buckets</span><span class="p">)</span> 
<span class="n">ticker</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[   0,    1,    2,  ..., 3069, 3070, 3071],
        [   0,    1,    2,  ..., 3069, 3070, 3071],
        [   0,    1,    2,  ..., 3069, 3070, 3071],
        ...,
        [   0,    1,    2,  ..., 3069, 3070, 3071],
        [   0,    1,    2,  ..., 3069, 3070, 3071],
        [   0,    1,    2,  ..., 3069, 3070, 3071]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that the id is not reset between hash groups:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ticker</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="n">sl</span><span class="o">+</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([507, 508, 509, 510, 511, 512, 513, 514, 515, 516])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But by taking the mod of sl, we can get the sequence id in each hash round:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">ticker</span> <span class="o">%</span> <span class="n">sl</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="n">sl</span><span class="o">+</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([507, 508, 509, 510, 511,   0,   1,   2,   3,   4])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Or the id of the hash round by integer division with sl</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">ticker</span><span class="o">//</span><span class="n">sl</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sl</span><span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="n">sl</span><span class="o">+</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Sorting">Sorting<a class="anchor-link" href="#Sorting"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the lsh layer we make a <code>buckets_and_t</code> index. It's the bucket id scaled by sl + <code>ticker % sl</code>. Let's inspect the first component:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 5,  0,  5, 14,  2,  4,  0,  1,  5,  3])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We scale it by muliplying with sl:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sl</span><span class="o">*</span><span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([2560,    0, 2560, 7168, 1024, 2048,    0,  512, 2560, 1536])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we add <code>ticker%sl</code> which is the the sequence id (see above)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># we add the bucket id scaled by seqlen</span>
<span class="c1"># shape: [bs, (seqlen*buckets)]</span>
<span class="c1"># let us sort according to bucket id and index in sequence</span>
<span class="n">buckets_and_t</span> <span class="o">=</span> <span class="n">sl</span> <span class="o">*</span> <span class="n">buckets</span> <span class="o">+</span> <span class="p">(</span><span class="n">ticker</span> <span class="o">%</span> <span class="n">sl</span><span class="p">)</span>
<span class="n">buckets_and_t</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 2560,     1,  2562,  ..., 41469, 48638, 45567],
        [ 7680,  4097,  4098,  ..., 43517, 41982, 47103],
        [ 1024,  4097,  7170,  ..., 45565, 48638, 41983],
        ...,
        [ 4096,  6657,  3586,  ..., 43005, 47102, 44031],
        [ 7168,  4609,   514,  ..., 44029, 44542, 44031],
        [ 1536,  7681,  3586,  ..., 44541, 43518, 42495]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Buckets_and_t</code> is an index that takes both hash group id and sequence id into account, on the form <code>bucket_id*sl + sequence id</code>. The first item can take one of 16 values (n_buckets):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span><span class="o">*</span><span class="n">sl</span> <span class="o">+</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_buckets</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[0, 512, 1024, 1536, 2048, 2560, 3072, 3584, 4096, 4608, 5120, 5632, 6144, 6656, 7168, 7680]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the second position we add 1 to offset for the sequence id:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">([</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[1, 513, 1025, 1537, 2049, 2561, 3073, 3585, 4097, 4609, 5121, 5633, 6145, 6657, 7169, 7681]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can recreate the original ids from <code>buckets_and_t</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">buckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">20</span><span class="p">]</span> <span class="o">-</span> <span class="n">ticker</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">20</span><span class="p">])</span><span class="o">/</span><span class="n">sl</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 5.,  0.,  5., 14.,  2.,  4.,  0.,  1.,  5.,  3., 14.,  9., 10.,  0.,
         3.,  4.,  0., 15., 12.,  6.])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 5,  0,  5, 14,  2,  4,  0,  1,  5,  3, 14,  9, 10,  0,  3,  4,  0, 15,
        12,  6])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Buckets_and_t</code> is a unique index:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">buckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">buckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(3072, 3072)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's also non overlapping between hash groups - the max id of the first group is smaller than the min id of the second group:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">sl</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">buckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="p">:</span><span class="n">sl</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor(8189), tensor(8202))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we sort buckets_and_t. Since we scaled the id's, hash groups are autmatically sorted.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sbuckets_and_t</span><span class="p">,</span> <span class="n">sticker</span> <span class="o">=</span> <span class="n">sort_key_val</span><span class="p">(</span><span class="n">buckets_and_t</span><span class="p">,</span> <span class="n">ticker</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># shapes are [bs, seqlen*n_hashes]</span>
<span class="n">sbuckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([  1,   6,  13,  16,  20,  34,  42,  45,  51,  52,  95,  98, 100, 107,
        112, 127, 158, 192, 215, 219])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>sticker</code> are the ids from our original <code>buckets_and_t</code>. That is, when we unsort <code>sbuckets</code>, <code>sticker</code> gives us the index of where the items belong in <code>buckets_and_t</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sticker</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="p">:</span><span class="n">sl</span><span class="o">+</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([522, 524, 531, 533, 540, 541, 567, 579, 596, 597])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use <code>sticker</code> to look up items in <code>buckets_and_t</code> to create <code>sbuckets_and_t</code>, but not the other way around!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">sticker</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">10</span><span class="p">:]],</span> <span class="n">sbuckets_and_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([48964, 48969, 48987, 48999, 49011, 49028, 49034, 49068, 49089, 49107]),
 tensor([48964, 48969, 48987, 48999, 49011, 49028, 49034, 49068, 49089, 49107]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But we must also be able to undo this sorting. By doing a normal sort on the sticker list, we get a set of ids that tells us where an item belong in the original buckets_and_t:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">undo_sort</span> <span class="o">=</span> <span class="n">sticker</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                                       <span class="c1"># indexes to undo sortings</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use the gather function to recreate <code>buckets_and_t</code> from the sorted buckets. And we already know we can recreate seqlen id and hash group id from <code>buckets_and_t</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sbuckets_and_t</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">undo_sort</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 2560,     1,  2562,  ..., 41469, 48638, 45567],
        [ 7680,  4097,  4098,  ..., 43517, 41982, 47103],
        [ 1024,  4097,  7170,  ..., 45565, 48638, 41983],
        ...,
        [ 4096,  6657,  3586,  ..., 43005, 47102, 44031],
        [ 7168,  4609,   514,  ..., 44029, 44542, 44031],
        [ 1536,  7681,  3586,  ..., 44541, 43518, 42495]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">sbuckets_and_t</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">undo_sort</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span> <span class="o">-</span> <span class="n">ticker</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">20</span><span class="p">])</span><span class="o">/</span><span class="n">sl</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 5.,  0.,  5., 14.,  2.,  4.,  0.,  1.,  5.,  3., 14.,  9., 10.,  0.,
         3.,  4.,  0., 15., 12.,  6.])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 5,  0,  5, 14,  2,  4,  0,  1,  5,  3, 14,  9, 10,  0,  3,  4,  0, 15,
        12,  6])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>To recap:</strong></p>
<ul>
<li><code>sbuckets_and_t</code> is a "double index" sorted by <strong>both</strong> hash group id and sequence id</li>
<li>we can use <code>undo_sort</code> to recreate <code>buckets_and_t</code>, which is an index of hash_group and sequence id in the original order</li>
<li>we can use <code>buckets_and_t</code> to get the original bucket ids in their respective hash groups, i.e. <code>buckets</code></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Chunking">Chunking<a class="anchor-link" href="#Chunking"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next step is to extract the relevant vectors from our input. First we take the <code>sticker</code> mod with sl to produce the sequence id for each hash group. This gives us the id of <code>qk</code> and <code>v</code> to look up:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">st</span> <span class="o">=</span> <span class="p">(</span><span class="n">sticker</span> <span class="o">%</span> <span class="n">sl</span><span class="p">)</span>              <span class="c1"># index of [0..seqlen-1] for each hash round (n_hashes)[bs, seqlen*n_hashes]</span>
<span class="n">st</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[  1,   6,  13,  ..., 428, 449, 467],
        [ 12,  20,  27,  ..., 483, 491, 506],
        [ 24,  26,  29,  ..., 409, 423, 468],
        ...,
        [  6,  21,  31,  ..., 459, 468, 469],
        [  9,  13,  80,  ..., 433, 457, 500],
        [ 13,  25,  36,  ..., 448, 474, 482]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We then lookup the vectors according to their (sorted) seqlen id:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sqk</span> <span class="o">=</span> <span class="n">batched_index_select</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="n">st</span><span class="p">)</span>   <span class="c1"># get the sorted qk, [bs, seqlen, model_dim]</span>
<span class="n">sv</span> <span class="o">=</span> <span class="n">batched_index_select</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">st</span><span class="p">)</span>     <span class="c1"># get the sorted v, [bs, seqlen, model_dim] </span>
<span class="n">sqk</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sv</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 3072, 256]), torch.Size([64, 3072, 256]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>sqk</code> and <code>sv</code> now contains the input vectors sorted by hash group and seqlen id. Next we calculate the number of chunks. We assume chunks of even size, regardless of how many vectors are actually in a particular group:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_chunks</span> <span class="o">=</span> <span class="n">n_rounds</span> <span class="o">*</span> <span class="n">n_buckets</span>
<span class="n">n_chunks</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>96</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each chunk has a size along the sl dimension (chunk_size) of:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sqk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">n_chunks</span><span class="p">,</span> <span class="n">sl</span><span class="o">/</span><span class="n">n_buckets</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(32.0, 32.0)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we reshape both the <code>st</code> index, <code>sqk</code> and <code>sv</code> to add the <code>n_chunks</code> dimension.
We operate with one index for the query, and a another (duplicate) for k and v:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># get the qk and v chunks and also the indexes to undo sort later</span>
<span class="n">bq_t</span> <span class="o">=</span> <span class="n">bkv_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">st</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>   <span class="c1"># [bs, n_chunks, chunk_size]</span>
<span class="n">bq_t</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bqk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sqk</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>      <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>
<span class="n">bv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sv</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>        <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>
<span class="n">bqk</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bv</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 96, 32, 256]), torch.Size([64, 96, 32, 256]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next step it to split <code>k</code> and <code>v</code>. We normalize <code>k</code> but <strong>not</strong> <code>q</code>. This is mentioned in the paper:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/paper_3.png" alt="paper_3.png"></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq</span> <span class="o">=</span> <span class="n">bqk</span>
<span class="n">bk</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">bqk</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">bq</span><span class="p">)</span>
<span class="n">bq</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">bk</span><span class="p">[</span><span class="o">...</span><span class="p">,:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor(0.0001), tensor(8.9945e-06))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we add the previous chunk as described in the paper, but only for <code>k</code> and <code>v</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bk</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">bk</span><span class="p">)</span>            <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
<span class="n">bv</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">bv</span><span class="p">)</span>            <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
<span class="n">bkv_t</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">bkv_t</span><span class="p">)</span>      <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that bq and bk now have different shapes:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bk</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bq</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 96, 64, 256]), torch.Size([64, 96, 32, 256]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="dot-product-attention">dot product attention<a class="anchor-link" href="#dot-product-attention"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bnsd,bnzd-&gt;bnsz&#39;</span><span class="p">,</span> 
                            <span class="n">bq</span><span class="p">,</span>                  <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>
                            <span class="n">bk</span>                   <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
                           <span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>     <span class="c1"># dots: [bs, n_chunks, chunk_size, chunk_size*2]</span>
<span class="n">dots</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/paper_4.png" alt="paper_4.png"></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">self_mask</span> <span class="o">=</span> <span class="n">bq_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">bkv_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">self_mask</span><span class="o">.</span><span class="n">shape</span>      <span class="c1"># [bs, n_chunks, chunk_size, chunk_size*2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We achieve this by comparing the sorted and chunked buckets index with each other for <code>k</code> and <code>q</code> respectively. If they are equal we mask them.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">bkv_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:][</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([ 1,  6, 13, 16, 20, 34, 42, 45, 51, 52]),
 tensor([ 1,  6, 13, 16, 20, 34, 42, 45, 51, 52]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">self_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:</span><span class="mi">5</span><span class="p">,:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ True, False, False, False, False],
        [False,  True, False, False, False],
        [False, False,  True, False, False],
        [False, False, False,  True, False],
        [False, False, False, False,  True]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">self_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">self_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([32, 64]), tensor(34))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">self_mask</span><span class="p">,</span> <span class="n">TOKEN_SELF_ATTN_VALUE</span><span class="p">)</span>
<span class="k">del</span> <span class="n">self_mask</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Avoid-double-counting-of-query-key-pairs">Avoid double counting of query-key pairs<a class="anchor-link" href="#Avoid-double-counting-of-query-key-pairs"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>According to the appendix of the paper we need to avoid double counting q/k pairs over multiple rounds of hashing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/reformer_fastai/images/paper_5.png" alt="paper_5.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The lucidrains <a href="https://github.com/lucidrains/reformer-pytorch/blob/5eb10786dee3fcdb0092a3d8cc415665f0bbdb14/reformer_pytorch/reformer_pytorch.py#L354">implementation</a> mentions two strategies to deal with this, where only the first one is implemented, and seems to align with the paper:<br>
<em>The default is to count how many times a query-key pair is repeated, and to lower its log-prob correspondingly at each repetition</em></p>
<p><strong>Note that this implementation does not fold this term into the mask (like the paper), but calculates it separately.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>The goal is to consider each attention chunk (i.e. our input sorted by hash bucket id, and sequence id, and split in to n_chunks). For each chunk we have to assess:did a one particular key/query pair end up in similar attention chunks in a later hash round. If so we can argue that the model might "over focus" on this particular q/k pair, and that regularizing it (penalizing) probably makes sense.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First of all, we will be subracting the counts from <code>dots</code>, so they should have a similar shape in the end. We also expect the max count to be 6 (<code>n_rounds</code>).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># [bs, n_chunk, chunk_size, chunk_size*2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider item [0,0] in the first attention chunk (in eg. the 0th sample of the batch dimension). It ended up here because of the sorting. I.e. it must have had bucket number 0, and a pretty low sequence id (let's call it <code>x_id</code>) to end up as the first item (in this particular round). But in the next hash rounds it will get a different bucket id, and thus a different location in the sorting for that particular round. We have to associate the item from the input sequence,<code>x_id</code>, with our item[0,0] from the first hash round, and track where this item (<code>x_id</code>) in the original input sequence ended up in later hash rounds.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also note that it dosen't have to be identical attention chunks, just similar. So if [k32, q34] end up in the attention chunk 44 in the first round, 88 in the second, but 44 and 33 in the third, our count will be 2.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we need to create an index which tells us at which attention chunk a particular item in the input end up in in each hash round. Note that <code>undo_sort</code> is the key to take our sorted vectors back to original order. Eg. <code>undo_sort[0,0]</code> tells us where in the original sequence the item that ended up as item 0 after sorting belongs:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">undo_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">undo_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="p">:</span><span class="n">sl</span><span class="o">+</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([174,   0, 175, 447,  69, 148,   1,  38, 176, 110]),
 tensor([974, 775, 655, 710, 550, 776, 737, 777, 656, 657]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>undo_sort</code> tells us the sequence id where the sorted items will move to. Consider item 50 in our sorted sequences. It comes from various spots in the original input sequence in each of the 6 hash rounds, depending on which bucket it happend to fall into:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">undo_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">s_id</span> <span class="o">+</span> <span class="mi">50</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">s_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="n">sl</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[366, 940, 1368, 1631, 2483, 2635]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we take the mod of sl, we get the item's location <em>within</em> the original sequence at each hash round. And after sorting they end up at index 50.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">undo_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">s_id</span> <span class="o">+</span> <span class="mi">50</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">%</span><span class="k">sl</span> for s_id in range(0,sl*6, sl)]
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[366, 428, 344, 95, 435, 75]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we instead take the mod of <code>chunk_size</code> (32 in this case), we get an index of which of the 96 chunks (<code>n_rounds*n_buckets</code>) the items ends up in. That means that the 50th element in the sorted sequences will end up in the following attention chunks:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">undo_sort</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">s_id</span> <span class="o">+</span> <span class="mi">50</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">//</span><span class="mi">32</span> <span class="k">for</span> <span class="n">s_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="n">sl</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[11, 29, 42, 50, 77, 82]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can do this for all items, and thus create a <code>locs1</code> id that gives us an item's attention chunk id.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">locs1</span> <span class="o">=</span> <span class="n">undo_sort</span> <span class="o">//</span> <span class="n">bq_t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># same as chunk size</span>
<span class="n">locs1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">locs1</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">locs1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 3072]), tensor(0), tensor(95))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Eg. items 0-5 in the sorted sequences end up in various attention chunks depending on which hash round we are in:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">locs1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">slen</span><span class="p">:</span><span class="n">slen</span><span class="o">+</span><span class="mi">5</span><span class="p">]</span> <span class="k">for</span> <span class="n">slen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="o">*</span><span class="n">n_rounds</span><span class="p">,</span> <span class="n">sl</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[tensor([ 5,  0,  5, 13,  2]),
 tensor([30, 24, 20, 22, 17]),
 tensor([44, 42, 32, 36, 40]),
 tensor([49, 55, 50, 59, 53]),
 tensor([72, 74, 77, 78, 68]),
 tensor([85, 92, 88, 83, 94])]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We divided attention in even chunks, so bucket_id and attention chunk id won't always match:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="p">:</span><span class="n">sl</span><span class="o">+</span><span class="mi">5</span><span class="p">],</span> <span class="n">locs1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="p">:</span><span class="n">sl</span><span class="o">+</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([30, 24, 20, 22, 17]), tensor([30, 24, 20, 22, 17]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we create an id that is offset by one, except when it overflows our maximum id of 95, by taking the mod. We need this since our <code>keys</code> will be twice the size of the <code>queries</code>, and come from the neighbouring chunk.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="mi">95</span><span class="o">%</span><span class="k">n_chunks</span>, 96%n_chunks, 97%n_chunks
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(95, 0, 1)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">locs2</span> <span class="o">=</span> <span class="p">(</span><span class="n">locs1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_chunks</span>
<span class="n">locs2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 3072])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This means that the combination of locs1 and 2 gives us ids to neighbour chunks.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">locs1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">15</span><span class="p">:],</span> <span class="n">locs2</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">15</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([88, 89, 91, 89, 92, 89, 87, 82, 90, 87, 82, 94, 80, 94, 88]),
 tensor([89, 90, 92, 90, 93, 90, 88, 83, 91, 88, 83, 95, 81, 95, 89]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we reshape <code>locs1</code> and <code>locs2</code> to [bs, n_rounds, sl], concatenate them along the <code>n_rounds</code> dim, and switches the <code>n_rounds</code> and <a href="/reformer_fastai/experiment-script.html#sl"><code>sl</code></a> axis.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     locs1 = buckets * chunk_size + locs1</span>
<span class="c1">#     locs2 = buckets * chunk_size + locs2</span>

<span class="n">locs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">locs1</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">sl</span><span class="p">)),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">locs2</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">sl</span><span class="p">)),</span>
<span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">locs</span><span class="o">.</span><span class="n">shape</span>           <span class="c1"># [bs, sl, n_rounds*2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 512, 12])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This means that the item that happende to be first in the sorted sequences ends up in various chunks depending on which hash bucket it ends up in in each round since this will affect it's sorting. The last half of the list is the neighbour chunks to our item:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 5, 30, 44, 49, 72, 85,  6, 31, 45, 50, 73, 86])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is just a reshaped version of <code>locs1</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">locs1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">slen</span><span class="p">]</span> <span class="k">for</span> <span class="n">slen</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">sl</span><span class="o">*</span><span class="n">n_rounds</span><span class="p">,</span> <span class="n">sl</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[tensor(5), tensor(30), tensor(44), tensor(49), tensor(72), tensor(85)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>But we have to get the attention chunk id for the original sequence order</strong> We can achieve this by looking up the <code>locs</code> according to <code>st</code>. <code>st</code> is our chunking key, and we used it to reorder our input <code>kq</code> and <code>v</code> into the sorted order. We can now reorder locs in the same way:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">slocs</span> <span class="o">=</span> <span class="n">batched_index_select</span><span class="p">(</span><span class="n">locs</span><span class="p">,</span> <span class="n">st</span><span class="p">)</span>
<span class="n">slocs</span><span class="o">.</span><span class="n">shape</span>    <span class="c1"># [bs, sl*n_rounds, n_rounds*2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 3072, 12])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now have the <strong>original</strong> order. This means that the first item in the <em>original input</em> sequence ends up in the following chunks (+ neighbours).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">slocs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 0, 24, 42, 55, 74, 92,  1, 25, 43, 56, 75, 93])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='Above we used the <code>s</code> prefix to denote the result after sorting. This time we have sorted the sorted sequence (and is back to the original order...)' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We reshape <code>slocs</code> to include the <code>n_chunks</code> dimension:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b_locs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">slocs</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_rounds</span><span class="p">))</span>
<span class="n">b_locs</span><span class="o">.</span><span class="n">shape</span>          <span class="c1"># [bs, n_chunks, chunk_size, n_round * 2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 12])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We read this as the 0th element in the batch, the 0th attention block, and the 0th element. Which attention blocks did that particular element end up in in subsequent hash rounds (+ neighbours):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 0, 24, 42, 55, 74, 92,  1, 25, 43, 56, 75, 93])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the first element in chunck id 55, we get the following. Note that chunk 55 comes from the 3rd hash round (55//n_buckets).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 3, 24, 32, 55, 77, 90,  4, 25, 33, 56, 78, 91])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we slice the first half of <code>b_locs</code>, and add unit axis at -2. This is for later comparison purposes.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b_locs1</span> <span class="o">=</span> <span class="n">b_locs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:</span><span class="n">n_rounds</span><span class="p">]</span>
<span class="n">b_locs1</span><span class="o">.</span><span class="n">shape</span>     <span class="c1"># [bs, n_chunks, chunk_size, 1, n_rounds]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 1, 6])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We copy <code>b_locs1</code> along the 1 axis. <strong>This will be our query indexes</strong> Remember that we have 2x keys compared to queries.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_locs</span> <span class="o">=</span> <span class="n">b_locs1</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">b_locs</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">))</span>
<span class="n">bq_locs</span><span class="o">.</span><span class="n">shape</span>    <span class="c1"># [bs, n_chunks, chunk_size, 2, n_rounds ]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 2, 6])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">bq_locs</span><span class="p">[:,:,:,</span><span class="mi">0</span><span class="p">,:]</span><span class="o">==</span><span class="n">bq_locs</span><span class="p">[:,:,:,</span><span class="mi">1</span><span class="p">,:])</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([True])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>bq_locs is now "doubled" in size, so it should be suitable to compare to the keys chunk locations:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0, 24, 42, 55, 74, 92],
        [ 0, 24, 42, 55, 74, 92]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we reshape to [bs, n_chunks, chunk_size, n_rounds*2]</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_locs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bq_locs</span><span class="p">,</span> <span class="n">b_locs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">bq_locs</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># [bs, n_chunks, chunk_size, n_rounds*2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 12])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This means that <code>bq_locs</code> is similar to our <code>b_locs</code> except for the final half:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">b_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([ 0, 24, 42, 55, 74, 92,  0, 24, 42, 55, 74, 92]),
 tensor([ 0, 24, 42, 55, 74, 92,  1, 25, 43, 56, 75, 93]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>To recap:<code>bq_locs</code> let's us inspect each attention chunk to see which attention chunk one particular query ended up in at later hash rounds:</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we need our <code>k</code> (similar to <code>v</code>) chunk ids. We use our <code>b_locks</code> and append the previous chunk, similar to the attention calculation.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bkv_locs</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">b_locs</span><span class="p">)</span>
<span class="n">bkv_locs</span><span class="o">.</span><span class="n">shape</span>      <span class="c1"># [bs, n_chunks, chunk_size*2, n_rounds]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 64, 12])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that we appended the chunk along the -2 dimension:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b_locs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 12])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our key locs are offset by one in the second half to reflect the neighbour chunk:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bkv_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 0, 24, 42, 55, 74, 92,  1, 25, 43, 56, 75, 93])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The final step is to count elements that have ended up in the same chunk, and thus can attend to each other. Let's try for the first chunk:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_locs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bkv_locs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 96, 32, 12]), torch.Size([64, 96, 64, 12]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In attention chunk 0, key 3 and query 24 has landed in the following chunks over the rounds, with 1 match:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bkv_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,:],</span> <span class="n">bq_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,:],</span> <span class="p">(</span><span class="n">bkv_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,:]</span><span class="o">==</span><span class="n">bq_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,:])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([ 0, 17, 35, 48, 64, 80,  1, 18, 36, 49, 65, 81]),
 tensor([ 0, 23, 38, 56, 67, 80,  0, 23, 38, 56, 67, 80]),
 tensor(2))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But by adding appropriate unit axis, we can make sure that the sl dimensions don't overlap and broadcast's along each other:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">bq_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> 
 <span class="n">bkv_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> 
 <span class="p">(</span><span class="n">bq_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">bkv_locs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([32, 1, 12]), torch.Size([1, 64, 12]), torch.Size([32, 64, 12]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We compare all elements:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dup_counts</span> <span class="o">=</span> <span class="p">(</span><span class="n">bq_locs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">bkv_locs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
<span class="n">dup_counts</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># [bs, n_chunks, chunk_size, chunk_size*2, n_rounds]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64, 12])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dup_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">24</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ True, False, False, False, False,  True, False, False, False, False,
        False, False])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And sum across the last dimension:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tmp</span> <span class="o">=</span> <span class="n">dup_counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tmp</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For memory considerations the summation is chunked</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dup_counts</span> <span class="o">=</span> <span class="n">chunked_sum</span><span class="p">(</span><span class="n">dup_counts</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="p">(</span><span class="n">n_rounds</span> <span class="o">*</span> <span class="n">bs</span><span class="p">))</span>
<span class="n">dup_counts</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># [bs, n_chunks, chunk_size, chunk_size * 2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But it's the same as summing along -1:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">tmp</span><span class="o">==</span><span class="n">dup_counts</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dup_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">,:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[6, 3, 2, 1, 1, 2, 1, 1, 1, 1],
        [1, 6, 3, 3, 1, 1, 1, 2, 1, 2],
        [2, 3, 6, 3, 1, 1, 1, 2, 3, 1],
        [1, 2, 3, 6, 2, 1, 1, 1, 2, 1],
        [1, 1, 1, 2, 6, 1, 1, 1, 2, 1],
        [2, 2, 1, 1, 1, 6, 3, 2, 1, 3],
        [1, 1, 1, 1, 1, 3, 6, 2, 1, 3],
        [1, 2, 2, 1, 1, 2, 2, 6, 2, 4],
        [1, 1, 3, 2, 1, 2, 1, 2, 6, 1],
        [2, 2, 1, 2, 1, 1, 2, 3, 1, 6]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that the diagonal in the first block equals number of hash rounds. This makes sense sinse i,i pairs are indentical and always come in the same chunk - we will mask them later. But this is not the case of the other half of the keys (from neighbour chunk):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dup_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">:</span><span class="mi">42</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[1, 2, 0, 0, 0, 2, 0, 2, 0, 0],
        [2, 0, 1, 0, 0, 1, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 1, 1, 2, 0, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 2, 0, 0, 0, 2, 2],
        [0, 0, 0, 1, 0, 1, 0, 1, 1, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [1, 0, 1, 1, 0, 1, 1, 2, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 1, 1, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Dup_counts and dots should now be of same shapes:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">dup_counts</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">dots</span><span class="o">.</span><span class="n">shape</span>
<span class="n">dots</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dup_counts</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 96, 32, 64]), torch.Size([64, 96, 32, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally we extract the log values of our duplicate counts from the self attention value <code>dots</code> as described in the paper's appendix:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span> <span class="o">=</span> <span class="n">dots</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">dup_counts</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">del</span> <span class="n">dup_counts</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Pad-masking">Pad masking<a class="anchor-link" href="#Pad-masking"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">masked_value</span> <span class="o">=</span> <span class="n">max_neg_value</span><span class="p">(</span><span class="n">dots</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can pass along pad masks if our input is of variable length and padded. We don't wan't the model to see the pad token. Since our original input has been sorted and chunked, we have to sorte the masks similar:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">sl</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">input_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In case the input_mask is is shorter than input, this pads to appropriate shape with the value True:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#if input_mask is not None:</span>
<span class="n">input_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sl</span> <span class="o">-</span> <span class="n">input_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">value</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">input_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 512])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We reorder the sequence with <code>st</code> and reshapes to include the <code>n_chunks</code> dimension. This gives us the mask for <code>q</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mq</span> <span class="o">=</span> <span class="n">input_mask</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">st</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">mq</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The mask for <code>k</code> is similar, but includes the neighbour chunk as usual.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mkv</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">mq</span><span class="p">)</span>
<span class="n">mkv</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We add unit axis for comparision across the final dimensions of <code>q</code> and <code>k</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">mq</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">mkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="n">masked_value</span><span class="p">)</span>
<span class="n">dots</span>
<span class="c1">#del mask</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[[-5.0002e+04, -1.0404e+00, -6.2487e-01,  ...,  1.0480e-02,
           -5.2455e-02,  2.0620e+01],
          [ 5.5138e-02, -5.0002e+04, -1.2008e+00,  ...,  2.0702e+01,
            2.0725e+01, -7.3569e-01],
          [-6.2263e-01, -1.2101e+00, -5.0002e+04,  ...,  2.0632e+01,
            2.0816e+01,  2.0912e+01],
          ...,
          [-3.6686e-02, -6.8328e-01, -9.6327e-01,  ...,  2.0708e+01,
            2.0822e+01,  2.0851e+01],
          [-6.7548e-01,  4.3852e-02, -7.3175e-01,  ...,  2.0624e+01,
            2.0718e+01,  2.0613e+01],
          [-6.8309e-01,  4.6035e-02, -3.3323e-02,  ..., -9.9436e-01,
            1.6633e-01,  2.0727e+01]],

         [[-5.0002e+04,  7.7878e-02, -6.6826e-01,  ..., -5.6195e-01,
           -2.5396e-02, -3.7712e-02],
          [ 7.5854e-02, -5.0002e+04,  5.5370e-02,  ...,  7.3007e-02,
            5.5364e-02, -1.1282e+00],
          [ 2.6058e-02,  5.9516e-02, -5.0002e+04,  ...,  7.8917e-02,
           -1.1041e-01, -2.5485e-02],
          ...,
          [-7.1051e-01,  2.7179e-02, -7.4234e-01,  ...,  1.7444e-02,
           -7.4681e-01,  8.4078e-02],
          [ 3.5303e-02,  3.0089e-02,  1.8635e-02,  ...,  3.3148e-02,
           -1.0739e+00,  4.6189e-02],
          [-7.5039e-01, -6.1051e-01,  6.3676e-02,  ..., -6.9512e-02,
            2.4852e-02,  3.3326e-02]],

         [[-5.0002e+04,  1.1893e-01, -3.1758e-02,  ..., -6.8121e-02,
           -6.4720e-01, -8.7898e-02],
          [ 1.2217e-01, -5.0002e+04, -1.3385e-02,  ...,  9.2867e-02,
            1.5681e-01, -6.3884e-02],
          [-3.1093e-02, -1.2758e-02, -5.0002e+04,  ...,  1.1031e-01,
           -7.0500e-01, -6.7539e-01],
          ...,
          [-6.4484e-02, -1.0909e+00,  7.4930e-03,  ...,  3.0774e-02,
            6.0454e-02, -1.0151e+00],
          [ 4.0968e-02,  3.0282e-02,  1.4719e-01,  ..., -7.6498e-02,
           -1.0360e+00, -5.6562e-01],
          [-1.1170e+00, -6.8563e-01, -1.0466e+00,  ..., -4.8035e-02,
            8.5094e-02, -6.5407e-02]],

         ...,

         [[-5.0002e+04, -9.9101e-01, -6.7734e-01,  ..., -9.2610e-01,
           -1.2709e+00, -7.3165e-01],
          [ 9.9512e-02, -5.0002e+04,  3.1806e-02,  ..., -7.6206e-01,
           -5.8745e-01,  9.7768e-03],
          [-1.0824e+00, -6.5788e-01, -5.0002e+04,  ..., -5.9225e-01,
           -6.3531e-01, -7.3157e-01],
          ...,
          [-1.1562e-03,  7.0130e-03, -6.1178e-01,  ..., -1.1010e-03,
            4.2915e-02,  5.8910e-02],
          [-7.2236e-02, -7.0673e-01, -1.6531e-02,  ..., -1.2631e-02,
            4.9684e-02, -1.0097e+00],
          [-1.4670e+00, -7.9756e-01, -1.0041e+00,  ...,  4.1251e-02,
           -5.4923e-01,  1.1196e-01]],

         [[-5.0002e+04, -2.2864e-02, -7.0810e-01,  ..., -8.8877e-02,
           -3.5036e-02,  8.6752e-02],
          [-2.3829e-02, -5.0002e+04, -7.4363e-02,  ..., -7.0153e-01,
           -5.9906e-01,  8.5919e-02],
          [-1.5706e-02, -7.4920e-02, -5.0002e+04,  ..., -7.0312e-01,
           -3.6608e-03, -8.4188e-02],
          ...,
          [-6.4937e-01, -7.5320e-01, -6.7659e-01,  ..., -1.9999e-02,
           -3.0025e-02, -9.6157e-02],
          [-7.8045e-02, -2.7767e-02, -3.8730e-02,  ..., -6.3076e-02,
            2.4677e-02, -1.0538e+00],
          [-3.8923e-02, -1.2101e+00, -6.1684e-01,  ..., -7.3212e-02,
           -4.1560e-02, -7.2177e-03]],

         [[-5.0002e+04,  1.5744e-02, -3.6656e-02,  ...,  7.9805e-03,
            6.9037e-02, -6.4532e-01],
          [ 1.6047e-02, -5.0002e+04, -3.9217e-02,  ..., -6.0678e-02,
           -1.0407e-01, -6.9506e-01],
          [-1.1336e+00, -3.6765e-02, -5.0002e+04,  ...,  3.8987e-02,
           -1.0660e+00, -9.1082e-02],
          ...,
          [-6.3117e-01,  1.7853e-03, -6.1776e-01,  ..., -6.1914e-01,
           -6.6772e-01, -8.4740e-02],
          [-1.0818e+00, -7.2100e-01,  6.8703e-03,  ..., -6.4826e-01,
            5.2051e-02,  8.0880e-02],
          [-1.0235e+00,  8.3285e-03, -7.2880e-01,  ..., -6.3390e-01,
           -6.5563e-01, -9.3100e-01]]],


        [[[-5.0002e+04, -1.0287e+00,  6.8346e-03,  ...,  2.0748e+01,
           -7.4605e-01,  2.0731e+01],
          [-1.0326e+00, -5.0002e+04,  9.4427e-02,  ...,  2.0679e+01,
            2.0148e-03, -7.3524e-02],
          [ 6.7672e-03,  9.8932e-02, -5.0002e+04,  ...,  2.0788e+01,
            2.0776e+01,  2.0755e+01],
          ...,
          [-6.8427e-01, -7.6995e-01,  6.8449e-03,  ..., -6.7159e-01,
            2.0742e+01, -1.8058e-02],
          [ 1.0916e-01, -7.6654e-02, -7.8409e-01,  ..., -7.7711e-01,
            2.0800e+01,  6.0956e-03],
          [-4.2894e-02, -7.6302e-01, -6.0873e-01,  ...,  1.2333e-01,
            4.8868e-02, -6.7437e-01]],

         [[-5.0002e+04, -7.4121e-01,  6.8539e-03,  ..., -7.7586e-01,
           -1.2751e-01, -6.7393e-01],
          [-7.4487e-01, -5.0002e+04, -5.8602e-01,  ..., -1.0587e+00,
           -6.9275e-01,  9.9422e-02],
          [ 7.3751e-03, -9.9150e-01, -5.0002e+04,  ..., -1.0861e+00,
           -9.0756e-02, -7.3040e-01],
          ...,
          [-1.1303e+00,  1.2959e-01, -1.3156e-01,  ..., -8.1061e-03,
           -5.8722e-01,  1.6247e-02],
          [-2.1086e-02,  4.1514e-02,  4.0395e-04,  ..., -1.0630e+00,
           -1.5798e+00, -6.9583e-01],
          [-6.8207e-01, -6.7544e-01, -1.0706e+00,  ..., -6.4096e-01,
           -2.5246e-02, -4.0463e-02]],

         [[-5.0002e+04, -6.2382e-01, -7.4519e-01,  ...,  1.8341e-02,
           -7.4715e-01, -7.3090e-01],
          [-6.2022e-01, -5.0002e+04, -1.0710e+00,  ..., -6.7639e-01,
           -5.5210e-02, -8.7723e-03],
          [-5.8139e-02, -1.3569e+00, -5.0002e+04,  ..., -1.3842e-01,
           -5.7647e-02, -7.6138e-01],
          ...,
          [-1.2874e+00, -3.7172e-02, -1.1971e-01,  ..., -1.1135e+00,
           -5.5599e-01,  4.4560e-02],
          [-6.2956e-01, -6.5243e-01, -6.1504e-01,  ...,  3.3182e-02,
           -1.3783e-02, -6.9928e-01],
          [-1.0058e-02, -6.9272e-01, -7.2070e-01,  ..., -6.2283e-01,
           -6.7011e-01, -1.2668e+00]],

         ...,

         [[-5.0002e+04,  1.7537e-02, -7.5630e-02,  ..., -6.0353e-01,
           -4.8025e-03, -6.2234e-01],
          [ 1.6636e-02, -5.0002e+04, -6.2238e-01,  ..., -6.3683e-01,
           -6.7922e-01, -9.3318e-02],
          [-7.6349e-02, -6.1783e-01, -5.0002e+04,  ..., -6.6223e-01,
            5.2067e-02, -7.7739e-01],
          ...,
          [-7.0994e-01,  3.1005e-02, -5.0515e-02,  ...,  2.2233e-02,
            1.0445e-02, -6.2186e-01],
          [-4.8771e-02, -8.4967e-02, -7.1187e-02,  ..., -1.0790e+00,
            2.7524e-02, -1.0127e+00],
          [-7.0821e-01, -1.0360e+00, -1.1156e+00,  ..., -1.0150e+00,
            3.9711e-02,  4.6916e-02]],

         [[-5.0002e+04, -6.6504e-01, -7.0476e-01,  ..., -1.8005e-02,
           -6.4876e-01, -2.3274e-02],
          [ 2.9005e-02, -5.0002e+04, -4.0386e-02,  ...,  2.6925e-02,
            1.0708e-02, -1.1037e+00],
          [-7.0423e-01, -3.7335e-02, -5.0002e+04,  ..., -8.9376e-02,
           -5.9300e-01, -7.5755e-01],
          ...,
          [-1.3852e-01,  4.8598e-02, -1.0691e+00,  ..., -7.3187e-01,
           -1.1090e+00,  8.3399e-02],
          [ 4.7208e-02, -5.5907e-01, -1.0945e+00,  ...,  9.3991e-03,
           -1.1883e-01, -1.1545e+00],
          [-6.1052e-01, -6.6075e-01, -6.6978e-02,  ..., -6.1715e-02,
           -4.9534e-02, -3.5774e-02]],

         [[-5.0002e+04, -1.1524e+00, -3.8361e-03,  ...,  8.2180e-02,
            7.1498e-02, -1.0261e+00],
          [-1.1526e+00, -5.0002e+04, -6.5538e-01,  ...,  9.0787e-02,
           -2.6950e-02,  8.9988e-02],
          [-3.2501e-03, -6.6129e-01, -5.0002e+04,  ..., -7.3576e-02,
           -6.7899e-01, -6.6380e-01],
          ...,
          [ 3.9641e-02, -1.8851e-02, -8.0709e-02,  ..., -1.0730e+00,
            1.0194e-01, -6.4881e-02],
          [-7.9839e-01, -5.5824e-01, -8.1107e-01,  ..., -6.1297e-01,
            1.9333e-02,  1.6730e-02],
          [-7.0130e-02,  6.4639e-03, -6.0883e-01,  ..., -6.6599e-01,
           -8.3970e-02, -1.4761e-02]]],


        [[[-5.0002e+04, -7.0643e-01,  1.8945e-02,  ...,  2.0638e+01,
            2.0691e+01,  2.0777e+01],
          [-7.0638e-01, -5.0002e+04,  3.6565e-02,  ..., -7.9589e-01,
            2.0858e+01, -2.3841e-04],
          [ 1.7914e-02,  3.4687e-02, -5.0002e+04,  ...,  2.0742e+01,
            2.0621e+01,  2.0714e+01],
          ...,
          [ 4.6797e-02, -6.9868e-01,  8.5909e-02,  ...,  2.0669e+01,
            7.8236e-02,  2.0897e+01],
          [ 3.1428e-02, -7.2612e-01, -8.3142e-03,  ..., -6.7803e-01,
            2.0717e+01,  4.5941e-02],
          [-1.0209e+00, -5.9870e-01,  2.0145e-02,  ...,  2.0809e+01,
           -9.1042e-03,  2.0818e+01]],

         [[-5.0002e+04, -1.1261e+00, -6.7612e-01,  ..., -7.1700e-01,
            2.3992e-02, -6.2569e-01],
          [-7.2246e-01, -5.0002e+04,  6.1423e-02,  ..., -6.6325e-01,
            7.7635e-02, -7.4096e-01],
          [-6.7703e-01, -6.3860e-01, -5.0002e+04,  ...,  3.2460e-02,
           -7.8134e-01, -1.4316e-02],
          ...,
          [ 5.3191e-02,  2.2016e-02,  1.4697e-02,  ..., -1.0586e+00,
           -8.0766e-01, -8.0925e-01],
          [-5.9089e-01, -1.0910e+00, -1.0396e-02,  ...,  4.3137e-02,
           -3.1703e-02,  1.9830e-01],
          [-2.9503e-02,  3.5661e-02, -1.0629e-01,  ..., -1.1272e+00,
            1.3117e-01, -1.5163e-02]],

         [[-5.0002e+04, -7.1261e-04, -1.0960e+00,  ...,  4.6192e-02,
            3.9001e-02, -6.4270e-02],
          [-7.2621e-04, -5.0002e+04, -5.9984e-01,  ..., -1.0438e+00,
           -5.8060e-01,  1.7381e-02],
          [-1.0961e+00, -6.0384e-01, -5.0002e+04,  ..., -7.6103e-01,
            3.6515e-02, -7.4593e-01],
          ...,
          [ 3.7372e-02, -7.8854e-01, -6.5888e-01,  ..., -9.8734e-01,
           -1.5392e-02, -4.7270e-02],
          [ 1.0730e-01,  6.2754e-02, -1.1074e+00,  ..., -7.4467e-01,
            2.4133e-02, -1.3177e+00],
          [-1.6706e-02, -6.1856e-02,  6.7825e-02,  ..., -6.8134e-01,
           -6.5658e-01, -6.5555e-01]],

         ...,

         [[-5.0002e+04, -7.6860e-01, -3.9312e-02,  ..., -7.2845e-01,
            6.8510e-02,  6.1583e-02],
          [-1.4567e+00, -5.0002e+04,  2.9249e-02,  ..., -1.0235e+00,
           -6.3884e-02, -7.4344e-01],
          [-3.9045e-02,  3.1126e-02, -5.0002e+04,  ...,  4.3207e-02,
            6.4832e-02, -7.6816e-01],
          ...,
          [-7.0602e-01, -1.0803e+00, -6.8090e-03,  ..., -6.6567e-01,
            3.3728e-02, -5.9617e-01],
          [-1.1273e+00, -6.8315e-01, -6.5739e-01,  ..., -6.4077e-02,
           -8.0328e-01, -6.8134e-01],
          [-6.8594e-01, -4.0942e-02, -1.1023e+00,  ...,  1.3437e-01,
            1.2066e-01, -6.2484e-02]],

         [[-5.0002e+04, -6.6004e-02,  7.6812e-02,  ..., -1.7134e-02,
           -2.7026e-02, -7.5400e-01],
          [-6.6043e-02, -5.0002e+04, -4.1537e-02,  ..., -4.9382e-02,
           -6.3732e-01, -3.7253e-02],
          [ 7.1348e-02, -7.3171e-01, -5.0002e+04,  ...,  5.1813e-02,
           -9.0516e-02, -2.4009e-02],
          ...,
          [-1.0931e-01, -6.2518e-01, -7.7963e-01,  ..., -3.7894e-02,
           -8.5473e-02, -7.6068e-01],
          [-8.2336e-02,  2.4009e-02, -1.1192e+00,  ..., -6.7947e-01,
           -2.0608e-02,  1.7448e-02],
          [-9.8855e-01,  1.2772e-02, -7.5337e-01,  ..., -2.1447e-04,
            1.0220e-02, -6.5317e-01]],

         [[-5.0002e+04, -7.0534e-01, -1.0543e+00,  ...,  3.8087e-02,
            3.0223e-02, -1.1817e-02],
          [-1.1970e-02, -5.0002e+04,  6.5715e-02,  ..., -7.4209e-03,
            2.7954e-02, -1.1809e+00],
          [-6.5327e-01,  6.0199e-02, -5.0002e+04,  ...,  2.2354e-02,
           -6.1508e-01,  1.7708e-02],
          ...,
          [ 1.3893e-02, -1.6235e+00,  2.4286e-02,  ..., -6.4143e-01,
           -1.5737e-01, -1.1110e+00],
          [-1.1153e+00, -3.6544e-02, -1.9669e-02,  ..., -6.0147e-03,
           -6.2389e-01, -6.8890e-01],
          [-1.0333e+00, -7.9172e-01, -6.5686e-01,  ..., -6.1925e-01,
           -6.6389e-02,  3.2094e-02]]],


        ...,


        [[[-5.0002e+04,  5.2400e-02,  4.0179e-02,  ..., -7.2231e-01,
            2.0599e+01,  2.0778e+01],
          [ 5.0721e-02, -5.0002e+04, -7.6406e-01,  ...,  1.6523e-01,
            2.6825e-02,  2.0738e+01],
          [ 4.2541e-02, -1.4639e+00, -5.0002e+04,  ..., -6.9885e-01,
           -1.8322e-02,  2.0849e+01],
          ...,
          [ 1.0936e-02, -3.8569e-03,  3.8444e-02,  ...,  7.6564e-02,
           -3.5330e-02, -3.7757e-02],
          [ 1.5892e-02, -2.9148e-02, -8.3444e-02,  ...,  3.1712e-02,
            2.0789e+01,  2.0783e+01],
          [ 2.7258e-02, -1.1054e+00, -1.3504e+00,  ..., -7.6170e-01,
            2.0742e+01,  2.0647e+01]],

         [[-5.0002e+04,  1.3080e-03,  8.1217e-02,  ..., -1.0570e+00,
           -1.1043e+00,  3.2203e-02],
          [ 1.2573e-03, -5.0002e+04,  1.3325e-02,  ...,  1.9905e-02,
           -6.6156e-02,  1.3769e-02],
          [-6.2069e-01,  1.2368e-02, -5.0002e+04,  ..., -4.0843e-02,
            2.0159e-02, -5.7616e-01],
          ...,
          [-8.2025e-01, -4.8631e-02, -1.2133e+00,  ...,  1.0454e-02,
           -6.0751e-01, -6.9148e-01],
          [ 1.7746e-01,  3.9400e-02,  5.5643e-02,  ..., -1.0628e+00,
           -4.7920e-02, -7.6982e-01],
          [-2.8525e-02,  8.3297e-04, -7.1022e-01,  ..., -5.9549e-01,
           -1.2106e-01, -2.2919e-02]],

         [[-5.0002e+04, -1.1456e-02, -7.1848e-01,  ..., -3.9141e-02,
            5.7795e-02, -2.4669e-02],
          [-1.1195e-02, -5.0002e+04, -8.5146e-02,  ...,  6.2385e-02,
           -7.1808e-01,  9.2060e-02],
          [-2.7466e-02, -9.4474e-02, -5.0002e+04,  ..., -1.1161e+00,
           -6.8552e-02, -6.5372e-01],
          ...,
          [-6.9378e-01, -5.8283e-01, -7.2303e-01,  ..., -1.1824e-01,
           -6.9322e-01, -2.0603e-02],
          [-7.6447e-01,  5.2700e-02, -6.6736e-01,  ..., -4.6526e-02,
           -6.9072e-01, -6.5179e-01],
          [ 1.6763e-02, -7.0631e-01, -3.0940e-02,  ..., -3.0100e-02,
           -6.7885e-01, -1.6511e-01]],

         ...,

         [[-5.0002e+04, -5.5700e-03, -3.5600e-03,  ..., -1.1313e-01,
           -6.7945e-01,  5.3541e-02],
          [-5.8371e-03, -5.0002e+04, -6.8315e-01,  ..., -7.4106e-02,
           -1.7017e-01, -1.1960e-04],
          [-3.8548e-03, -6.8282e-01, -5.0002e+04,  ..., -4.0129e-02,
           -6.2009e-01,  1.3382e-02],
          ...,
          [ 3.6557e-02, -1.1954e+00, -5.5640e-01,  ...,  4.2153e-02,
           -6.4036e-01, -2.4292e-03],
          [-1.2519e-02, -1.1392e+00,  6.1505e-02,  ..., -6.0619e-01,
            7.6575e-02, -7.6867e-01],
          [-7.4817e-01, -7.0911e-01,  5.0159e-02,  ..., -7.8706e-02,
            1.3180e-02,  4.9392e-02]],

         [[-5.0002e+04,  1.0073e-02, -1.0350e+00,  ...,  1.0790e-02,
            7.2497e-02, -6.2705e-01],
          [-6.8405e-01, -5.0002e+04,  3.0868e-02,  ..., -8.5882e-04,
           -6.6511e-01, -6.5233e-01],
          [-1.0362e+00,  3.3509e-02, -5.0002e+04,  ..., -5.5677e-01,
            2.5940e-02, -8.0767e-02],
          ...,
          [-7.3452e-02, -6.2962e-02,  5.7764e-02,  ...,  3.3967e-02,
           -7.1557e-01,  9.3924e-03],
          [ 2.1821e-02,  1.0805e-02,  2.8415e-02,  ..., -6.7669e-01,
            2.5231e-02, -1.1014e+00],
          [-9.8223e-01, -4.6196e-02, -6.0867e-01,  ..., -9.9995e-01,
           -2.9909e-02, -6.2392e-01]],

         [[-5.0002e+04, -6.4706e-01,  9.6263e-03,  ...,  2.2836e-02,
           -1.0565e+00, -1.1900e+00],
          [ 5.6961e-02, -5.0002e+04,  1.5624e-01,  ..., -1.0936e-02,
           -7.5872e-01, -5.6278e-01],
          [ 1.1487e-02,  1.5085e-01, -5.0002e+04,  ...,  2.9795e-02,
           -7.2889e-01,  2.3513e-02],
          ...,
          [-5.9941e-03, -5.8503e-01, -7.1425e-01,  ..., -6.7522e-01,
           -2.6663e-02, -6.6142e-01],
          [ 1.3411e-02, -1.1007e+00,  2.1675e-02,  ...,  6.0577e-02,
           -3.4467e-02, -5.5980e-01],
          [-9.8314e-02, -1.0674e+00,  4.0643e-02,  ...,  7.4771e-02,
           -7.7645e-02, -1.0807e+00]]],


        [[[-5.0002e+04,  8.8070e-02, -7.7905e-01,  ...,  2.0628e+01,
            2.0665e+01, -7.6268e-01],
          [ 9.4692e-02, -5.0002e+04, -7.5558e-01,  ..., -7.6037e-02,
            2.0707e+01,  2.0690e+01],
          [-9.5035e-02, -1.1628e+00, -5.0002e+04,  ..., -8.3059e-03,
            2.0753e+01,  2.0778e+01],
          ...,
          [-1.0943e+00, -7.2827e-01, -8.2381e-02,  ...,  2.0671e+01,
            2.0755e+01, -7.5985e-01],
          [-6.7897e-01, -1.2315e-02,  1.4186e-01,  ...,  2.0691e+01,
            2.0799e+01,  3.2438e-02],
          [ 9.1267e-02, -5.0751e-03, -6.0676e-02,  ...,  2.0686e+01,
           -4.8605e-02,  2.0722e+01]],

         [[-5.0002e+04, -9.1360e-02,  3.6774e-02,  ..., -4.5187e-02,
           -9.9560e-01,  3.7443e-02],
          [-8.9719e-02, -5.0002e+04, -1.0807e+00,  ..., -6.1840e-01,
            1.1114e-01, -2.1489e-02],
          [ 3.6256e-02, -1.0807e+00, -5.0002e+04,  ..., -1.1177e+00,
           -1.0720e+00, -6.6439e-01],
          ...,
          [-6.5939e-01,  1.1382e-01, -5.5178e-01,  ..., -6.0889e-01,
           -9.7330e-01, -6.8352e-01],
          [-6.8595e-01,  1.1712e-01, -7.2646e-03,  ...,  8.4488e-02,
           -7.0226e-01, -7.3382e-01],
          [-1.0390e-02, -3.0717e-02, -7.7255e-02,  ..., -1.0751e+00,
           -1.0663e-01, -3.2555e-03]],

         [[-5.0002e+04, -7.7553e-01, -6.2018e-01,  ..., -7.3105e-01,
            2.0386e-02,  3.8393e-02],
          [-8.6760e-02, -5.0002e+04,  3.7554e-02,  ...,  1.1413e-01,
           -7.6764e-02, -1.0876e+00],
          [-6.2147e-01,  3.5030e-02, -5.0002e+04,  ..., -5.7204e-01,
           -7.1048e-01, -6.2546e-01],
          ...,
          [-6.8105e-01, -1.1926e+00,  4.4543e-03,  ...,  3.3833e-03,
            7.4808e-02, -7.4500e-01],
          [-1.0620e+00, -7.8984e-01, -7.3705e-01,  ..., -1.1305e+00,
           -6.3342e-03, -3.2360e-03],
          [ 5.8125e-02, -1.3288e-01, -5.9944e-01,  ..., -6.8650e-01,
           -7.2090e-01,  2.9946e-02]],

         ...,

         [[-5.0002e+04,  4.3551e-02, -1.0962e+00,  ..., -6.8317e-01,
           -7.8139e-01, -9.3639e-02],
          [ 4.6974e-02, -5.0002e+04,  4.3626e-03,  ..., -7.1435e-01,
           -4.5532e-02, -6.9462e-01],
          [-6.9041e-01,  4.4991e-03, -5.0002e+04,  ..., -1.3844e-02,
           -6.9408e-01, -8.3989e-01],
          ...,
          [ 3.9229e-02,  5.6270e-02, -6.7296e-01,  ..., -2.6244e-02,
            1.7100e-02, -6.7346e-01],
          [-7.3858e-01, -6.0334e-01,  4.6726e-04,  ...,  4.2693e-02,
            7.6321e-02, -1.1988e+00],
          [-5.0949e-03,  1.0270e-01,  1.0649e-01,  ..., -5.3066e-03,
            5.2132e-02,  1.8803e-03]],

         [[-5.0002e+04, -1.7331e-03, -8.4754e-01,  ..., -6.1742e-01,
           -7.2971e-01, -7.6575e-02],
          [-1.9064e-03, -5.0002e+04, -6.2878e-01,  ...,  6.9941e-02,
           -6.7593e-01,  3.5147e-02],
          [-1.5143e-01, -6.3576e-01, -5.0002e+04,  ..., -2.0979e-02,
           -5.4055e-03,  8.4016e-02],
          ...,
          [-7.5724e-01, -4.9299e-02, -6.8022e-01,  ..., -1.1208e-01,
           -6.7211e-01,  4.6864e-02],
          [-7.8377e-01, -7.7395e-01, -1.0539e+00,  ..., -2.9844e-02,
           -8.4063e-01,  1.3819e-01],
          [-9.6225e-02, -5.9687e-01,  7.6544e-02,  ..., -1.2057e-02,
            1.1503e-01, -7.0326e-01]],

         [[-5.0002e+04, -7.2567e-01, -6.8366e-02,  ..., -6.7452e-01,
           -7.7318e-01, -6.8616e-01],
          [-7.2622e-01, -5.0002e+04, -7.2297e-01,  ..., -1.0318e+00,
            2.7554e-02, -1.3620e+00],
          [-7.6179e-01, -1.1281e+00, -5.0002e+04,  ..., -1.0515e+00,
           -6.8863e-01, -1.1271e+00],
          ...,
          [-1.1460e-01, -5.9372e-01, -1.1546e+00,  ...,  2.2698e-02,
           -1.0151e+00, -6.9752e-01],
          [-7.4110e-01, -9.6526e-03,  2.2601e-02,  ..., -6.6647e-01,
           -6.2969e-01, -6.2080e-02],
          [-6.8643e-01, -5.5423e-03, -7.5430e-01,  ..., -9.9779e-01,
           -6.3925e-01,  2.6166e-02]]],


        [[[-5.0002e+04, -7.0775e-01, -7.4425e-01,  ...,  2.0819e+01,
            2.0683e+01,  2.0665e+01],
          [-1.4056e-02, -5.0002e+04, -1.0334e-01,  ...,  2.0730e+01,
           -5.0211e-03, -1.7995e-02],
          [-7.3941e-01, -9.7168e-02, -5.0002e+04,  ..., -6.9308e-01,
           -2.1837e-02,  2.0897e+01],
          ...,
          [-6.5692e-01,  1.7358e-01, -6.7614e-01,  ..., -1.1155e+00,
           -2.7077e-02,  2.0594e+01],
          [-3.2807e-02, -1.0641e+00, -6.9466e-01,  ...,  5.9205e-02,
            2.0657e+01,  7.3325e-02],
          [-7.0752e-01,  1.4747e-02, -1.1124e+00,  ...,  1.1635e-01,
            2.0619e+01,  2.0606e+01]],

         [[-5.0002e+04, -7.8155e-01,  4.2360e-02,  ..., -6.7046e-03,
           -6.2317e-01, -6.8764e-02],
          [-7.7636e-01, -5.0002e+04, -6.6514e-01,  ...,  1.6600e-02,
           -6.0266e-01,  6.9248e-02],
          [ 4.4054e-02, -6.6220e-01, -5.0002e+04,  ..., -4.0247e-02,
           -1.0447e+00, -2.4753e-02],
          ...,
          [-1.1650e+00, -6.5193e-01, -5.2799e-02,  ..., -7.1232e-01,
            6.2070e-02, -5.7036e-01],
          [ 6.6437e-02, -6.3776e-01, -6.2095e-01,  ..., -7.5672e-02,
           -7.2539e-01,  3.8178e-03],
          [-2.0841e-02, -7.0457e-01, -7.1138e-01,  ..., -8.5316e-01,
           -6.8849e-01, -1.1017e+00]],

         [[-5.0002e+04, -1.0052e-01, -1.4052e-02,  ..., -6.1361e-01,
           -7.6047e-01, -6.4266e-01],
          [-7.9946e-01, -5.0002e+04, -7.0874e-01,  ..., -1.3187e-02,
           -7.2766e-01, -6.8447e-01],
          [-1.4337e-02, -7.0819e-01, -5.0002e+04,  ...,  6.2653e-02,
            8.2925e-02, -6.1827e-03],
          ...,
          [ 5.5368e-02, -7.9218e-02, -4.9271e-02,  ..., -1.0849e+00,
           -1.1037e+00, -6.8339e-01],
          [-2.3466e-02, -1.2710e+00, -6.9268e-01,  ..., -1.2275e-01,
            3.8920e-02, -7.5020e-01],
          [ 4.9185e-03,  7.8125e-02, -1.0954e+00,  ...,  1.8172e-02,
           -9.7184e-03, -7.6607e-01]],

         ...,

         [[-5.0002e+04, -5.5086e-02,  7.3198e-03,  ..., -7.2712e-01,
           -6.9818e-01, -5.7679e-01],
          [-5.2186e-02, -5.0002e+04,  1.5717e-01,  ..., -6.6108e-01,
           -5.1954e-02, -7.7383e-02],
          [ 7.2771e-03,  1.6493e-01, -5.0002e+04,  ..., -1.7657e-02,
           -5.9511e-01, -8.8096e-02],
          ...,
          [ 5.3069e-03, -2.2161e-02, -1.0259e+00,  ..., -6.4013e-02,
           -1.1779e+00, -7.8668e-01],
          [ 1.0662e-01,  4.2206e-02, -5.7852e-01,  ..., -7.2492e-01,
           -6.3088e-01, -5.4970e-01],
          [-3.0151e-02,  1.5905e-01, -6.5035e-01,  ..., -5.0565e-02,
           -1.2373e-01, -1.1663e+00]],

         [[-5.0002e+04, -4.1103e-02, -2.3522e-02,  ..., -6.5312e-01,
           -5.2362e-06,  8.0098e-04],
          [-7.4138e-01, -5.0002e+04, -9.3284e-01,  ...,  7.8361e-02,
           -5.8864e-01, -7.6385e-01],
          [-1.1233e+00, -9.5003e-01, -5.0002e+04,  ..., -6.1614e-01,
           -5.3328e-02, -7.3725e-01],
          ...,
          [-1.4016e+00, -3.7399e-02, -7.3395e-01,  ..., -6.1674e-01,
           -6.8356e-02, -7.4714e-01],
          [-1.1417e+00, -6.6967e-01, -7.7276e-01,  ..., -6.3851e-01,
           -6.8503e-01, -6.3605e-01],
          [-4.3367e-02,  1.3107e-01, -6.3032e-01,  ...,  2.9499e-03,
           -7.3189e-01, -6.3194e-03]],

         [[-5.0002e+04,  1.6120e-01, -1.0919e+00,  ...,  3.7141e-02,
           -6.4391e-01, -6.6716e-01],
          [ 1.6676e-01, -5.0002e+04,  2.1009e-02,  ..., -1.4134e-02,
           -2.4726e-02,  1.4608e-01],
          [-6.8621e-01, -6.7221e-01, -5.0002e+04,  ...,  3.3562e-02,
           -1.3796e-02, -8.1103e-02],
          ...,
          [-5.9770e-01, -7.5384e-01, -6.6301e-01,  ..., -6.2909e-01,
            2.6888e-02, -6.9892e-01],
          [ 8.2479e-02,  1.5413e-02,  1.0300e-01,  ..., -7.2118e-01,
            2.8926e-02,  7.7242e-03],
          [-6.9020e-01,  7.0585e-02, -6.1188e-01,  ..., -5.4735e-01,
           -1.3714e-01, -2.1106e-02]]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Casual-masking">Casual masking<a class="anchor-link" href="#Casual-masking"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">del</span> <span class="n">mask</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Add casual masking to not allow the model to peek into future tokens. Mask if query comes after key in input sequence.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">55</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([220, 226, 255, 261, 268, 274, 290, 313, 316, 317, 322, 323, 338, 356,
        368, 407, 410, 437, 446, 447, 500,   1,   7,  24,  25,  27,  46,  47,
         73,  96, 104, 109])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bkv_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">55</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([220, 226, 255, 261, 268, 274, 290, 313, 316, 317, 322, 323, 338, 356,
        368, 407, 410, 437, 446, 447, 500,   1,   7,  24,  25,  27,  46,  47,
         73,  96, 104, 109, 283, 301, 302, 331, 337, 344, 361, 421, 432, 456,
        464, 465, 470, 476, 497, 501,   6,   9,  10,  11,  30,  32,  66,  69,
         72,  79,  90, 122, 157, 170, 190, 206])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#        if self.causal:</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">bq_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">bkv_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the tokens are sorted before masking, the masking is not triangular like in the base transformer: in this case all tokens after 220 should be masked (True):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">t</span><span class="p">,</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span><span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bkv_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">55</span><span class="p">,:],</span> <span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">0</span><span class="p">,:])]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(tensor(220), tensor(False)),
 (tensor(226), tensor(True)),
 (tensor(255), tensor(True)),
 (tensor(261), tensor(True)),
 (tensor(268), tensor(True)),
 (tensor(274), tensor(True)),
 (tensor(290), tensor(True)),
 (tensor(313), tensor(True)),
 (tensor(316), tensor(True)),
 (tensor(317), tensor(True)),
 (tensor(322), tensor(True)),
 (tensor(323), tensor(True)),
 (tensor(338), tensor(True)),
 (tensor(356), tensor(True)),
 (tensor(368), tensor(True)),
 (tensor(407), tensor(True)),
 (tensor(410), tensor(True)),
 (tensor(437), tensor(True)),
 (tensor(446), tensor(True)),
 (tensor(447), tensor(True)),
 (tensor(500), tensor(True)),
 (tensor(1), tensor(False)),
 (tensor(7), tensor(False)),
 (tensor(24), tensor(False)),
 (tensor(25), tensor(False)),
 (tensor(27), tensor(False)),
 (tensor(46), tensor(False)),
 (tensor(47), tensor(False)),
 (tensor(73), tensor(False)),
 (tensor(96), tensor(False)),
 (tensor(104), tensor(False)),
 (tensor(109), tensor(False)),
 (tensor(283), tensor(True)),
 (tensor(301), tensor(True)),
 (tensor(302), tensor(True)),
 (tensor(331), tensor(True)),
 (tensor(337), tensor(True)),
 (tensor(344), tensor(True)),
 (tensor(361), tensor(True)),
 (tensor(421), tensor(True)),
 (tensor(432), tensor(True)),
 (tensor(456), tensor(True)),
 (tensor(464), tensor(True)),
 (tensor(465), tensor(True)),
 (tensor(470), tensor(True)),
 (tensor(476), tensor(True)),
 (tensor(497), tensor(True)),
 (tensor(501), tensor(True)),
 (tensor(6), tensor(False)),
 (tensor(9), tensor(False)),
 (tensor(10), tensor(False)),
 (tensor(11), tensor(False)),
 (tensor(30), tensor(False)),
 (tensor(32), tensor(False)),
 (tensor(66), tensor(False)),
 (tensor(69), tensor(False)),
 (tensor(72), tensor(False)),
 (tensor(79), tensor(False)),
 (tensor(90), tensor(False)),
 (tensor(122), tensor(False)),
 (tensor(157), tensor(False)),
 (tensor(170), tensor(False)),
 (tensor(190), tensor(False)),
 (tensor(206), tensor(False))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#mask = mask &amp; (bkv_t[:, :, None, :] &lt; query_len)</span>
<span class="n">dots</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">masked_value</span><span class="p">)</span>
<span class="k">del</span> <span class="n">mask</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Self-masking">Self masking<a class="anchor-link" href="#Self-masking"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The paper states: <em>While attention to the future is not allowed, typical implementations of the Transformer do allow a position to attend to itself. Such behavior is undesirable in a shared-QK formulation because the dot-product of a query vector with itself will almost always be greater than the dot product of a query vector with a vector at another position. <strong>We therefore modify the masking to forbid a token from attending to itself</strong>, except in situations where a token has no other valid attention targets (e.g. the first token in a sequence).</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recall that <code>bq_t</code> and <code>bkc_t</code> is the look up key we use to reorder input <code>qk</code> and <code>v</code> into sorted and chunked order. The diagonal of the first part of <code>k</code> is eg. always similar to <code>q</code>. We have to compare all elements with each other (32*64)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">bkv_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([ 20,  29,  87, 112, 118, 168, 177, 182, 202, 271, 288, 319, 345, 357,
         365, 366, 374, 377, 383, 410, 419, 420, 447, 469,   5,  23,  46,  55,
          66,  93,  96, 102]),
 tensor([ 20,  29,  87, 112, 118, 168, 177, 182, 202, 271, 288, 319, 345, 357,
         365, 366, 374, 377, 383, 410, 419, 420, 447, 469,   5,  23,  46,  55,
          66,  93,  96, 102, 368, 396, 410, 423, 425, 446, 456, 472, 491, 494,
          20,  38, 100, 105, 135, 189, 210, 241, 242, 285, 286, 304, 306, 322,
         346, 367, 404, 450, 480, 485, 498, 500]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We achieve this by adding appropriate unit axis (se section below):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bkv_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 96, 32, 1]), torch.Size([64, 96, 1, 64]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">self_mask</span> <span class="o">=</span> <span class="n">bq_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">bkv_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">self_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">self_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 96, 32, 64]), tensor(34))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">self_mask</span><span class="p">,</span> <span class="n">TOKEN_SELF_ATTN_VALUE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.4651e-01, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.1882e-01,  2.4822e-02, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [ 8.9638e-02, -7.2742e-01, -7.8459e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-8.0705e-02,  3.9972e-02, -9.7325e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-7.0885e-01, -1.1693e+00, -1.1712e+00,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ...,  6.0113e-02,
           -6.6529e-01, -7.0489e-01],
          [-2.0178e-02, -5.0000e+04, -3.4028e+38,  ..., -7.3383e-01,
            3.2632e-02, -1.1272e+00],
          [ 3.3639e-02, -1.0865e+00, -5.0000e+04,  ..., -1.1597e+00,
           -1.4506e-01, -6.3097e-01],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -2.0544e-02,
           -6.6849e-01, -3.1145e-03],
          [-1.1505e+00, -5.0000e+04, -3.4028e+38,  ..., -5.2326e-02,
           -6.3007e-01, -2.3459e-02],
          [ 4.2065e-02, -5.8718e-01, -5.0000e+04,  ..., -1.1978e-01,
           -9.9706e-01, -4.5898e-02],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -5.5354e-02,
            2.8623e-02, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -7.3655e-01,
           -2.4440e-02, -3.4028e+38],
          [ 3.8226e-02,  5.8523e-02, -7.1844e-01,  ...,  1.1530e-01,
            7.8461e-02, -8.4067e-02]],

         ...,

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -5.7121e-01,
           -3.8492e-03,  1.0117e-01],
          [-7.0567e-01, -5.0000e+04, -3.4028e+38,  ..., -9.9979e-01,
           -6.8849e-01, -7.3988e-02],
          [-6.3723e-01,  1.8232e-02, -5.0000e+04,  ...,  3.3746e-02,
           -6.0018e-01, -1.1584e+00],
          ...,
          [ 6.1799e-02,  6.9101e-03,  3.7042e-02,  ...,  2.6486e-02,
           -6.1869e-01, -7.4292e-01],
          [-3.8282e-02,  7.3337e-02, -1.0109e-01,  ...,  5.5641e-02,
           -6.4948e-01,  5.4328e-02],
          [-9.8530e-02, -3.5202e-02, -1.5972e+00,  ..., -1.2660e-01,
           -7.4450e-01, -1.0500e+00]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -6.6170e-01,
           -9.9171e-01, -6.4411e-02],
          [ 2.8565e-02, -5.0000e+04, -3.4028e+38,  ...,  5.4141e-02,
           -3.5999e-02,  3.9631e-02],
          [-1.2061e+00, -3.8697e-02, -5.0000e+04,  ...,  4.6656e-02,
           -6.1919e-01, -7.5294e-01],
          ...,
          [-1.4103e+00, -7.2197e-01, -6.3220e-01,  ..., -6.5529e-01,
           -7.4984e-01,  4.3277e-02],
          [-2.8961e-02, -7.4635e-01,  1.7832e-02,  ..., -5.7393e-02,
           -6.4106e-01,  1.1850e-02],
          [-6.2059e-02, -3.3901e-02, -1.1619e-02,  ...,  1.4928e-02,
            5.3628e-04, -7.0925e-01]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -1.9251e-02,
            5.2760e-02, -6.8958e-02],
          [-6.8157e-01, -5.0000e+04, -3.4028e+38,  ..., -5.8809e-01,
            4.5345e-03, -5.4893e-01],
          [-5.7760e-01, -6.0494e-01, -5.0000e+04,  ..., -6.8532e-01,
           -7.2322e-01,  5.7745e-02],
          ...,
          [ 7.2948e-03, -6.7351e-01,  6.9916e-02,  ..., -6.7309e-01,
           -7.1030e-01,  1.0441e-02],
          [-6.2357e-01, -7.3820e-01, -6.2938e-01,  ..., -6.8691e-02,
           -1.3458e+00, -8.4144e-01],
          [-8.7684e-01, -6.4115e-01, -6.4251e-01,  ...,  7.9770e-02,
           -3.5258e-02,  1.1269e-01]]],


        [[[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.6076e-01, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.2430e-01, -1.2219e-01, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-8.6926e-01,  6.5723e-02, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.4740e-02, -1.1260e+00, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.6764e-01, -8.3105e-02, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ...,  7.7333e-02,
            3.1600e-02,  1.8005e-02],
          [-6.7779e-01, -5.0000e+04, -3.4028e+38,  ..., -1.0047e+00,
           -7.1437e-01, -1.2170e-02],
          [-1.1878e-02,  1.3605e-01, -5.0000e+04,  ..., -2.3003e-02,
            1.3730e-02, -7.3811e-01],
          ...,
          [-7.4399e-01, -7.2079e-01,  1.6003e-02,  ..., -1.2630e+00,
           -4.9323e-02,  7.9365e-02],
          [-1.3635e+00, -6.7924e-01, -2.3513e-02,  ..., -1.3940e+00,
           -3.1175e-02, -2.2558e-02],
          [-6.5811e-01, -1.0604e+00, -9.5996e-01,  ..., -7.6033e-01,
           -5.5959e-01,  2.1492e-02]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -6.1800e-01,
           -6.2581e-01,  5.1135e-02],
          [-9.6606e-01, -5.0000e+04, -3.4028e+38,  ..., -6.0455e-01,
           -7.3208e-01, -4.9317e-02],
          [-3.2136e-02, -7.2623e-03, -5.0000e+04,  ...,  1.4396e-01,
           -6.6859e-01, -6.1585e-01],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ...,  1.5643e-02,
           -3.4028e+38, -3.4028e+38]],

         ...,

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ...,  1.5109e-01,
           -6.7132e-01, -7.1930e-01],
          [-6.4984e-01, -5.0000e+04, -3.4028e+38,  ...,  1.3963e-03,
           -1.1509e+00, -4.5689e-02],
          [-1.6850e-02, -1.3503e+00, -5.0000e+04,  ...,  8.6320e-02,
           -6.6129e-01, -6.6410e-01],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -1.1687e+00,
           -7.9722e-01,  5.0385e-02],
          [ 4.2493e-02, -5.0000e+04, -3.4028e+38,  ..., -7.5008e-01,
            1.0685e-01, -6.0812e-01],
          [-6.5271e-01, -1.1316e+00, -5.0000e+04,  ..., -8.8241e-03,
            1.7627e-01, -3.8736e-02],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ...,  1.0364e-01,
            7.5225e-02,  7.8665e-02],
          [-1.0867e-01, -5.0000e+04, -3.4028e+38,  ..., -1.5266e-03,
           -6.3571e-01,  3.4974e-02],
          [ 1.1809e-02, -1.0740e+00, -5.0000e+04,  ..., -6.9976e-03,
           -6.9689e-01, -1.1085e+00],
          ...,
          [-1.0437e-01, -1.2239e-02, -6.6529e-01,  ...,  2.5524e-02,
            3.4716e-02,  8.2917e-02],
          [-2.2990e-02, -6.3193e-01, -1.1988e+00,  ..., -7.3525e-01,
           -6.0445e-02, -4.4580e-02],
          [-6.6057e-01, -3.8727e-02, -5.7678e-02,  ..., -1.1243e+00,
            6.1690e-02, -6.3970e-01]]],


        [[[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-7.0107e-01, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.7432e-01, -5.8993e-01, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-1.0163e+00, -5.7144e-02,  6.1773e-02,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.7664e-01, -6.7368e-01, -1.1599e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [ 2.0432e-02, -1.1016e+00,  5.7354e-02,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -1.0858e+00,
            3.1600e-02, -1.0709e+00],
          [-5.6724e-01, -5.0000e+04, -3.4028e+38,  ...,  1.1074e-03,
           -1.1556e+00,  1.6300e-02],
          [-1.0745e+00, -6.8645e-01, -5.0000e+04,  ..., -6.0015e-01,
           -7.3519e-01,  7.4664e-02],
          ...,
          [-1.0592e+00, -1.0839e-01, -8.5495e-02,  ..., -1.1811e-01,
           -6.6579e-01, -1.0507e+00],
          [-7.0449e-01,  2.2444e-02, -6.2353e-01,  ...,  8.6373e-02,
            3.3366e-02, -7.6842e-01],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -7.4734e-01],
          [-7.3401e-01, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -7.3220e-01],
          [-1.1460e+00, -9.8449e-02, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -7.0000e-01],
          ...,
          [ 5.0304e-02,  2.1384e-02, -5.5987e-02,  ..., -5.6092e-02,
           -3.4028e+38,  6.2510e-02],
          [ 7.9292e-03, -5.9112e-01, -1.1575e+00,  ...,  6.3428e-02,
           -1.9939e-02, -6.2542e-01],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -1.7875e-01]],

         ...,

         [[-5.0000e+04, -3.4028e+38, -7.1105e-01,  ...,  2.1309e-02,
           -6.9780e-01,  1.1153e-01],
          [-1.2436e-01, -5.0000e+04, -4.3738e-02,  ...,  6.7087e-02,
           -7.3648e-01, -1.0334e+00],
          [-3.4028e+38, -3.4028e+38, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-8.3788e-01, -7.0348e-01, -1.0948e+00,  ..., -6.6368e-01,
           -1.3585e+00,  6.5881e-02],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -9.0959e-03, -3.0965e-02],
          [-1.0970e+00, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -1.0928e-01, -6.8574e-01],
          [-6.4475e-01, -6.4413e-02, -5.0000e+04,  ..., -3.4028e+38,
           -1.1016e+00, -7.0929e-01],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -1.1587e+00, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -7.8249e-01, -7.3240e-01],
          [-6.3891e-01,  3.5043e-02, -1.1211e+00,  ..., -3.4028e+38,
           -4.2181e-03, -6.6783e-01]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ...,  6.6128e-02,
            5.7813e-04, -6.3975e-01],
          [-1.1157e+00, -5.0000e+04, -3.4028e+38,  ...,  5.5402e-02,
           -6.8001e-01, -4.1885e-02],
          [-3.1970e-02, -6.2763e-01, -5.0000e+04,  ...,  8.5007e-02,
           -8.2336e-02, -6.7272e-01],
          ...,
          [-7.1669e-01, -2.6079e-02, -1.0298e-01,  ..., -1.1081e+00,
           -1.0416e+00, -1.1158e+00],
          [-7.3221e-02, -1.8096e-02, -5.7792e-01,  ..., -8.2817e-01,
           -5.4756e-02,  6.0642e-02],
          [-6.2527e-01, -7.3235e-01,  3.6161e-02,  ..., -6.5685e-01,
           -1.0227e+00,  4.3928e-02]]],


        ...,


        [[[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [ 2.6721e-03, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-2.7559e-02, -1.0628e+00, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-5.6986e-01, -1.1886e-02, -7.5952e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.0231e+00, -1.7792e-02, -5.0195e-02,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.1377e+00, -7.1819e-01, -6.3855e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -1.0323e+00,
           -9.5445e-01, -1.1035e+00],
          [-8.1883e-01, -5.0000e+04, -3.4028e+38,  ..., -1.1960e-01,
            1.3267e-01,  3.6285e-02],
          [-3.4274e-02,  5.2932e-02, -5.0000e+04,  ...,  3.8055e-02,
           -1.3925e-02,  2.2453e-02],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -6.2002e-01,
            8.9151e-03,  1.0790e-01],
          [-2.7135e-02, -5.0000e+04, -3.4028e+38,  ..., -6.6650e-01,
           -9.2817e-01, -2.2317e-02],
          [ 8.0496e-02, -6.8004e-01, -5.0000e+04,  ...,  4.2166e-02,
           -3.6741e-02, -5.3699e-01],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         ...,

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -5.5951e-01,
            3.6174e-02, -6.4231e-01],
          [-7.4560e-01, -5.0000e+04, -3.4028e+38,  ..., -6.3169e-01,
           -1.4407e+00, -7.1516e-01],
          [-5.7026e-01, -7.8466e-01, -5.0000e+04,  ..., -9.9972e-03,
           -6.0130e-02,  1.1593e-01],
          ...,
          [-7.3289e-01,  2.0110e-02, -7.2580e-01,  ..., -1.0738e-02,
           -1.1695e-01, -1.0707e+00],
          [-1.0875e+00,  3.9373e-02,  1.4219e-02,  ..., -6.3023e-01,
            5.5321e-02, -5.9529e-01],
          [-5.9247e-01,  1.2369e-01, -7.3387e-02,  ..., -7.6871e-02,
           -7.2903e-01, -6.1954e-02]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -1.4231e-04,
            2.5220e-02, -1.0773e+00],
          [-6.3603e-01, -5.0000e+04, -3.4028e+38,  ..., -1.0874e+00,
           -4.9859e-02, -6.4749e-01],
          [-6.4161e-01, -7.4738e-01, -5.0000e+04,  ..., -5.7791e-01,
           -6.2543e-03, -1.1642e+00],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -7.0094e-01,
            4.2724e-03,  5.4235e-02],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -2.3470e-02,
           -1.0217e-01, -6.4626e-01],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -7.0674e-01,
           -6.3122e-01, -1.0745e-02]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -6.2641e-01,
            4.7402e-02,  8.4206e-02],
          [-7.0791e-01, -5.0000e+04, -3.4028e+38,  ..., -4.3338e-02,
           -2.8378e-02,  1.2988e-01],
          [ 1.0670e-04,  8.5781e-02, -5.0000e+04,  ..., -6.3421e-01,
           -1.2349e+00, -7.5487e-01],
          ...,
          [-2.5026e-03, -6.0923e-01,  9.6376e-02,  ...,  1.4107e-01,
            1.2839e-02, -5.6102e-01],
          [-6.8853e-01, -9.3420e-02, -5.8252e-02,  ...,  2.7106e-02,
           -6.8703e-01,  8.3605e-02],
          [-7.6307e-01, -6.7374e-01, -2.8049e-02,  ..., -1.0166e-01,
           -6.2191e-01,  2.8614e-02]]],


        [[[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.0153e+00, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-8.1250e-02, -7.3463e-01, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-6.7312e-01, -1.0561e-02, -6.1704e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [ 3.9926e-02,  4.2183e-03, -7.0787e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.4681e-01, -7.6523e-01, -7.3376e-01,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38,  2.7195e-02,  ..., -5.2110e-02,
           -6.0999e-02,  4.4449e-02],
          [-1.3868e-01, -5.0000e+04, -4.3554e-02,  ..., -3.3023e-02,
            1.8760e-02, -7.5677e-01],
          [-3.4028e+38, -3.4028e+38, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [ 5.7223e-04, -3.4028e+38, -6.2095e-01,  ..., -7.2350e-01,
           -7.8402e-01, -6.9741e-01],
          [-8.4171e-01, -3.4028e+38, -5.2057e-02,  ..., -4.2653e-02,
            3.0755e-02, -6.8030e-02],
          [-6.5412e-01, -3.4028e+38, -9.7690e-01,  ..., -6.2359e-01,
           -1.6402e-02, -5.3178e-02]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.9276e-02, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-6.8574e-01, -5.7047e-01, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-7.4750e-01,  1.0921e-01, -1.0828e+00,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [ 4.8119e-02, -5.7358e-01, -1.0990e+00,  ...,  5.4826e-02,
           -3.4028e+38, -3.4028e+38],
          [-6.2310e-01, -1.0785e+00, -1.0339e+00,  ..., -1.3479e-02,
            2.0068e-01, -3.4028e+38]],

         ...,

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ...,  1.0200e-02,
           -6.7591e-01, -1.1870e+00],
          [ 1.5608e-02, -5.0000e+04, -3.4028e+38,  ..., -5.0493e-01,
           -9.9869e-01, -1.4343e+00],
          [-8.0914e-02,  2.9625e-02, -5.0000e+04,  ..., -7.0351e-01,
           -6.4617e-01,  5.6308e-03],
          ...,
          [-8.9133e-03, -1.3096e-01, -4.7320e-04,  ..., -7.2473e-01,
           -1.6868e-02,  2.8787e-02],
          [-7.9361e-01, -6.4984e-01,  7.5883e-02,  ..., -4.4323e-03,
           -6.8407e-02, -1.0846e-01],
          [ 5.3698e-02, -7.0342e-01, -8.5320e-02,  ..., -4.1425e-02,
           -7.9851e-01, -6.7153e-01]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [ 1.1630e-01, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [ 7.7609e-02,  1.4753e-02, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-6.1882e-02, -1.4174e-02, -1.3796e+00,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-2.5393e-02, -7.5303e-01,  1.0076e-01,  ...,  6.1855e-02,
           -7.9291e-01, -3.4028e+38],
          [-6.2580e-01, -7.6510e-01,  1.8015e-02,  ..., -6.0075e-01,
           -8.3052e-01, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -6.4054e-01,
           -8.7310e-02,  5.3704e-02],
          [-6.4269e-01, -5.0000e+04, -3.4028e+38,  ..., -8.2971e-02,
           -1.4855e-01, -8.9776e-02],
          [ 2.3660e-02, -7.1981e-01, -5.0000e+04,  ...,  9.7790e-02,
           -4.8083e-02, -6.1394e-01],
          ...,
          [-1.3635e+00, -6.5168e-01, -1.2835e-02,  ..., -6.8055e-01,
            3.6372e-02, -6.5923e-01],
          [ 5.5290e-02, -7.6812e-01, -1.1189e+00,  ..., -6.3953e-01,
           -6.5224e-01, -7.0943e-01],
          [ 1.3939e-02, -8.7076e-02, -6.2809e-01,  ..., -6.5883e-01,
           -8.7338e-02, -7.4225e-01]]],


        [[[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.4639e+00, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-1.0586e+00, -1.1582e+00, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [ 5.8896e-02, -5.7658e-01, -7.5433e-01,  ...,  2.0647e+01,
           -2.6651e-02,  2.0653e+01],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-7.2978e-01, -1.0459e+00,  6.2408e-02,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -7.2156e-01,  1.8013e-02],
          [-1.5039e-02, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
           -3.7105e-02, -5.8817e-01],
          [-3.9265e-02, -6.1889e-02, -5.0000e+04,  ..., -3.4028e+38,
           -3.4280e-02, -5.5754e-01],
          ...,
          [-7.2015e-01, -8.7395e-02, -1.2293e-02,  ..., -3.4028e+38,
           -6.6509e-01,  8.0814e-02],
          [ 4.6665e-02, -1.0344e+00, -7.1668e-01,  ..., -3.4028e+38,
           -1.0477e+00, -6.8540e-01],
          [-4.9705e-01,  7.4967e-03, -1.0317e+00,  ..., -3.4028e+38,
           -5.8730e-02,  4.6460e-02]],

         [[-5.0000e+04, -3.4028e+38, -8.5118e-01,  ..., -3.2423e-03,
           -4.3115e-02,  4.8089e-02],
          [-1.0768e+00, -5.0000e+04, -1.3421e-02,  ..., -4.7596e-02,
           -5.9070e-02,  8.5101e-02],
          [-3.4028e+38, -3.4028e+38, -5.0000e+04,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          ...,
          [-3.4028e+38, -3.4028e+38, -6.0726e-02,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38,  2.6122e-02,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -4.6727e-02,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         ...,

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -2.7874e-02, -1.1563e+00],
          [-5.7874e-01, -5.0000e+04, -3.4028e+38,  ..., -3.4028e+38,
            5.7971e-02, -3.7483e-02],
          [ 7.5618e-03, -6.5969e-01, -5.0000e+04,  ..., -3.4028e+38,
           -7.3267e-01,  1.2819e-02],
          ...,
          [-6.5722e-01,  1.0743e-01,  3.1043e-02,  ..., -3.4028e+38,
           -5.9182e-01,  1.4952e-01],
          [-6.9334e-01, -1.0488e+00, -6.3029e-02,  ..., -3.4028e+38,
           -6.6509e-01,  8.8972e-03],
          [-8.1567e-01,  9.3357e-03, -5.8275e-02,  ..., -3.4028e+38,
            7.6341e-02,  1.6951e-01]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -4.5265e-03,
           -7.1776e-01, -7.0639e-01],
          [-6.6058e-01, -5.0000e+04, -3.4028e+38,  ...,  4.5980e-02,
           -4.9668e-02, -1.0652e-02],
          [-7.0612e-01, -1.0613e+00, -5.0000e+04,  ..., -7.4379e-01,
            5.2316e-02, -6.9348e-01],
          ...,
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38],
          [-3.4028e+38, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,
           -3.4028e+38, -3.4028e+38]],

         [[-5.0000e+04, -3.4028e+38, -3.4028e+38,  ..., -3.6865e-02,
           -6.0356e-01, -3.1227e-02],
          [ 6.7016e-02, -5.0000e+04, -3.4028e+38,  ..., -1.2225e+00,
           -1.3129e-02, -1.1694e+00],
          [-6.3382e-01, -6.9378e-01, -5.0000e+04,  ..., -1.0572e+00,
            3.5980e-02, -6.9805e-01],
          ...,
          [-7.4151e-01, -1.0349e+00, -2.2069e-02,  ..., -9.5842e-02,
            3.5878e-03, -6.5626e-01],
          [-6.8698e-01, -3.7117e-02, -1.1274e+00,  ..., -6.7698e-01,
           -3.0499e-02, -2.4080e-02],
          [-1.1675e-01, -7.4351e-03,  1.2485e-01,  ...,  9.3894e-02,
            1.1018e-01, -7.3857e-01]]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Comparing-all-elements-by-adding-unit-axis">Comparing all elements by adding unit axis<a class="anchor-link" href="#Comparing-all-elements-by-adding-unit-axis"> </a></h5>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By adding appropriate unit axis, we can compare all elements of the final dimension of a tensor</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([2]), torch.Size([5]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span><span class="o">+</span><span class="n">b</span><span class="p">[:,</span><span class="kc">None</span><span class="p">],</span> <span class="n">a</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span><span class="o">+</span><span class="n">b</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0, 1]),
 tensor([0, 1, 2, 3, 4]),
 tensor([[0, 1],
         [1, 2],
         [2, 3],
         [3, 4],
         [4, 5]]),
 tensor([[0, 1, 2, 3, 4],
         [1, 2, 3, 4, 5]]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Mask-out-attention-to-other-hash-buckets.">Mask out attention to other hash buckets.<a class="anchor-link" href="#Mask-out-attention-to-other-hash-buckets."> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: The paper sugests NOT attenting across buckets: <em>Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by <strong>only allowing attention within a single hash bucket</strong>.</em> Lucidrains' inmplementation sets this to True by default however.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will only run this part of the code if we want to restrict attention across buckets.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we get the hasbucket by integer dividing by sl. Note that hasbucket ids are consecutive across hash rounds (not overlapping in each round). We also reshape to n_chunks:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#if not self._attend_across_buckets:</span>
<span class="n">bq_buckets</span> <span class="o">=</span> <span class="n">bkv_buckets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sbuckets_and_t</span> <span class="o">//</span> <span class="n">sl</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">bq_buckets</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Eg. attention chunk 1 has a mix of buckets 0 and 1:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 2, 2, 2, 2, 2])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We add previous chunk to the keys:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bkv_buckets</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">bkv_buckets</span><span class="p">)</span>
<span class="n">bkv_buckets</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bkv_buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bucket_mask</span> <span class="o">=</span> <span class="n">bq_buckets</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">!=</span> <span class="n">bkv_buckets</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">bucket_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bq_buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">bkv_buckets</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor(2),
 tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bucket_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True, False, False, False,
        False, False,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">bucket_mask</span><span class="p">,</span> <span class="n">masked_value</span><span class="p">)</span>
<span class="k">del</span> <span class="n">bucket_mask</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Softmax">Softmax<a class="anchor-link" href="#Softmax"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We take the softmax with the <a href="https://blog.feedly.com/tricks-of-the-trade-logsumexp/">logsumexp trick</a>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots_logsumexp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">dots</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">dots</span> <span class="o">-</span> <span class="n">dots_logsumexp</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">dots</span><span class="p">)</span>
<span class="n">dots</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 64])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dots</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And finally compute our self attention:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bnsz,bnzd-&gt;bnsd&#39;</span><span class="p">,</span> 
                  <span class="n">dots</span><span class="p">,</span>                  <span class="c1"># [bs, n_chunks, chunk_size, chunk_size*2]</span>
                  <span class="n">bv</span><span class="p">)</span>                    <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
<span class="n">bo</span><span class="o">.</span><span class="n">shape</span>                                 <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 96, 32, 256])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Unsorting">Unsorting<a class="anchor-link" href="#Unsorting"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The final step is to reconstruct the batched, chunked and sorted <code>q, k and v</code> back to our original representation. First we reshape the contextualised values to remove the <code>n_chunks</code> dimension. It's still sorted though:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we reshape self-attnetion to remove the <code>n_chunks</code> dimension. It's still sorted though:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">so</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bo</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>                 <span class="c1"># [bs, seqlen*n_rounds, model_dim]</span>
<span class="n">so</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 3072, 256])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we unsort so by looking up our unsort keys <code>undo_sort</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">o</span> <span class="o">=</span> <span class="n">batched_index_select</span><span class="p">(</span><span class="n">so</span><span class="p">,</span> <span class="n">undo_sort</span><span class="p">)</span>
<span class="n">o</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 3072, 256])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And reshapes it to include a <code>n_rounds</code> dimension:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>    <span class="c1"># [bs, n_rounds, sl, dim]</span>
<span class="n">o</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 6, 512, 256])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then the same steps for the logits (<code>dots_logsumexp</code>):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">slogits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dots_logsumexp</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,))</span>    <span class="c1"># [bs, seqlen*n_rounds]</span>
<span class="n">slogits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 3072])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">slogits</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">undo_sort</span><span class="p">)</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>                                    <span class="c1"># [bs, seqlen*n_rounds]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 3072])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>                                       <span class="c1"># [bs, n_rounds, sl, 1]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 6, 512, 1])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We take the softmax over the <code>n_rounds</code> dimension, "averaging" the contribution to self-attention over each hashing round.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">probs</span><span class="o">.</span><span class="n">shape</span>           <span class="c1"># [bs, n_rounds, sl, 1]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 6, 512, 1])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So summing over the <code>n_rounds</code> dimension equals 1:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(0.9988)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And our final self-attention weighted by contribution from each round:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">o</span> <span class="o">*</span> <span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># [bs, sl, dim]</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">qk</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 512, 256]),
 torch.Size([64, 512, 256]),
 torch.Size([64, 512, 256]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="LSHAttention---minimal-implementation">LSHAttention - minimal implementation<a class="anchor-link" href="#LSHAttention---minimal-implementation"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LSHAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">bucket_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">n_hashes</span> <span class="o">=</span> <span class="mi">8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_size</span> <span class="o">=</span> <span class="n">bucket_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span> <span class="o">=</span> <span class="n">n_hashes</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="o">*</span><span class="n">qk</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">qk</span><span class="o">.</span><span class="n">device</span>
        
        <span class="c1">#pdb.set_trace()</span>
        <span class="c1"># f&#39;Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}&#39;</span>
        <span class="k">assert</span> <span class="n">seqlen</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
        
        <span class="c1"># Get buckets. We use the above method</span>
        <span class="n">n_buckets</span> <span class="o">=</span> <span class="n">seqlen</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_size</span>                      
        <span class="n">buckets</span> <span class="o">=</span> <span class="n">hash_vectors</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="n">n_buckets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span><span class="p">)</span>        <span class="c1"># buckets: [bs, (sl * n_hashes)]</span>

        <span class="c1"># We use the same vector as both a query and a key.</span>
        <span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span> <span class="o">*</span> <span class="n">seqlen</span>

        <span class="c1"># a vector of [bs, n_hashes*seqlen), where ticker[0,:]= [0,1,2, ..-, seqlen*n_hash-1]</span>
        <span class="n">ticker</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span> <span class="o">*</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">buckets</span><span class="p">)</span> 
        
        <span class="c1"># ticker % seqlen = [o...seqlen-1, 0...seqlen-1, ...] n_bucket times</span>
        <span class="c1"># we add the bucket id scaled by seqlen</span>
        <span class="c1"># shape: [bs, (seqlen*buckets)]</span>
        <span class="c1"># let us sort according to bucket id and index in sequence</span>
        <span class="n">buckets_and_t</span> <span class="o">=</span> <span class="n">seqlen</span> <span class="o">*</span> <span class="n">buckets</span> <span class="o">+</span> <span class="p">(</span><span class="n">ticker</span> <span class="o">%</span> <span class="n">seqlen</span><span class="p">)</span>
        
        <span class="c1"># disable gradients for tensor</span>
        <span class="c1">#buckets_and_t = buckets_and_t.detach()</span>

        <span class="c1"># Hash-based sort (&quot;s&quot; at the start of variable names means &quot;sorted&quot;)</span>
        <span class="n">sbuckets_and_t</span><span class="p">,</span> <span class="n">sticker</span> <span class="o">=</span> <span class="n">sort_key_val</span><span class="p">(</span><span class="n">buckets_and_t</span><span class="p">,</span> <span class="n">ticker</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># shapes are [bs, seqlen*n_hashes]</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">undo_sort</span> <span class="o">=</span> <span class="n">sticker</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                                       <span class="c1"># indexes to undo sortings</span>
        <span class="k">del</span> <span class="n">ticker</span>

        <span class="n">st</span> <span class="o">=</span> <span class="p">(</span><span class="n">sticker</span> <span class="o">%</span> <span class="n">seqlen</span><span class="p">)</span>              <span class="c1"># index of [0..seqlen-1] for each hash round (n_hashes)[bs, seqlen*n_hashes]</span>
        <span class="n">sqk</span> <span class="o">=</span> <span class="n">batched_index_select</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="n">st</span><span class="p">)</span>   <span class="c1"># get the sorted qk, [bs, seqlen, model_dim]</span>
        <span class="n">sv</span> <span class="o">=</span> <span class="n">batched_index_select</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">st</span><span class="p">)</span>     <span class="c1"># get the sorted v, [bs, seqlen, model_dim] </span>

        <span class="c1"># Split off a &quot;bin&quot; axis so that attention only occurs within chunks.</span>
        <span class="c1"># get the qk and v chunks and also the indexes to undo sort later</span>
        <span class="n">n_chunks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span> <span class="o">*</span> <span class="n">n_buckets</span>
        <span class="n">bq_t</span> <span class="o">=</span> <span class="n">bkv_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">st</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>   <span class="c1"># [bs, n_chunks, chunk_size]</span>
        <span class="n">bqk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sqk</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>      <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>
        <span class="n">bv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sv</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>        <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>

        <span class="c1"># Hashing operates on unit-length vectors. Unnormalized query vectors are</span>
        <span class="c1"># fine because they effectively provide a learnable temperature for the</span>
        <span class="c1"># attention softmax, but normalizing keys is needed so that similarity for</span>
        <span class="c1"># the purposes of attention correctly corresponds to hash locality.</span>
        <span class="n">bq</span> <span class="o">=</span> <span class="n">bqk</span>
        <span class="n">bk</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">bqk</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">bq</span><span class="p">)</span>

        <span class="c1">## attent to previous chunk as well - append previous chunk, cat along dim=2 (the sl dimension)</span>
        <span class="n">bk</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">bk</span><span class="p">)</span>            <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
        <span class="n">bv</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">bv</span><span class="p">)</span>            <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
        <span class="n">bkv_t</span> <span class="o">=</span> <span class="n">look_one_back</span><span class="p">(</span><span class="n">bkv_t</span><span class="p">)</span>      <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>

        <span class="c1"># Dot-product attention</span>
        <span class="n">dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bnsd,bnzd-&gt;bnsz&#39;</span><span class="p">,</span> 
                            <span class="n">bq</span><span class="p">,</span>                  <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>
                            <span class="n">bk</span>                   <span class="c1"># [bs, n_chunks, chunk_size*2, model_dim]</span>
                           <span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>     <span class="c1"># dots: [bs, n_chunks, chunk_size, chunk_size*2]</span>

        <span class="c1"># Mask out attention to self except when no other targets are available.</span>
        <span class="n">self_mask</span> <span class="o">=</span> <span class="n">bq_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">bkv_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">dots</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">self_mask</span><span class="p">,</span> <span class="n">TOKEN_SELF_ATTN_VALUE</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">self_mask</span>

        <span class="c1"># Softmax.</span>
        <span class="n">dots_logsumexp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">dots</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">dots</span> <span class="o">-</span> <span class="n">dots_logsumexp</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">dots</span><span class="p">)</span>
                
        <span class="c1"># calculate self-attention (attn * values)</span>
        <span class="n">bo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bnsz,bnzd-&gt;bnsd&#39;</span><span class="p">,</span> <span class="n">dots</span><span class="p">,</span> <span class="n">bv</span><span class="p">)</span>                 <span class="c1"># [bs, n_chunks, chunk_size, model_dim]</span>
        
        <span class="c1"># unchunk, unsort and reshape self-attention</span>
        <span class="n">so</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bo</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>                  <span class="c1"># [bs, seqlen*n_hashes, model_dim]</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">batched_index_select</span><span class="p">(</span><span class="n">so</span><span class="p">,</span> <span class="n">undo_sort</span><span class="p">)</span>                        <span class="c1"># [bs, seqlen*n_hashes, model_dim]</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span> <span class="c1"># [bs, n_hashes, seqlen, model_dim]</span>
                
        <span class="c1"># unchunk, unsort and reshape logits</span>
        <span class="n">slogits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dots_logsumexp</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,))</span>              <span class="c1"># [bs, seqlen*n_hashes]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">slogits</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">undo_sort</span><span class="p">)</span>                                   <span class="c1"># [bs, seqlen*n_hashes]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># [bs, n_hashes, seqlen, 1]</span>
        
        <span class="c1"># average probabilites across hash rounds (dim 1) and get weighted attention</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="c1"># [bs, n_rounds, seqlen, 1]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">o</span> <span class="o">*</span> <span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                                        <span class="c1"># [bs, seqlen, model_dim]</span>

        <span class="c1"># return output and bucket distribution</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">buckets</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">qk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lsh_att</span> <span class="o">=</span> <span class="n">LSHAttention</span><span class="p">()</span>
<span class="n">out</span><span class="p">,</span> <span class="n">buckets</span> <span class="o">=</span> <span class="n">lsh_att</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">buckets</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 512, 128]), torch.Size([64, 4096]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

