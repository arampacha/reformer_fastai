---

title: Experiment Commands


keywords: fastai
sidebar: home_sidebar

summary: "All commands used in our Reproducibility Challenge submission experiments"
description: "All commands used in our Reproducibility Challenge submission experiments"
nb_path: "nbs/22_experiment.experiment-commands.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/22_experiment.experiment-commands.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>--do_wandb_logging=False --wandb_group='lm_base_final' --wandb_notes='Final Baseline TransformerLM experiment run' --wandb_tags='lm exp lm_base'</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Running-Experiments">Running Experiments<a class="anchor-link" href="#Running-Experiments"> </a></h2><p>After installing our library, experiments are run by running <code>run_exp TASK_NAME PARAMS</code>. The TASK_NAME sets the data and model to use while PARAMS adjust some model and training settings as well as logging.</p>
<h2 id="Experiment-Task-Names">Experiment Task Names<a class="anchor-link" href="#Experiment-Task-Names"> </a></h2><p>When running the experiment script, each experiment type has its own "task" which defines the data and model to be used as well as the training schedule. Most model settings are hard-coded in configs to mirror the configurations used in the Reformer paper. All model and training hypermaramters used in training can be found in <a href="https://arampacha.github.io/reformer_fastai/experiment-configs.html">Experiments/Configs</a>. Depending on the task, a limited number of configs can be changed such as <code>seq_len</code> or <code>n_layers</code> for example.</p>
<ul>
<li><code>synt</code> : Language Modelling with Synthetic Data</li>
<li><code>lm_XX</code> : The <code>lm_base</code> argument will train a baseline TransformerLM, <code>lm_rev</code> to train a ReversibleLM and <code>lm_shared_qk</code> for a baseline Transformer with shared query-key values. All training is on the enwik9 dataset.</li>
<li><code>n_hashes</code> : Trains a LSH-LM on the enwik8 data</li>
<li><code>n_layers</code> : Trains a LSH-LM on the enwik8 data</li>
<li><code>wmt_XX</code> : The <code>wmt_base</code> argument will train a classic Transformer on the WMT-14 dataset, while <code>wmt_rev</code> will train a ReversibleTransformer on the same dataset</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Commands-Used">Commands Used<a class="anchor-link" href="#Commands-Used"> </a></h2><h3 id="Language-Model-Experiments-with-enwik8">Language Model Experiments with enwik8<a class="anchor-link" href="#Language-Model-Experiments-with-enwik8"> </a></h3><p><strong>Section 4.2 -  LSH attention analysis on synthetic task</strong></p>

<pre><code>run_exp "synt" --n_epochs=750 --bs=64 --save_model=True --seed=123 --do_wandb_logging=False --n_hashes=4</code></pre>
<p><strong>Section 4.3 &amp; 4.4 - Effect of sharing QK &amp; Effect of reversible layers</strong>
The language modelling experiments outlined in section 4.3 and 4.4 of the paper were run with the following command. The <code>lm_base</code> argument was passed to train a baseline TransformerLM, <code>lm_rev</code> to train a ReversibleLM and <code>lm_shared_qk</code> for a baseline Transformer with shared query-key values</p>

<pre><code>run_exp "lm_base" --n_epochs=10 --bs=1 --max_seq_len=4096 --grad_accum=8 --save_model=True --clip=1.0 --seed=42 --do_wandb_logging=False</code></pre>
<p><strong>Section 4.5 -  Reversible Transformer on translation task experiment</strong></p>
<p>ReversibleTransformer on WMT-14:</p>

<pre><code>run_exp "wmt_rev" --lr=1e-4 --n_epochs=2 --bs=64 --n_layers=6 --max_seq_len=256 --do_wandb_logging=False --save_model=True --clip=1.0 --seed=8230 --precision=2</code></pre>
<p>Run the above with task <code>"wmt_base"</code> to train a baseline Transformer</p>
<p><strong>Section 4.6 - Effect of number of hashing rounds on the performance</strong></p>

<pre><code>run_exp "n_hashes" --n_hashes=2 --n_epochs=10 --bs=8 --max_seq_len=4096 --do_wandb_logging=True --wandb_group='n_hashes' --wandb_notes='performance as function of n_hashes (2)' --wandb_tags='lm exp lsh nhashes' --grad_accum=8  --clip=1.0 --seed=2</code></pre>
<p><strong>Section 4.7 - LSH attention evaluation speed</strong>
The LSH-LM evaluation speed experiment used the same functions as the script but was carried out in the <a href="https://arampacha.github.io/reformer_fastai/experiment.speed-lsh_synthetic-task.html">"LSH evaluation speed" notebook here</a></p>
<p><strong>Section 4.8 - Deep Reformer models</strong></p>

<pre><code>"n_layers" --n_layers=6 --n_epochs=8 --bs=2 --max_seq_len=16384 --do_wandb_logging=True --wandb_group='n_layers' --wandb_notes='performance as function of n_layers (6)' --wandb_tags='lm exp lsh nlayers' --grad_accum=8 --clip=1.0 --seed=48 --save_model=True</code></pre>

</div>
</div>
</div>
</div>
 

