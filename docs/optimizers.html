---

title: Optimizers


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/08_optimizers.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/08_optimizers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Adafactor" class="doc_header"><code>class</code> <code>Adafactor</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/optimizers.py#L10" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Adafactor</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>None</code></em>, <strong><code>eps</code></strong>=<em><code>(1e-30, 0.001)</code></em>, <strong><code>clip_threshold</code></strong>=<em><code>1.0</code></em>, <strong><code>decay_rate</code></strong>=<em><code>-0.8</code></em>, <strong><code>mom</code></strong>=<em><code>None</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0.0</code></em>, <strong><code>scale_parameter</code></strong>=<em><code>True</code></em>, <strong><code>relative_step</code></strong>=<em><code>True</code></em>, <strong><code>warmup_init</code></strong>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Implements Adafactor algorithm.</p>
<p>This implementation is based on: <code>Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</code> (see <a href="https://arxiv.org/abs/1804.04235">https://arxiv.org/abs/1804.04235</a>)</p>
<p>Note that this optimizer internally adjusts the learning rate depending on the <em>scale_parameter</em>, <em>relative_step</em> and <em>warmup_init</em> options. To use a manual (external) learning rate schedule you should set <code>scale_parameter=False</code> and <code>relative_step=False</code>.</p>
<p><strong>Arguments</strong></p>

<pre><code>`params` (iterable): iterable of parameters to optimize or dicts defining parameter groups

[`lr`](/reformer_fastai/experiment-script.html#lr) (float, optional): external learning rate (default: None)

`eps` (tuple[float, float]): regularization constans for square gradient and parameter scale respectively (default: (1e-30, 1e-3))

`clip_threshold` (float): threshold of root mean square of final gradient update (default: 1.0)

`decay_rate` (float): coefficient used to compute running averages of square gradient (default: -0.8)

`mom` (float): coefficient used for computing running averages of gradient (default: None)

`weight_decay` (float, optional): weight decay (L2 penalty) (default: 0)

`scale_parameter` (bool): if True, learning rate is scaled by root mean square of parameter (default: True)

`relative_step` (bool): if True, time-dependent learning rate is computed instead of external learning rate (default: True)

`warmup_init` (bool): time-dependent learning rate computation depends on whether warm-up initialization is being used (default: False)</code></pre>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="adafactor" class="doc_header"><code>adafactor</code><a href="https://github.com/arampacha/reformer_fastai/tree/master/reformer_fastai/optimizers.py#L182" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>adafactor</code>(<strong><code>param_groups</code></strong>, <strong><code>lr</code></strong>=<em><code>None</code></em>, <strong><code>eps</code></strong>=<em><code>(1e-30, 0.001)</code></em>, <strong><code>clip_threshold</code></strong>=<em><code>1.0</code></em>, <strong><code>decay_rate</code></strong>=<em><code>-0.8</code></em>, <strong><code>mom</code></strong>=<em><code>None</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0.0</code></em>, <strong><code>scale_parameter</code></strong>=<em><code>True</code></em>, <strong><code>relative_step</code></strong>=<em><code>True</code></em>, <strong><code>warmup_init</code></strong>=<em><code>False</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@delegates</span><span class="p">(</span><span class="n">Adafactor</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">adafactor</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">OptimWrapper</span><span class="p">(</span><span class="n">Adafactor</span><span class="p">([{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">ps</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span> <span class="k">for</span> <span class="n">ps</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wrapping a pytorch optimizer in fastai's <code>OptimWrapper</code> enables its use with fastai</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ps</span> <span class="o">=</span>  <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])]</span> <span class="c1">#, tensor([4,5,6])]</span>

<span class="n">adaf</span> <span class="o">=</span> <span class="n">Adafactor</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">mom</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">test_adaf</span> <span class="o">=</span> <span class="n">adafactor</span><span class="p">(</span><span class="n">param_groups</span><span class="o">=</span><span class="n">ps</span><span class="p">,</span> <span class="n">mom</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>

<span class="c1">#Access to param_groups</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">test_adaf</span><span class="o">.</span><span class="n">param_lists</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">adaf</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
<span class="c1">#Set param_groups</span>
<span class="n">test_adaf</span><span class="o">.</span><span class="n">param_lists</span> <span class="o">=</span> <span class="p">[[</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])]]</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">test_adaf</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)])</span>
<span class="c1">#Access to hypers</span>
<span class="c1"># test_eq(test_adaf.hypers, [{**adaf.defaults}])</span>
<span class="c1"># #Set hypers</span>
<span class="n">test_adaf</span><span class="o">.</span><span class="n">set_hyper</span><span class="p">(</span><span class="s1">&#39;mom&#39;</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">test_adaf</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;mom&#39;</span><span class="p">],</span> <span class="mf">0.95</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

