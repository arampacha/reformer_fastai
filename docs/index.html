---

title: Reformer Reproducibility Experiments


keywords: fastai
sidebar: home_sidebar

summary: "Fastai community entry to <a href='https://paperswithcode.com/rc2020'>2020 Papers With Code Reproducibility Challenge</a>"
description: "Fastai community entry to <a href='https://paperswithcode.com/rc2020'>2020 Papers With Code Reproducibility Challenge</a>"
nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Our-Reproducibility-Challenge-Submission">Our Reproducibility Challenge Submission<a class="anchor-link" href="#Our-Reproducibility-Challenge-Submission"> </a></h2><ul>
<li>Our OpenReview paper submission to the challenge can be found <a href="https://openreview.net/forum?id=3s8Y7dHYkN-">here</a> </li>
<li>Our Weights &amp; Biases Report, with interactive charts, is available <a href="https://wandb.ai/fastai_community/reformer-fastai/reports/Reformer-Reproducibility-Report---Vmlldzo0MzQ1OTg">here</a> </li>
</ul>
<h2 id="Installation">Installation<a class="anchor-link" href="#Installation"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Setup">Setup<a class="anchor-link" href="#Setup"> </a></h3><p>If you don't already, its a good idea to install the package into a virtual environment</p>

<pre><code>python3 -m venv my_env
source ./my_env/bin/activate</code></pre>
<h3 id="Install">Install<a class="anchor-link" href="#Install"> </a></h3><p>Then you can install the package via pip:</p>
<p><code>pip install reformer-fastai</code></p>
<p>Or (even better) install latest version from github:</p>
<p><code>pip install git+git://github.com/arampacha/reformer_fastai.git</code></p>
<h3 id="Contributing">Contributing<a class="anchor-link" href="#Contributing"> </a></h3><p>This project used nbdev for all development, see <a href="https://nbdev.fast.ai/">their docs here</a> to install nbdev and get started. Once you have nbdev installed we suggest you follow the suggested <a href="https://github.com/arampacha/reformer_fastai/blob/master/CONTRIBUTING.md">contributor workflow</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Running-Experiments">Running Experiments<a class="anchor-link" href="#Running-Experiments"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A pip installed version of this library is needed to run experiments. All experiments are run using the <a href="/reformer_fastai/experiment-script.html#run_exp"><code>run_exp</code></a> command, followed by the particular task name and then the parameters related to that task. <code>run_exp --help</code> will display a list of all parameters as well as a brief description. For brevity, an example of how to run a Reformer Language Model experiment is show below, <strong>a list of all experiment commands can be found here</strong></p>
<h3 id="Example:-Reversible-Language-Model">Example: Reversible Language Model<a class="anchor-link" href="#Example:-Reversible-Language-Model"> </a></h3><p>Below is an example of the code used that generated the results in Section 4.4 "Effect of reversible layers" of our submission paper.</p>

<pre><code>run_exp "lm_rev" \
        --n_epochs=10 \
        --bs=2 \
        --max_seq_len=4096 \
        --grad_accum=8 \
        --save_model=True  \
        --clip=0.5 \
        --seed=444 \
        --precision=2 \
        --do_wandb_logging=False \</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hyperparameters-Used">Hyperparameters Used<a class="anchor-link" href="#Hyperparameters-Used"> </a></h2><p>The main hyperparameters used are documented in the <a href="https://arampacha.github.io/reformer_fastai/experiment.experiment-commands.html">Experiment Commands</a> page and the <a href="https://arampacha.github.io/reformer_fastai/experiment-configs.html">Experiment Configs</a> page. In addition, a full list of our hyperparameters can be found in the Run Sets tables of <a href="https://wandb.ai/fastai_community/reformer-fastai/reports/Reformer-Reproducibility-Report---Vmlldzo0MzQ1OTg">our Weights &amp; Biases Report</a>. To see these, navigate to the experiment of interests, click on the "Run Set" button under each chart and scroll across to find all hyperparameters.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results">Results<a class="anchor-link" href="#Results"> </a></h2><p>All full description of our results, including charts and tables can be found in our paper <a href="https://openreview.net/forum?id=3s8Y7dHYkN-">here on OpenReview</a>. Our results are summarised as follows:</p>
<blockquote><p>Claims around speed on longer sequences and reduced memory footprint were validated; as sequence length
increased, Locality Sensitive Hashing ("LSH") Attention became faster and increasing the number of hashes improved
performance. We could not achieve the performance of a traditional Transformer with Reformer. Some experiments
were not run for as long as in the paper due to a lack of computational resources. Potentially the under-performance
of our Reformer may be due to under-training, implementation differences or nuances in JAX vs Pytorch. Also,
exploding gradients were encountered with mixed precision training and several model settings were found to be
unstable depending on the random seed or learning rate.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Trained-Models">Trained Models<a class="anchor-link" href="#Trained-Models"> </a></h2><p>All trained models from this project can be found in our <a href="https://wandb.ai/fastai_community/reformer-fastai/artifacts">Weights &amp; Biases project here</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluation">Evaluation<a class="anchor-link" href="#Evaluation"> </a></h2><p>An notebook example of running evaluation on one of our trained models saved on Weights &amp; Biases can be <a href="https://github.com/arampacha/reformer_fastai/blob/master/nbs/61_evaluation.ipynb">found here</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Project-Links">Project Links<a class="anchor-link" href="#Project-Links"> </a></h2><ul>
<li><a href="https://openreview.net/forum?id=3s8Y7dHYkN-">Our OpenReview paper submission</a></li>
<li><a href="https://wandb.ai/fastai_community/reformer-fastai/reports/Reformer-Reproducibility-Final-Edits---Vmlldzo0MzQ1OTg">Reformer Reproducibility Report on WandB</a></li>
<li><a href="https://arampacha.github.io/reformer_fastai/">Our project documentation</a></li>
<li><a href="https://forums.fast.ai/t/reproducibility-challenge-2020-fastai-folks-interested/80336/39">Fastai forums thread</a></li>
<li><a href="https://docs.google.com/document/d/1wF83E3B3yXIGZixEgOUJI2T2XXhT1DVCrPXS5Dbsyh8/edit">Google doc used for early planning</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Resources">Resources<a class="anchor-link" href="#Resources"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Author's-Code-and-Resources">Author's Code and Resources<a class="anchor-link" href="#Author's-Code-and-Resources"> </a></h3><ul>
<li><a href="https://openreview.net/pdf?id=rkgNKkHtvB">Reformer Paper</a></li>
<li><a href="https://iclr.cc/virtual_2020/poster_rkgNKkHtvB.html">Authors ICLR video</a></li>
<li><a href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html">Google Blog</a></li>
<li><a href="https://github.com/google/trax/tree/master/trax/models/reformer">Authors code (TRAX)</a></li>
<li><a href="https://github.com/google/trax/blob/f8024e8057599b92fce82842f342cb3d39c8f405/trax/supervised/configs/reformer_enwik8.gin">Reformer enwik8 model and training config</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="More-Code">More Code<a class="anchor-link" href="#More-Code"> </a></h3><ul>
<li><a href="https://github.com/lucidrains/reformer-pytorch/">@lucidrainâ€™s Reformer code</a></li>
<li><a href="https://github.com/huggingface/transformers/blob/a1bbcf3f6c20e15fe799a8659d6b7bd36fdf11ed/src/transformers/modeling_reformer.py">HuggingFace: Reformer source code</a><ul>
<li><a href="https://github.com/google/trax/blob/master/trax/supervised/configs/reformer_enwik8.gin">Reformer enwik8 configs</a></li>
<li><a href="https://github.com/google/trax/blob/master/trax/supervised/configs/reformer_wmt_ende.gin">Reformer WMT14 en-de configs</a></li>
<li><a href="https://github.com/google/trax/blob/a0483a12cb7ebece40b5e302e8e81fd9249c6ef6/trax/models/reformer/machine_translation.ipynb">Reformer Machine Translation example</a>
= <a href="https://github.com/tensorflow/tensor2tensor/blob/21dba2c1bdcc7ab582a2bfd8c0885c217963bb4f/tensor2tensor/data_generators/text_encoder.py#L448">SubwordTextEncoder tokenizer used for Machine Translation</a></li>
</ul>
</li>
<li><a href="https://colab.research.google.com/github/patrickvonplaten/blog/blob/master/notebooks/03_reformer.ipynb">HuggingFace: Reformer notebook example</a></li>
<li><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb">HuggingFace: long sequences</a></li>
<li><a href="https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing">HuggingFace: Pretraining</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data">Data<a class="anchor-link" href="#Data"> </a></h3><p>Tokenizers used with these datasets can be <a href="https://arampacha.github.io/reformer_fastai/tokenizers.html">found here</a></p>
<p><strong>enwik8</strong></p>
<ul>
<li><a href="http://mattmahoney.net/dc/enwik8.zip">enwik8.zip, raw data, 100mb</a></li>
<li><a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/enwik8.py">Tensor2Tensor enwik8 data generator code, with train/dev/test split</a>. File lengths:<ul>
<li>Train: 89,621,832</li>
<li>Eval: 5,000,000</li>
<li>Test: 5,000,000</li>
</ul>
</li>
<li>Tokenier used: ByteTextTokenizer</li>
</ul>
<p><strong>WMT14</strong></p>
<ul>
<li><a href="https://huggingface.co/datasets/viewer/?dataset=wmt14&amp;config=cs-en">WMT on HuggingFace Datasets</a></li>
<li><a href="https://github.com/google/trax/tree/a0483a12cb7ebece40b5e302e8e81fd9249c6ef6/trax/models/reformer/testdata">Reformer pre-trained WMT14 vocab</a><ul>
<li>Vocab size = 33300, from <a href="https://github.com/google/trax/blob/master/trax/supervised/configs/reformer_wmt_ende.gin">WMT14 model config</a></li>
</ul>
</li>
<li>Train Test split: newstest2013 for validation and newstest2014 for test, in consistence with Vaswani et al. (2017) - from <a href="https://arxiv.org/pdf/2009.02070.pdf">https://arxiv.org/pdf/2009.02070.pdf</a></li>
<li>Tokenizer used: SubWordTextEncoder</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Explainers">Explainers<a class="anchor-link" href="#Explainers"> </a></h3><ul>
<li><a href="https://www.youtube.com/watch?v=i4H0kjxrias&amp;t=1s">Yannic K explainer</a></li>
<li><a href="https://huggingface.co/blog/reformer">HuggingFace blog post</a></li>
<li><a href="https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0">Illustrating the Reformer blog post</a></li>
<li><a href="https://www.pragmatic.ml/reformer-deep-dive/">Reformer Deep Dive</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Related">Related<a class="anchor-link" href="#Related"> </a></h3><ul>
<li><a href="https://www.coursera.org/learn/attention-models-in-nlp">Coursera Attention Models in NLP course, with Reformer co-author</a></li>
<li><a href="https://hallvagi.github.io/dl-explorer/fastai/attention/lstm/2020/06/29/Attention.html">@hallvagi Attention blogpost</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html">The Transformer Family by @lilianweng</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms">A Family of Attention Mechanisms bu @lilianweng</a></li>
</ul>

</div>
</div>
</div>
</div>
 

