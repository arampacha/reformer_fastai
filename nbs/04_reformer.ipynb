{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.autograd.function import Function\n",
    "from torch.utils.checkpoint import get_device_states, set_device_states\n",
    "from functools import wraps\n",
    "\n",
    "from fastai.basics import *\n",
    "from reformer_fastai.core import *\n",
    "from reformer_fastai.layers import *\n",
    "from reformer_fastai.attention import *\n",
    "from reformer_fastai.transformer import LMMixin, EncDecMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "from torch.autograd import profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to @lucidrains https://github.com/lucidrains/reformer-pytorch\n",
    "\n",
    "raw version to be added LSH attention and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Chunk(Module):\n",
    "    \"Applies fn to input chunked along dim\"\n",
    "    def __init__(self, chunks:int, fn:Module, dim:int=-1):\n",
    "        store_attr()\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.chunks == 1:\n",
    "            return self.fn(x, **kwargs)\n",
    "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
    "        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ChunkedFeedForward(Module):\n",
    "    \"Applies positionwise feed-forward layer to input chunced along dim\"\n",
    "    def __init__(self, d:int, d_ff:int=None, chunks:int=1, dropout:float=0., dim:int=-1):\n",
    "        d_ff = default(d_ff, 4*d)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d),\n",
    "            nn.Dropout(dropout)\n",
    "            )\n",
    "        self.chunks = chunks\n",
    "        self.dim = dim\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.chunks == 1:\n",
    "            return self.net(x)\n",
    "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
    "        return torch.cat([self.net(c) for c in chunks], dim = self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "sl = 64\n",
    "d = 128\n",
    "x = torch.randn(bs, sl, d)\n",
    "ff  = ChunkedFeedForward(d, chunks=8, dim=1)\n",
    "out = ff(x)\n",
    "assert out.size() == (bs, sl, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "ff  = PostNorm(d, Residual(ChunkedFeedForward(d, chunks=8, dim=1)))\n",
    "out = ff(x)\n",
    "assert out.size() == (bs, sl, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "# with profiler.profile(record_shapes=True, profile_memory=True) as prof:\n",
    "#     with profiler.record_function(\"fwd\"):\n",
    "#         out = ff(x)\n",
    "# print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Deterministic(Module):\n",
    "    \"\"\"\n",
    "    Wrapper module to ensure determinism for backward pass\n",
    "    following example for saving and setting rng here \n",
    "    https://pytorch.org/docs/stable/_modules/torch/utils/checkpoint.html\n",
    "    \"\"\"\n",
    "    def __init__(self, net:Module):\n",
    "        self.net = net\n",
    "        self.cpu_state = None\n",
    "        self.cuda_in_fwd = None\n",
    "        self.gpu_devices = None\n",
    "        self.gpu_states = None\n",
    "\n",
    "    def record_rng(self, *args):\n",
    "        self.cpu_state = torch.get_rng_state()\n",
    "        if torch.cuda._initialized:\n",
    "            self.cuda_in_fwd = True\n",
    "            self.gpu_devices, self.gpu_states = get_device_states(*args)\n",
    "\n",
    "    def forward(self, *args, record_rng = False, set_rng = False, **kwargs):\n",
    "        if record_rng:\n",
    "            self.record_rng(*args)\n",
    "\n",
    "        if not set_rng:\n",
    "            return self.net(*args, **kwargs)\n",
    "\n",
    "        rng_devices = []\n",
    "        if self.cuda_in_fwd:\n",
    "            rng_devices = self.gpu_devices\n",
    "\n",
    "        with torch.random.fork_rng(devices=rng_devices, enabled=True):\n",
    "            torch.set_rng_state(self.cpu_state)\n",
    "            if self.cuda_in_fwd:\n",
    "                set_device_states(self.gpu_devices, self.gpu_states)\n",
    "            return self.net(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add tests here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# heavily inspired by https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py\n",
    "# once multi-GPU is confirmed working, refactor and send PR back to source\n",
    "class ReversibleBlock(Module):\n",
    "    \"Applies f and g in reversible manner. Avoids storing outputs for backpropagation\"\n",
    "    def __init__(self, f:Module, g:Module, depth=None, send_signal=False):\n",
    "        store_attr('depth, send_signal')\n",
    "        self.f = Deterministic(f)\n",
    "        self.g = Deterministic(g)\n",
    "\n",
    "    def forward(self, x, f_args = {}, g_args = {}):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
    "        y1, y2 = None, None\n",
    "\n",
    "        if self.send_signal:\n",
    "            f_args['_reverse'] = g_args['_reverse'] = False\n",
    "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)\n",
    "            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)\n",
    "\n",
    "        return torch.cat([y1, y2], dim=2)\n",
    "\n",
    "    def backward_pass(self, y, dy, f_args = {}, g_args = {}):\n",
    "        y1, y2 = torch.chunk(y, 2, dim=2)\n",
    "        del y\n",
    "\n",
    "        dy1, dy2 = torch.chunk(dy, 2, dim=2)\n",
    "        del dy\n",
    "\n",
    "        if self.send_signal:\n",
    "            f_args['_reverse'] = g_args['_reverse'] = True\n",
    "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            y1.requires_grad = True\n",
    "            gy1 = self.g(y1, set_rng=True, **g_args)\n",
    "            torch.autograd.backward(gy1, dy2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x2 = y2 - gy1\n",
    "            del y2, gy1\n",
    "\n",
    "            dx1 = dy1 + y1.grad\n",
    "            del dy1\n",
    "            y1.grad = None\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            x2.requires_grad = True\n",
    "            fx2 = self.f(x2, set_rng=True, **f_args)\n",
    "            torch.autograd.backward(fx2, dx1, retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x1 = y1 - fx2\n",
    "            del y1, fx2\n",
    "\n",
    "            dx2 = dy2 + x2.grad\n",
    "            del dy2\n",
    "            x2.grad = None\n",
    "\n",
    "            x = torch.cat([x1, x2.detach()], dim=2)\n",
    "            dx = torch.cat([dx1, dx2], dim=2)\n",
    "        \n",
    "        return x, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 64\n",
    "d = 128\n",
    "x = torch.randn(bs, sl, d)\n",
    "# revblock is called on twin x\n",
    "x2 = torch.cat([x, x], dim=-1)\n",
    "attn = Attention(d)\n",
    "ff = ChunkedFeedForward(d, chunks=8, dim=-2)\n",
    "revblock = ReversibleBlock(attn, ff)\n",
    "out = revblock(x2)\n",
    "assert out.size() == (bs, sl, d*2)\n",
    "# no grads are stored\n",
    "out = torch.stack(out.chunk(2, dim=-1)).mean(dim=0)\n",
    "try: out.mean().backward()\n",
    "except RuntimeError as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "# with profiler.profile(record_shapes=True, profile_memory=True) as prof:\n",
    "#     with profiler.record_function(\"fwd\"):\n",
    "#         out = revblock(x2)\n",
    "# print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IrreversibleBlock(Module):\n",
    "    \"Mimics ReversibleBlock computation but gradients are computed as ussual\"\n",
    "    def __init__(self, f, g):\n",
    "        store_attr()\n",
    "    def forward(self, x, f_args={}, g_args={}):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
    "        y1 = x1 + self.f(x2, **f_args)\n",
    "        y2 = x2 + self.g(y1, **g_args)\n",
    "        return torch.cat([y1, y2], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(d)\n",
    "ff = ChunkedFeedForward(d, chunks=8, dim=-2)\n",
    "irrevblock = IrreversibleBlock(attn, ff)\n",
    "out = irrevblock(x2)\n",
    "assert out.size() == (bs, sl, d*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "# with profiler.profile(record_shapes=True, profile_memory=True) as prof:\n",
    "#     with profiler.record_function(\"fwd\"):\n",
    "#         out = irrevblock(x2)\n",
    "# print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _ReversibleFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, blocks, kwargs):\n",
    "        ctx.kwargs = kwargs\n",
    "        for block in blocks:\n",
    "            x = block(x, **kwargs)\n",
    "        ctx.y = x.detach()\n",
    "        ctx.blocks = blocks\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy):\n",
    "        y = ctx.y\n",
    "        kwargs = ctx.kwargs\n",
    "        for block in ctx.blocks[::-1]:\n",
    "            y, dy = block.backward_pass(y, dy, **kwargs)\n",
    "        return dy, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReversibleSequence(Module):\n",
    "    \"\"\"\n",
    "    Stack of ReversibleBlocks constructed from blocks.Applies ReversibleBlocks if\n",
    "    sequence length is > rev_thres or else IrreversibleBlocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, blocks, rev_thres = 0, send_signal = False):\n",
    "        self.rev_thres = rev_thres # uses revblocks if seq_len else irrev_blocks\n",
    "\n",
    "        self.blocks = nn.ModuleList([ReversibleBlock(f, g, depth, send_signal) for depth, (f, g) in enumerate(blocks)])\n",
    "        self.irrev_blocks = nn.ModuleList([IrreversibleBlock(f=f, g=g) for f, g in blocks])\n",
    "\n",
    "    def forward(self, x, arg_route = (True, True), **kwargs):\n",
    "        reverse = x.shape[1] > self.rev_thres\n",
    "        blocks = self.blocks if reverse else self.irrev_blocks\n",
    "\n",
    "        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)\n",
    "        block_kwargs = {'f_args': f_args, 'g_args': g_args}\n",
    "\n",
    "        if not reverse:\n",
    "            for block in blocks:\n",
    "                x = block(x, **block_kwargs)\n",
    "            return x\n",
    "\n",
    "        return _ReversibleFunction.apply(x, blocks, block_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "sl = 64\n",
    "d = 128\n",
    "x = torch.randn(bs, sl, d)\n",
    "x2 = torch.cat([x, x], dim=-1)\n",
    "blocks = []\n",
    "for i in range(2):\n",
    "    f = PreNorm(d, Attention(d))\n",
    "    g = PreNorm(d, FeedForward(d))\n",
    "    blocks.append(nn.ModuleList([f, g]))\n",
    "layers = ReversibleSequence(nn.ModuleList(blocks))\n",
    "out = layers(x2)\n",
    "assert out.size() == (bs, sl, 2*d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "sl = 64\n",
    "d = 128\n",
    "x = torch.randn(bs, sl, d)\n",
    "x2 = torch.cat([x, x], dim=-1)\n",
    "blocks = []\n",
    "for i in range(2):\n",
    "    f = PreNorm(d, LSHSelfAttention(d, bucket_size=16))\n",
    "    g = PreNorm(d, FeedForward(d))\n",
    "    blocks.append(nn.ModuleList([f, g]))\n",
    "layers = ReversibleSequence(nn.ModuleList(blocks))\n",
    "out = layers(x2, arg_route=(True, False), _reverse=False, _depth=1)\n",
    "assert out.size() == (bs, sl, 2*d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "try: out.mean().backward()\n",
    "except RuntimeError as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "# with profiler.profile(record_shapes=True, profile_memory=True) as prof:\n",
    "#     with profiler.record_function(\"fwd\"):\n",
    "#         out = layers(x2)\n",
    "# print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReversibleTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReversibleEncoder(Module):\n",
    "    \"Stack of ReversibleBlocks\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6, \n",
    "                 n_heads:int = 8, \n",
    "                 max_seq_len:int = 512,\n",
    "                 ff_chunks:int = 1, \n",
    "                 causal:bool = False,\n",
    "                 attn_dropout:float = 0.,\n",
    "                 post_attn_dropout:float = None,\n",
    "                 attn_bias:bool=False,\n",
    "                 ff_dropout:float = 0.,\n",
    "                 d_ff:int = None,\n",
    "                 prenorm:bool=True,\n",
    "                 final_norm:Module=None,\n",
    "                 rev_thres:int=0):\n",
    "        # store_attr()\n",
    "\n",
    "        blocks = []\n",
    "        norm_wrapper = PreNorm if prenorm else PostNorm\n",
    "        for ind in range(n_layers):\n",
    "            attn = Attention(d_model, n_heads, causal=causal, dropout=attn_dropout, out_dropout=post_attn_dropout, bias=attn_bias)\n",
    "            ff = ChunkedFeedForward(d_model, d_ff, chunks=ff_chunks, dropout=ff_dropout, dim=1)\n",
    "\n",
    "            f = norm_wrapper(d_model, attn)\n",
    "            g = norm_wrapper(d_model, ff)\n",
    "\n",
    "            blocks.append(nn.ModuleList([f, g]))\n",
    "        self.norm = final_norm(d_model) if exists(final_norm) else None\n",
    "        # send_signal is not implemented for now\n",
    "        self.layers = ReversibleSequence(nn.ModuleList(blocks), rev_thres=rev_thres, send_signal=False)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.cat([x, x], dim = -1)\n",
    "        arg_route = (False, False)\n",
    "        # pdb.set_trace()\n",
    "        x = self.layers(x, arg_route = arg_route, **kwargs)\n",
    "        x = torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n",
    "        if exists(self.norm): x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = ReversibleEncoder(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReversibleDecoder(Module):\n",
    "    \"Stack of ReversibleBlocks. Uses AdditiveAttention.\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers = 6, \n",
    "                 heads = 8,  \n",
    "                 max_seq_len = 512,\n",
    "                 d_head = None, \n",
    "                 bucket_size = 64, \n",
    "                 n_hashes = 8, \n",
    "                 ff_chunks = 100, \n",
    "                 attn_chunks = None, # ??\n",
    "                 attn_dropout = 0.,\n",
    "                 post_attn_dropout = None,\n",
    "                 attn_bias:bool=False,\n",
    "                 ff_dropout = 0.,  \n",
    "                 d_ff = None,\n",
    "                 prenorm=True,\n",
    "                 final_norm:Module=None,\n",
    "                 rev_thres = 0,\n",
    "                 ):\n",
    "        store_attr('d_model,n_layers')\n",
    "        \n",
    "        get_attn = lambda: AdditiveAttention(d_model, heads, causal=True, dropout=attn_dropout, out_dropout=post_attn_dropout, bias=attn_bias)\n",
    "        get_ff = lambda: ChunkedFeedForward(d_model, d_ff, chunks=ff_chunks, dropout=ff_dropout, dim=1)\n",
    "        norm_wrapper = PreNorm if prenorm else PostNorm\n",
    "        blocks = []\n",
    "        for ind in range(n_layers):\n",
    "            f = norm_wrapper(d_model, get_attn())\n",
    "            g = norm_wrapper(d_model, get_ff())\n",
    "\n",
    "            blocks.append(nn.ModuleList([f, g]))\n",
    "        self.norm = final_norm(d_model) if exists(final_norm) else None\n",
    "        # send_signal is not implemented for now\n",
    "        self.layers = ReversibleSequence(nn.ModuleList(blocks), rev_thres=rev_thres, send_signal=False)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.cat([x, x], dim = -1)\n",
    "        arg_route = (True, False)\n",
    "        # pdb.set_trace()\n",
    "        x = self.layers(x, arg_route = arg_route, **kwargs)\n",
    "        x = torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n",
    "        if exists(self.norm): x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = ReversibleDecoder(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReversibleLM(Module, LMMixin):\n",
    "    \"\"\"\n",
    "    Reversible Transformer for language modelling\n",
    "    \n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - if True projection layers attention modules will have bias\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "        * rev_thres: int - if (seq_len < rev_thres) applies irreversible blocks\n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz:int, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6,\n",
    "                 n_heads:int=8,\n",
    "                 d_ff:int=None,\n",
    "                 attn_dropout:float=0.1,\n",
    "                 ff_dropout:float=0.1,\n",
    "                 emb_dropout:float=0.1,\n",
    "                 tie_weights:bool=True,\n",
    "                 causal:bool=True,\n",
    "                 pos_enc:str='absolute',\n",
    "                 max_seq_len:int=512,\n",
    "                 axial_shape=None,\n",
    "                 axial_emb_dims=None,\n",
    "                 pad_idx:int=None,\n",
    "                 prenorm:bool=False,\n",
    "                 attn_bias:bool=False,\n",
    "                 rev_thres:int=0):\n",
    "        store_attr()\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, \n",
    "                                        pos_enc=pos_enc, axial_shape=axial_shape, \n",
    "                                        axial_emb_dims=axial_emb_dims)\n",
    "        self.encoder = ReversibleEncoder(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,\n",
    "                                         attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                         prenorm=prenorm, attn_bias=attn_bias,\n",
    "                                         final_norm=nn.LayerNorm, rev_thres=rev_thres)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = ReversibleLM(vocab_sz, d, n_layers=2, causal=False)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO test weight tying\n",
    "# Note on weight tying: it's done like here in fastai AWD_LSTM model\n",
    "# Lucidrains does it with custom MatrixMultiply module https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py#L106\n",
    "#TODO: update docstrings\n",
    "class ReversibleTransformer(Module):\n",
    "    \"\"\"\n",
    "    Basic Transformer Encoder-Decoder model\n",
    "    Parameters:\n",
    "        * enc_vocab_sz: int - source vocab size \n",
    "        * dec_vocab_sz: int - target vocab size\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_enc_layers: int (default: 6) \n",
    "        * n_dec_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * prenorm: bool - whether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - whether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, if pad_idx is provided, and no mask/context_mask are \n",
    "                passed to forward method will be used to generate padding masks\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * shared_emb: bool - if True encoder and decoder will use shared embedding layer\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * src - source input ids, shape [bs, src_sl]\n",
    "        * tgt - target input ids, shape [bs, tgt_sl]\n",
    "        * src_mask - optional boolean source mask, shape [bs, src_sl]\n",
    "        * tgt_mask - optional boolean target mask, shape [bs, tgt_sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, tgt_sl, tgt_vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 enc_vocab_sz, \n",
    "                 dec_vocab_sz, \n",
    "                 d_model,\n",
    "                 n_layers:int=6,\n",
    "                 n_enc_layers=None,\n",
    "                 n_dec_layers=None,\n",
    "                 n_heads=8,\n",
    "                 d_ff=None,\n",
    "                 pad_idx=None, \n",
    "                 tie_weights=True,\n",
    "                 shared_emb = False,\n",
    "                 attn_dropout=0.1, \n",
    "                 ff_dropout=0.1, \n",
    "                 emb_dropout=0.1,\n",
    "                 prenorm=False, \n",
    "                 attn_bias=False,\n",
    "                 comb_attn=False, \n",
    "                 pos_enc='absolute', \n",
    "                 max_seq_len=512, \n",
    "                 axial_shape=None, \n",
    "                 axial_emb_dims=None):\n",
    "        store_attr()\n",
    "        n_enc_layers = ifnone(n_enc_layers, n_layers)\n",
    "        n_dec_layers = ifnone(n_dec_layers, n_layers)\n",
    "        self.enc_emb = TransformerEmbedding(enc_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
    "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        if shared_emb:\n",
    "            assert (enc_vocab_sz == dec_vocab_sz), \"Encoder and decoder vocab size doesn't match\"\n",
    "            self.dec_emb = self.enc_emb\n",
    "        else:\n",
    "            self.dec_emb = TransformerEmbedding(dec_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
    "                                                axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        \n",
    "        self.encoder = ReversibleEncoder(d_model, n_enc_layers, n_heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                          prenorm=prenorm, attn_bias=attn_bias, final_norm=nn.LayerNorm, causal=False)\n",
    "        self.decoder = ReversibleDecoder(d_model, n_dec_layers, n_heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                          prenorm=prenorm, attn_bias=attn_bias, final_norm=nn.LayerNorm)\n",
    "        self.proj = nn.Linear(d_model, dec_vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.dec_emb.emb.weight\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "        tgt_mask = default(tgt_mask, self.get_padding_mask(tgt))\n",
    "        enc = self.encoder(self.enc_emb(src), mask=src_mask)\n",
    "        out = self.decoder(self.dec_emb(tgt), context=enc, mask=tgt_mask, context_mask=src_mask)\n",
    "        return self.proj(out)\n",
    "    \n",
    "    def get_padding_mask(self, x):\n",
    "        if self.pad_idx is None: return None\n",
    "        return (x != self.pad_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "src_sl = 70\n",
    "tgt_sl = 80\n",
    "d = 64\n",
    "src_vocab_sz = 256\n",
    "tgt_vocab_sz = 256\n",
    "src = torch.randint(src_vocab_sz, (bs, src_sl))\n",
    "tgt = torch.randint(tgt_vocab_sz, (bs, tgt_sl))\n",
    "model = ReversibleTransformer(src_vocab_sz, tgt_vocab_sz, d, n_enc_layers=2, n_dec_layers=2)\n",
    "out = model(src, tgt)\n",
    "assert (out.size() == (bs, tgt_sl, tgt_vocab_sz))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer with LSH attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSHEncoderBlock(Module):\n",
    "    \"Encoder block using ReformerAttention\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 d_ff:int = None, \n",
    "                 attn_dropout:float = 0.1,\n",
    "                 ff_dropout:float = 0.1,\n",
    "                 causal:bool = False, \n",
    "                 attn_bias:bool = False, \n",
    "                 prenorm:bool=False,\n",
    "                 use_lsh:bool=True,\n",
    "                 n_hashes:int=8,\n",
    "                 bucket_size:int=64, \n",
    "                 seed:int=None):\n",
    "        store_attr('attn_dropout') # mb separate argument attn_post_dropout\n",
    "        if prenorm:\n",
    "            self.attn = Residual(PreNorm(d_model, ReformerAttentionV2(d_model, n_heads=n_heads, causal=causal, \n",
    "                                                    dropout=attn_dropout, bias=attn_bias, use_lsh=use_lsh,\n",
    "                                                    n_hashes=n_hashes, bucket_size=bucket_size, \n",
    "                                                    seed=seed)))\n",
    "            self.ff = Residual(PreNorm(d_model, FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        else:\n",
    "            self.attn = PostNorm(d_model, Residual(ReformerAttentionV2(d_model, n_heads=n_heads, causal=causal, \n",
    "                                                    dropout=attn_dropout, bias=attn_bias, use_lsh=use_lsh,\n",
    "                                                    n_hashes=n_hashes, bucket_size=bucket_size, \n",
    "                                                    seed=seed)))\n",
    "            self.ff = PostNorm(d_model, Residual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        out = self.attn(x, mask=mask)\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = LSHEncoderBlock(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LSHEncoderBlock(d, use_lsh=False)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSHEncoder(Module):\n",
    "    \"\"\"Stack of TransformerEncoderBlocks\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None,\n",
    "                 ff_dropout=0.1, \n",
    "                 attn_dropout=0.1,\n",
    "                 attn_bias=False,\n",
    "                 causal=False, \n",
    "                 prenorm=False,\n",
    "                 use_lsh:bool=True,\n",
    "                 final_norm=None,\n",
    "                 n_hashes:int=8,\n",
    "                 bucket_size:int=64,\n",
    "                 seed:int=None):\n",
    "        store_attr('d_model')\n",
    "        self.layers = nn.ModuleList([])    \n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(LSHEncoderBlock(d_model, n_heads, causal=causal, \n",
    "                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                    prenorm=prenorm, attn_bias=attn_bias, use_lsh=use_lsh,\n",
    "                                    n_hashes=n_hashes, bucket_size=bucket_size, \n",
    "                                    seed=seed))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers: x = layer(x, mask=mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = LSHEncoder(d, n_layers=2)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LSHEncoder(d, n_layers=2, n_heads=4, use_lsh=False)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSHLM(Module, LMMixin):\n",
    "    \"\"\"\n",
    "    Transformer for language modelling with LSH attention\n",
    "    \n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "        * use_slh: bool - parameter to switch between LSH and full attention\n",
    "        * n_hashes: int - number of hashing rounds for LSH\n",
    "        * bucket_size: int - input sequence length should be divisible by 2*bucket_size\n",
    "        * seed: int - for LSHAttention module\n",
    "        \n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "        \n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz:int, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6,\n",
    "                 n_heads:int=8,\n",
    "                 d_ff:int=None,\n",
    "                 attn_dropout:float=0.1,\n",
    "                 ff_dropout:float=0.1,\n",
    "                 emb_dropout:float=0.1,\n",
    "                 tie_weights:bool=True,\n",
    "                 causal:bool=True,\n",
    "                 pos_enc:str='absolute',\n",
    "                 max_seq_len:int=512,\n",
    "                 axial_shape:tuple=None,\n",
    "                 axial_emb_dims:tuple=None,\n",
    "                 pad_idx:int=None,\n",
    "                 prenorm:bool=False,\n",
    "                 attn_bias:bool=False,\n",
    "                 use_lsh:bool=True,\n",
    "                 n_hashes:int=8,\n",
    "                 bucket_size:int=64, \n",
    "                 seed:int=None):\n",
    "        store_attr()\n",
    "        self._use_lsh = use_lsh\n",
    "        self._n_hashes = n_hashes\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, \n",
    "                                        pos_enc=pos_enc, axial_shape=axial_shape, \n",
    "                                        axial_emb_dims=axial_emb_dims)\n",
    "        final_norm = nn.LayerNorm if prenorm else None\n",
    "        self.encoder = LSHEncoder(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,\n",
    "                                  attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                  prenorm=prenorm, attn_bias=attn_bias, use_lsh=use_lsh,\n",
    "                                  final_norm=final_norm, n_hashes=n_hashes, bucket_size=bucket_size,\n",
    "                                  seed=seed)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)\n",
    "    \n",
    "    @property\n",
    "    def use_lsh(self):\n",
    "        return self._use_lsh\n",
    "    @use_lsh.setter\n",
    "    def use_lsh(self, val):\n",
    "        self._use_lsh = val\n",
    "        for c in self.children():\n",
    "            for m in c.modules():\n",
    "                if hasattr(m, 'use_lsh'): m.use_lsh=val\n",
    "    @property\n",
    "    def n_hashes(self):\n",
    "        return self._n_hashes\n",
    "    @n_hashes.setter\n",
    "    def n_hashes(self, val):\n",
    "        self._n_hashes = val\n",
    "        for c in self.children():\n",
    "            for m in c.modules():\n",
    "                if hasattr(m, 'n_hashes'): m.n_hashes=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = LSHLM(vocab_sz, d, n_layers=2, causal=False)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 ms ± 432 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "model.use_lsh = True\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))\n",
    "%timeit model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56 ms ± 10 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "model.use_lsh = False\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))\n",
    "%timeit model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "bs = 4\n",
    "sl = 100\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = LSHLM(vocab_sz, d, n_layers=2, causal=False, use_lsh=False)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReformerEncoder(Module):\n",
    "    \"Stack of ReversibleBlocks\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6, \n",
    "                 n_heads:int = 8, \n",
    "                 max_seq_len:int = 512,\n",
    "                 ff_chunks:int = 1, \n",
    "                 causal:bool = False,\n",
    "                 attn_dropout:float = 0.,\n",
    "                 post_attn_dropout:float = None,\n",
    "                 attn_bias:bool=False,\n",
    "                 ff_dropout:float = 0.,\n",
    "                 d_ff:int = None,\n",
    "                 prenorm:bool=True,\n",
    "                 final_norm:Module=None,\n",
    "                 rev_thres:int=0,\n",
    "                 use_lsh:bool=True,\n",
    "                 n_hashes:int=8,\n",
    "                 bucket_size:int=64,\n",
    "                 seed:int=None):\n",
    "        # store_attr()\n",
    "        blocks = []\n",
    "        norm_wrapper = PreNorm if prenorm else PostNorm\n",
    "        for ind in range(n_layers):\n",
    "            attn = ReformerAttentionV2(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout,\n",
    "                                       bias=attn_bias, use_lsh=use_lsh, n_hashes=n_hashes, bucket_size=bucket_size,\n",
    "                                       seed=seed)\n",
    "            ff = ChunkedFeedForward(d_model, d_ff, chunks=ff_chunks, dropout=ff_dropout, dim=1)\n",
    "\n",
    "            f = norm_wrapper(d_model, attn)\n",
    "            g = norm_wrapper(d_model, ff)\n",
    "            blocks.append(nn.ModuleList([f, g]))\n",
    "        self.norm = final_norm(d_model) if exists(final_norm) else None\n",
    "        self.layers = ReversibleSequence(nn.ModuleList(blocks), rev_thres=rev_thres, send_signal=True)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.cat([x, x], dim = -1)\n",
    "        arg_route = (True, False)\n",
    "        # pdb.set_trace()\n",
    "        x = self.layers(x, arg_route = arg_route, **kwargs)\n",
    "        x = torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n",
    "        if exists(self.norm): x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = ReformerEncoder(d, n_layers=2)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReformerLM(Module, LMMixin):\n",
    "    \"\"\"\n",
    "    Reformer for language modelling. Uses LSH or full sharedQK attention\n",
    "    \n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "        * rev_thres: int - if (seq_len < rev_thres) applies irreversible blocks\n",
    "        * use_slh: bool - parameter to switch between LSH and full attention\n",
    "        * n_hashes: int - number of hashing rounds for LSH\n",
    "        * bucket_size: int - input sequence length should be divisible by 2*bucket_size\n",
    "        * seed: int - for LSHAttention module\n",
    "        \n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "        \n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz:int, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6,\n",
    "                 n_heads:int=8,\n",
    "                 d_ff:int=None,\n",
    "                 ff_chunks:int=1, \n",
    "                 attn_dropout:float=0.1,\n",
    "                 ff_dropout:float=0.1,\n",
    "                 emb_dropout:float=0.1,\n",
    "                 tie_weights:bool=True,\n",
    "                 causal:bool=True,\n",
    "                 pos_enc:str='absolute',\n",
    "                 max_seq_len:int=512,\n",
    "                 axial_shape:tuple=None,\n",
    "                 axial_emb_dims:tuple=None,\n",
    "                 pad_idx:int=None,\n",
    "                 prenorm:bool=False,\n",
    "                 attn_bias:bool=False,\n",
    "                 use_lsh:bool=True,\n",
    "                 n_hashes:int=8,\n",
    "                 bucket_size:int=64,\n",
    "                 rev_thres:int=0,\n",
    "                 seed:int=None):\n",
    "        store_attr()\n",
    "        self._use_lsh = use_lsh\n",
    "        self._n_hashes = n_hashes\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, \n",
    "                                        pos_enc=pos_enc, axial_shape=axial_shape, \n",
    "                                        axial_emb_dims=axial_emb_dims)\n",
    "        final_norm = nn.LayerNorm if prenorm else None\n",
    "        self.encoder = ReformerEncoder(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,\n",
    "                                      attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                      prenorm=prenorm, attn_bias=attn_bias, use_lsh=use_lsh,\n",
    "                                      final_norm=final_norm, n_hashes=n_hashes, bucket_size=bucket_size,\n",
    "                                      ff_chunks=ff_chunks, rev_thres=rev_thres, seed=seed)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)\n",
    "    \n",
    "    @property\n",
    "    def use_lsh(self):\n",
    "        return self._use_lsh\n",
    "    @use_lsh.setter\n",
    "    def use_lsh(self, val):\n",
    "        self._use_lsh = val\n",
    "        for c in self.children():\n",
    "            for m in c.modules():\n",
    "                if hasattr(m, 'use_lsh'): m.use_lsh=val\n",
    "    @property\n",
    "    def n_hashes(self):\n",
    "        return self._n_hashes\n",
    "    @n_hashes.setter\n",
    "    def n_hashes(self, val):\n",
    "        self._n_hashes = val\n",
    "        for c in self.children():\n",
    "            for m in c.modules():\n",
    "                if hasattr(m, 'n_hashes'): m.n_hashes=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "bs = 4\n",
    "sl = 100\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = ReformerLM(vocab_sz, d, n_layers=2, causal=False, use_lsh=False)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = ReformerLM(vocab_sz, d, n_layers=2, causal=False)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cached buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'buckets:0': tensor([[ 0,  1,  0,  ..., 15, 15, 14],\n",
      "        [ 0,  1,  0,  ..., 14, 14, 14],\n",
      "        [ 0,  1,  0,  ..., 15, 15, 14],\n",
      "        [ 0,  1,  0,  ..., 15, 15, 14]])}\n",
      "torch.Size([4, 1024])\n",
      "{'buckets:1': tensor([[ 1,  0,  1,  ..., 14, 14, 14],\n",
      "        [ 0,  0,  1,  ..., 14, 14, 14],\n",
      "        [ 1,  0,  1,  ..., 14, 15, 14],\n",
      "        [ 0,  0,  0,  ..., 14, 14, 14]])}\n",
      "torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "for m in model.modules():\n",
    "    if isinstance(m, LSHAttention):\n",
    "        print(m._cache)\n",
    "        for v in m._cache.values():\n",
    "            print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = out.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LSHAttention` execution time depends on number of hashing rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hashing rounds 8\n",
      "159 ms ± 73.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "print(f'Number of hashing rounds {model._n_hashes}')\n",
    "%timeit model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hashing rounds 1\n",
      "63.7 ms ± 958 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "model.n_hashes = 1\n",
    "print(f'Number of hashing rounds {model.n_hashes}')\n",
    "%timeit model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reformer_lm_splits(model):\n",
    "    \"Splits ReformerLM `model` into groups for differential learning rates.\"\n",
    "    groups = L([model.emb] + [l for l in model.encoder.layers.blocks] + [model.proj])\n",
    "    return groups.map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(reformer_lm_splits(model)) == 2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "#hide\n",
    "# class ReformerDecoder(nn.Module):\n",
    "#     def __init__(self, \n",
    "#                  d_model, \n",
    "#                  n_layers = 6, \n",
    "#                  heads = 8,  \n",
    "#                  max_seq_len = 512,\n",
    "#                  d_head = None, \n",
    "#                  bucket_size = 64, \n",
    "#                  n_hashes = 8, \n",
    "#                  ff_chunks = 100, \n",
    "#                  attn_chunks = None, # ??\n",
    "#                  causal = False, \n",
    "#                  weight_tie = False, # weight sharing option do we need to keep this?\n",
    "#                  attn_dropout = 0.,\n",
    "#                  post_attn_dropout = 0.,\n",
    "#                  ff_dropout = 0.,  \n",
    "#                  d_ff = None, \n",
    "#                  layer_dropout = 0.,\n",
    "#                  prenorm=True,\n",
    "#                  rev_thres = 0,\n",
    "#                  ):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.n_layers = n_layers\n",
    "        \n",
    "#         # use regular attention for now\n",
    "#         get_attn = lambda: DecoderAttention(d_model, heads, causal=causal, dropout=attn_dropout)\n",
    "#         get_ff = lambda: ChunkedFeedForward(d_model, d_ff, chunks=ff_chunks, dropout=ff_dropout, dim=1)\n",
    "#         norm_wrapper = PreNorm if prenorm else PostNorm\n",
    "#         blocks = []\n",
    "#         for ind in range(n_layers):\n",
    "#             layer_num = ind + 1\n",
    "            \n",
    "#             f = norm_wrapper(d_model, get_attn())\n",
    "#             g = norm_wrapper(d_model, get_ff())\n",
    "\n",
    "#             blocks.append(nn.ModuleList([f, g]))\n",
    "#         # send_signal is not implemented for now\n",
    "#         self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout=layer_dropout, rev_thres=rev_thres, send_signal=False)\n",
    "\n",
    "#     def forward(self, x, **kwargs):\n",
    "#         x = torch.cat([x, x], dim = -1)\n",
    "#         arg_route = (True, False)\n",
    "#         # pdb.set_trace()\n",
    "#         x = self.layers(x, arg_route = arg_route, **kwargs)\n",
    "#         return torch.stack(x.chunk(2, dim=-1)).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "#hide\n",
    "# class ReformerEncDec(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Basic Transformer Encoder-Decoder model\n",
    "#     Parameters:\n",
    "#         * enc_vocab_sz: int - source vocab size\n",
    "#         * dec_vocab_sz: int - target vocab size\n",
    "#         * d_model: int - inner dimension of the model\n",
    "#         * n_enc_layers: int (default: 6) \n",
    "#         * n_dec_layers: int (default: 6)\n",
    "#         * heads: int (default: 8)\n",
    "#         * d_ff: int - inner dimension of the FeedForward net, if None defaults to 4*d_model\n",
    "#         * attn_dropout: float - attention dropout\n",
    "#         * ff_dropout: float - feed-forward dropout\n",
    "#         * emb_dropout: float - embedding dropout\n",
    "#         * max_seq_len: int (default: 512)\n",
    "#         * prenorm: bool - whether to use PreNorm or PostNorm\n",
    "#         * attn_bias: bool - whether to allow biases in attention projection layers\n",
    "#         * pad_idx: int - padding token id, if pad_idx is provided, and no mask/context_mask are passed to \n",
    "#                 forward method will be used to generate padding masks\n",
    "#         * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "#         * shared_emb: bool - if True encoder and decoder will use shared embedding layer\n",
    "#         * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "#         * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "#                 max_seq_len\n",
    "#         * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "#     Inputs:\n",
    "#         * src - source input ids, shape [bs, src_sl]\n",
    "#         * tgt - target input ids, shape [bs, tgt_sl]\n",
    "#         * src_mask - optional boolean source mask, shape [bs, src_sl]\n",
    "#         * tgt_mask - optional boolean target mask, shape [bs, tgt_sl]\n",
    "#     Returns:\n",
    "#         * logits - target token logits, shape [bs, tgt_sl, tgt_vocab_sz]\n",
    "#     \"\"\"\n",
    "#     def __init__(self,\n",
    "#                  enc_vocab_sz, \n",
    "#                  dec_vocab_sz, \n",
    "#                  d_model, \n",
    "#                  n_layers=6, \n",
    "#                  heads=8, \n",
    "#                  max_seq_len=512, \n",
    "#                  pad_idx=None, \n",
    "#                  tie_weights=True,                  \n",
    "#                  emb_dropout=0.1,\n",
    "#                  attn_dropout=0.1, \n",
    "#                  ff_dropout=0.1,\n",
    "#                  pos_enc='absolute', \n",
    "#                  d_ff=None, \n",
    "#                  prenorm=False, \n",
    "#                  axial_shape=None, \n",
    "#                  axial_emb_dims=None,\n",
    "#                  comb_attn=False,\n",
    "#                  rev_thres=0):\n",
    "#         super().__init__()\n",
    "#         self.max_seq_len = max_seq_len\n",
    "#         self.n_layers = n_layers\n",
    "#         self.pad_idx = pad_idx\n",
    "#         self.enc_emb = TransformerEmbedding(enc_vocab_sz, d_model, max_seq_len, dropout=emb_dropout,\n",
    "#                                             axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "#         self.dec_emb = TransformerEmbedding(dec_vocab_sz, d_model, max_seq_len, dropout=emb_dropout,\n",
    "#                                             axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "#         self.encoder = ReformerEncoder(d_model, n_layers, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm, rev_thres=rev_thres)\n",
    "#         self.decoder = ReformerDecoder(d_model, n_layers, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm, rev_thres=rev_thres)\n",
    "#         self.proj = nn.Linear(d_model, dec_vocab_sz)\n",
    "#         if tie_weights: self.proj.weight = self.dec_emb.emb.weight\n",
    "\n",
    "#     def forward(self, src, tgt, src_mask = None, tgt_mask = None):\n",
    "#         src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "#         tgt_mask = default(tgt_mask, self.get_padding_mask(tgt))\n",
    "#         enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
    "#         out = self.decoder(self.dec_emb(tgt), context=enc, mask=tgt_mask, context_mask=src_mask)\n",
    "#         return self.proj(out)\n",
    "#     def get_padding_mask(self, x):\n",
    "#         if self.pad_idx is None: return None\n",
    "#         return (x != self.pad_idx)\n",
    "#     #TODO add beam search and refactor\n",
    "#     @torch.no_grad()\n",
    "#     def generate(self, src,\n",
    "#                 src_mask=None,\n",
    "#                 max_len=50,\n",
    "#                 temperature=1.,\n",
    "#                 method = 'top_k',\n",
    "#                 top_k = 20,\n",
    "#                 top_p = 0.9,\n",
    "#                 early_stopping=False,\n",
    "#                 bos_idx=2, # TODO change to match future usecases\n",
    "#                 eos_idx=None):\n",
    "#         self.to(src.device) #TODO test for potential problems\n",
    "#         self.eval()\n",
    "#         thresh = top_k if method=='top_k' else top_p\n",
    "#         sampler = _sampler[method]\n",
    "#         src = expand_dim1(src)\n",
    "#         bs = src.size(0)\n",
    "#         inp = src.new_full((bs, 1), bos_idx) #start with bos tokens\n",
    "#         pdb.set_trace()\n",
    "#         src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "#         enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
    "#         out = inp\n",
    "#         for _ in range(max_len):\n",
    "#             x = out[:, -self.max_seq_len:]\n",
    "#             dec = self.decoder(self.dec_emb(out), context=enc)\n",
    "#             logits = self.proj(dec)[:, -1, :]\n",
    "#             if method == 'greedy':\n",
    "#                 sample = sampler(logits)\n",
    "#             else:\n",
    "#                 filtered_logits = sampler(logits)\n",
    "#                 probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "#                 sample = torch.multinomial(probs, 1)\n",
    "\n",
    "#             out = torch.cat((out, sample), dim=-1)\n",
    "\n",
    "#             if (early_stopping and \n",
    "#                 ((sample == eos_idx).all() or \n",
    "#                 (sample == self.pad_idx).all())):\n",
    "#                 break\n",
    "#         #TODO mb output cleanup\n",
    "#         return out\n",
    "    \n",
    "#     def store_attention(self, layer_ids=None, store_encoder=False, store_decoder=True):\n",
    "#         #defaults to storing attention for all layers\n",
    "#         layer_ids = default(layer_ids, list(range(self.n_layers)))\n",
    "#         for module in self.children():\n",
    "#             if issubclass(type(module), TransformerEncoder) and store_encoder:\n",
    "#                 for i, l in enumerate(module.layers):\n",
    "#                     if i in layer_ids:\n",
    "#                         for m in l.modules():\n",
    "#                             if issubclass(type(m), (Attention)):\n",
    "#                                 m.store_attention = True\n",
    "#             elif issubclass(type(module), TransformerDecoder) and store_decoder:\n",
    "#                 for i, l in enumerate(module.layers):\n",
    "#                     if i in layer_ids:\n",
    "#                         for m in l.modules():\n",
    "#                             if issubclass(type(m), (Attention)):\n",
    "#                                 m.store_attention = True\n",
    "#     #TODO mb separate encoder and decoder attention\n",
    "#     def get_attention_matrix(self, get_encoder=False, get_decoder=True):\n",
    "#         res = []\n",
    "#         if get_encoder:\n",
    "#             for m in self.encoder.modules():\n",
    "#                 if issubclass(type(m), (Attention)):\n",
    "#                     attention = getattr(m, 'attention', None)\n",
    "#                     if attention is not None:\n",
    "#                         res.append(attention)\n",
    "#                     # reset stored attention\n",
    "#                     m.attention = None\n",
    "#                     m.store_attention = False\n",
    "#         if get_decoder:\n",
    "#             for m in self.decoder.modules():\n",
    "#                 if issubclass(type(m), (Attention)):\n",
    "#                     attention = getattr(m, 'attention', None)\n",
    "#                     if attention is not None:\n",
    "#                         res.append(attention)\n",
    "#                     # reset stored attention\n",
    "#                     m.attention = None\n",
    "#                     m.store_attention = False\n",
    "#         return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MODELS = (LSHLM, ReversibleLM, ReversibleTransformer, ReformerLM)\n",
    "@patch(cls_method=True)\n",
    "def from_config(cls:MODELS, config):\n",
    "    return cls(**config.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 10c_experiment.synthetic-task-analysis.ipynb.\n",
      "Converted 11a_experiment.enwik8_baseline.ipynb.\n",
      "Converted 11b_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 11c_experiment.enwik8_reversible.ipynb.\n",
      "Converted 12_experiment.speed-lsh_synthetic-task.ipynb.\n",
      "Converted 13_experiment.enwik8-n_hashes.ipynb.\n",
      "Converted 14_experiment.enwik8-n_layers.ipynb.\n",
      "Converted 20_experiment-script-Copy1.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs-Copy1.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted enc_dec_nn_comparison_2.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
