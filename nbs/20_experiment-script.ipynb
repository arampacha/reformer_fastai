{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp expscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "\n",
    "from reformer_fastai.reformer import LSHLM\n",
    "from reformer_fastai.data import DeterministicTwinSequence, MaskTargCallback\n",
    "from reformer_fastai.metrics import MaskedAccuracy\n",
    "\n",
    "from reformer_fastai.tracking import *\n",
    "from reformer_fastai.tracking import WandbCallback\n",
    "from reformer_fastai.configs import SyntheticConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Script\n",
    "\n",
    "Experiments are run with this script by specifying:\n",
    "\n",
    "1) The task to run, i.e. synthetic task, language modelling or translation\n",
    "2) (Optionally) override default parameters for the dataloaders, models, training loop and logging\n",
    "\n",
    "To run the training script run `run_exp` from within the `reformer_fastai` repo. For example:\n",
    "\n",
    "```\n",
    "run_exp 'synth' lr=1e-4 bs=32 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Sequence length of 1024 is used in the paper and training for 150 k iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# Training\n",
    "lr=1e-3\n",
    "\n",
    "# Dataloaders\n",
    "bs=128\n",
    "sl=1024\n",
    "train_sz=900\n",
    "valid_sz=100\n",
    "n_epochs=1      # we want 150 k iterations according to the paper. Adjust n_epochs according to train_szs and bs\n",
    "\n",
    "# Model\n",
    "n_hashes=4\n",
    "bucket_size=64  # suggested in trax\n",
    "vocab_sz=128    # specific for the synthetic task\n",
    "d_model=256\n",
    "n_layers=1      # specified in paper\n",
    "n_heads=4\n",
    "d_ff=4*d_model\n",
    "\n",
    "attn_dropout=0.1\n",
    "ff_dropout=0.1\n",
    "emb_dropout=0.1\n",
    "\n",
    "max_seq_len=sl\n",
    "causal=True\n",
    "use_lsh=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example command to run full scale experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_exp 'synt' \\\n",
    "    --attn_dropout=0.0 \\\n",
    "    --ff_dropout=0.0 \\\n",
    "    --emb_dropout=0.0 \\\n",
    "    --d_ff=256 \\\n",
    "    --d_model=256 \\\n",
    "    --n_layers=1 \\\n",
    "    --bucket_size=64 \\\n",
    "    --bs=128 \\\n",
    "    --sl=1024 \\\n",
    "    --train_sz=19200000 \\\n",
    "    --valid_sz=10000 \\\n",
    "    --seed=123 \\\n",
    "    --wandb_group='Synthetic' \\\n",
    "    --wandb_tags='synthetic_task lsh lm test' \\\n",
    "    --wandb_name='synth_lsh_1_hash' \\\n",
    "    --use_lsh=True \\\n",
    "    --n_hashes=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twin Sequence Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_twin_sequence_dataloaders(bs:int=32, sl:int=1024, train_sz:int=500, valid_sz:int=100, seed=None):\n",
    "    \n",
    "    dls = DataLoaders.from_dsets(DeterministicTwinSequence(sl, train_sz, seed), \n",
    "                                 DeterministicTwinSequence(sl, valid_sz, seed), \n",
    "                                 bs=bs, shuffle=False, device='cuda')\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSHM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lshlm_model(vocab_sz:int=128, d_model:int=256, n_layers:int=1, n_heads:int=4, d_ff:int=None, \n",
    "              max_seq_len:int=64, bucket_size:int=32, n_hashes:int=4, causal:bool=True, use_lsh:bool=True,\n",
    "              attn_dropout:float=0.1, ff_dropout:float=0.1, emb_dropout:float=0.1, seed=None):\n",
    "    \n",
    "    model = LSHLM(vocab_sz=vocab_sz, d_model=d_model, n_layers=n_layers, n_heads=n_heads, d_ff=d_ff,\n",
    "                  max_seq_len=max_seq_len, bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, \n",
    "                  use_lsh=use_lsh, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                  emb_dropout=emb_dropout, random_state=seed)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_synthetic_learner(dls, model):\n",
    "    \n",
    "    learn = Learner(dls, model, \n",
    "                    loss_func=CrossEntropyLossFlat(ignore_index=-100), \n",
    "                    metrics=[MaskedAccuracy()]).to_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_wandb(cbs:list=[], wandb_name:str='', wandb_group:str='', wandb_notes:str='', wandb_tags:str='test'):\n",
    "    \n",
    "    wandb_tags_ls = wandb_tags.split(' ')\n",
    "    \n",
    "    try:\n",
    "        import wandb\n",
    "        #!wandb login\n",
    "    except ImportError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Init wandb\n",
    "    try:\n",
    "        wandb_run=wandb.init(reinit=True, project=\"reformer-fastai\", entity=\"fastai_community\", \n",
    "               name=wandb_name, group=wandb_group, notes=wandb_notes, tags=wandb_tags_ls, config={})\n",
    "        print('Weights & Biases initialised ...')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    cbs.append(WandbCallback(log_model=False, log_preds=False))\n",
    "    return wandb_run, cbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def run_exp(task:Param(help=\"Which exeriment task to run\", type=str),\n",
    "         n_epochs:Param(help=\"Number of epochs\", type=int, default=n_epochs),\n",
    "         lr:Param(help=\"Learning rate\", type=float, default=lr),               \n",
    "         bs:Param(help=\"Batch size\", type=int, default=bs),\n",
    "         sl:Param(help=\"Seqlence length\", type=int, default=sl),\n",
    "         d_model:Param(help=\"Model dimension\", type=int, default=d_model),\n",
    "         n_layers:Param(help=\"Number of model layers\", type=int, default=n_layers),\n",
    "         n_heads:Param(help=\"Number of attention heads\", type=int, default=n_heads),\n",
    "         vocab_sz:Param(help=\"Vocab size\", type=int, default=vocab_sz),\n",
    "         d_ff:Param(help=\"Vocab size\", type=int, default=d_ff),\n",
    "         attn_dropout:Param(help=\"Attention dropout rate\", type=float, default=attn_dropout),\n",
    "         ff_dropout:Param(help=\"Attention dropout rate\", type=float, default=ff_dropout),\n",
    "         emb_dropout:Param(help=\"Attention dropout rate\", type=float, default=emb_dropout),\n",
    "         train_sz:Param(help=\"TwinSequence train size\", type=int, default=train_sz),\n",
    "         valid_sz:Param(help=\"TwinSequence valid size\", type=int, default=valid_sz),\n",
    "         n_hashes:Param(help=\"Number of LSH Attention hashes\", type=int, default=n_hashes),\n",
    "         bucket_size:Param(help=\"LSH Attention bucket size\", type=int, default=bucket_size),\n",
    "         causal:Param(help=\"Use causal masking\", type=bool_arg, default=causal),\n",
    "         use_lsh:Param(help=\"Use LSH Attention\", type=bool_arg, default=use_lsh),\n",
    "         max_seq_len:Param(help=\"Max sequence length for model embedding\", type=int, default=max_seq_len),\n",
    "         do_wandb_logging:Param(help=\"Use weights and biases logging\", type=bool_arg, default=False),\n",
    "         wandb_name:Param(help=\"wandb run name\", type=str, default='my_experiment_name'),\n",
    "         wandb_group:Param(help=\"wandb group\", type=str, default='TEST'),\n",
    "         wandb_notes:Param(help=\"wandb notes\", type=str, default='My experiment notes'),\n",
    "#          wandb_config:Param(help=\"Use wandb logging\", type=bool_arg, default='my_experiment_name'),\n",
    "         wandb_tags:Param(help=\"wandb tags, add tags in a single string, space separated\", type=str, default='test'),\n",
    "         save_model:Param(help=\"Save model locally in /models\", type=bool_arg, default=False),\n",
    "         cuda_id:Param(help=\"Which cuda device to use\", type=int, default=0),\n",
    "         seed:Param(help=\"Set seed for reproducibiltiy, passing anything except 0 will use fastai's set_seed\", type=int, default=0)\n",
    "        ):\n",
    "\n",
    "    \"\"\"tasks: synt, lm, trans\"\"\"\n",
    "    \n",
    "    # Callbacks used for training\n",
    "    cbs = []               \n",
    "    \n",
    "    \n",
    "    #random seeds\n",
    "    random_state = seed if seed!=0 else None      # this is passed to LSH and data generator respectively\n",
    "    \n",
    "    if seed !=0 : \n",
    "        set_seed(seed, reproducible=True)          # this also sets `torch.backends.cudnn`\n",
    "\n",
    "        \n",
    "    if task == 'synt':\n",
    "        # Set which GPU to run the script on\n",
    "        torch.cuda.set_device(cuda_id)\n",
    "        \n",
    "        print('Getting dataloaders ...')\n",
    "        dls = get_twin_sequence_dataloaders(bs=bs, sl=sl, train_sz=train_sz, valid_sz=valid_sz)\n",
    "        print('done!')\n",
    "        \n",
    "        print('Getting model ...')\n",
    "        config = SyntheticConfig(**locals())\n",
    "        config.save(wandb_name, add_tstmp=True)\n",
    "        model = LSHLM.from_config(config)\n",
    "        print('done!')\n",
    "        \n",
    "        print('Getting learner ...')\n",
    "        learn = get_synthetic_learner(dls, model)\n",
    "        print('done!')\n",
    "        \n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging: \n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=wandb_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags)\n",
    "        \n",
    "        # Append training callbacks needed\n",
    "        cbs.append(MaskTargCallback())\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        learn.fit_one_cycle(n_epochs, lr, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed\n",
    "        if save_model:\n",
    "            import datetime\n",
    "            now = datetime.datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "            learn.save(f'{task}_n_hashes-{n_hashes}_use-lsh-{use_lsh}_epochs-{n_epochs}_{now}')\n",
    "        \n",
    "    elif task =='test':\n",
    "        print('testing testing :)')\n",
    "    else:\n",
    "        print('No task run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
