{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp expscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "import sys\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from reformer_fastai.all import *\n",
    "\n",
    "# from reformer_fastai.reformer import LSHLM\n",
    "# from reformer_fastai.data import DeterministicTwinSequence, MaskTargCallback\n",
    "# from reformer_fastai.metrics import MaskedAccuracy\n",
    "\n",
    "# from reformer_fastai.tracking import *\n",
    "# from reformer_fastai.tracking import WandbCallback\n",
    "# from reformer_fastai.configs import SyntheticConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Script\n",
    "\n",
    "Experiments are run with this script by specifying:\n",
    "\n",
    "1) The task to run, i.e. synthetic task, language modelling or translation\n",
    "2) (Optionally) override default parameters for the dataloaders, models, training loop and logging\n",
    "\n",
    "To run the training script run `run_exp` from within the `reformer_fastai` repo. For example:\n",
    "\n",
    "```\n",
    "run_exp 'synth' lr=1e-4 bs=32 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Sequence length of 1024 is used in the paper and training for 150 k iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# Training\n",
    "lr=1e-3\n",
    "\n",
    "# Dataloaders\n",
    "bs=128\n",
    "sl=1024\n",
    "train_sz=900\n",
    "valid_sz=100\n",
    "n_epochs=1      # we want 150 k iterations according to the paper. Adjust n_epochs according to train_szs and bs\n",
    "\n",
    "# Model\n",
    "n_hashes=4\n",
    "bucket_size=64  # suggested in trax\n",
    "vocab_sz=128    # specific for the synthetic task\n",
    "d_model=256\n",
    "n_layers=1      # specified in paper\n",
    "n_heads=4\n",
    "d_ff=4*d_model\n",
    "\n",
    "attn_dropout=0.1\n",
    "ff_dropout=0.1\n",
    "emb_dropout=0.1\n",
    "\n",
    "max_seq_len=sl\n",
    "causal=True\n",
    "use_lsh=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example command to run full scale experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_exp 'synt' \\\n",
    "    --attn_dropout=0.0 \\\n",
    "    --ff_dropout=0.0 \\\n",
    "    --emb_dropout=0.0 \\\n",
    "    --d_ff=256 \\\n",
    "    --d_model=256 \\\n",
    "    --n_layers=1 \\\n",
    "    --bucket_size=64 \\\n",
    "    --bs=128 \\\n",
    "    --sl=1024 \\\n",
    "    --train_sz=19200000 \\\n",
    "    --valid_sz=10000 \\\n",
    "    --seed=123 \\\n",
    "    --wandb_group='Synthetic' \\\n",
    "    --wandb_tags='synthetic_task lsh lm test' \\\n",
    "    --wandb_name='synth_lsh_1_hash' \\\n",
    "    --use_lsh=True \\\n",
    "    --n_hashes=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enwik8 Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_enwik8_data(dest='./data'):\n",
    "    return untar_data('http://mattmahoney.net/dc/enwik8.zip', dest='data')\n",
    "    \n",
    "def get_enwik8_dataloader(data_path='/data', bs:int=8, sl:int=1024, n_workers=None, val_test_chars:int=10e6, verbose=False):\n",
    "      \n",
    "    if 'google.colab' in sys.modules:\n",
    "        data_path = '/content' + data_path + '/enwik8'\n",
    "    else:\n",
    "        data_path = data_path + '/enwik8'\n",
    "    \n",
    "    if verbose: print('Reading data into dataframe ...')\n",
    "    df = pd.DataFrame({'text':read_lines(data_path)})\n",
    "    if verbose: print('done')\n",
    "    \n",
    "    # Do tokenization\n",
    "    btt = ByteTextTokenizer(is_lm=True, add_bos=False, add_eos=False)\n",
    "    if verbose: print('Tokenizing text ...')\n",
    "    df['toks'] = df['text'].apply(btt)\n",
    "    if verbose: print('done')\n",
    "    \n",
    "    # Get length of each sample and cumulative sum of lens\n",
    "    df['lens'] = df['toks'].apply(len)\n",
    "    df['lens_cum_sum'] = df.lens.cumsum()\n",
    "    \n",
    "    # Get splits, split train/valid/test based on count of tokens in each split\n",
    "    train_cutoff = df.lens.sum() - val_test_chars  # keep all but 10M characters for val and test\n",
    "    train_idxs = df.loc[df['lens_cum_sum'] < train_cutoff].index.values\n",
    "    train_idxs = list(range(0, max(train_idxs)))\n",
    "\n",
    "    remaining_idxs = len(df) - max(train_idxs)\n",
    "    validation_idxs = list(range(max(train_idxs), max(train_idxs) + int(remaining_idxs/2)))\n",
    "    test_idxs = list(range(max(validation_idxs), len(df)))\n",
    "\n",
    "    splits = [train_idxs, validation_idxs]\n",
    "     \n",
    "    # Get Datasets\n",
    "    if verbose: print('Setting up Datasets')\n",
    "    tfms = [attrgetter(\"text\"), btt]\n",
    "    dsets = Datasets(df, [tfms], splits=splits, dl_type=LMDataLoader)\n",
    "    if verbose: print('done')\n",
    "    \n",
    "    # Get Dataloaders\n",
    "    dl_kwargs = [{'lens':df['lens'].values[train_idxs]},\n",
    "                 {'val_lens':df['lens'].values[validation_idxs]}]\n",
    "    if verbose: print('Setting up Dataloaders ...')\n",
    "    \n",
    "    n_cpus = multiprocessing.cpu_count()\n",
    "    n_workers = n_cpus if n_workers is None else n_workers\n",
    "    \n",
    "    dls = dsets.dataloaders(bs=bs, seq_len=sl, dl_kwargs=dl_kwargs, shuffle_train=True, n_workers=n_workers)\n",
    "    print('done')\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns dls. Half of `val_test_chars` will be assigne to validation and half to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_dls = get_enwik8_dataloader(sl=65535, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = lm_dls.one_batch()\n",
    "# o[0].size(), o[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(lm_dls.train), len(lm_dls.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twin Sequence Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_twin_sequence_dataloaders(bs:int=32, sl:int=1024, train_sz:int=500, valid_sz:int=100, seed=None):\n",
    "    \n",
    "    dls = DataLoaders.from_dsets(DeterministicTwinSequence(sl, train_sz, seed), \n",
    "                                 DeterministicTwinSequence(sl, valid_sz, seed), \n",
    "                                 bs=bs, shuffle=False, device='cuda')\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSHM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lshlm_model(vocab_sz:int=128, d_model:int=256, n_layers:int=1, n_heads:int=4, d_ff:int=None, \n",
    "              max_seq_len:int=64, bucket_size:int=32, n_hashes:int=4, causal:bool=True, use_lsh:bool=True,\n",
    "              attn_dropout:float=0.1, ff_dropout:float=0.1, emb_dropout:float=0.1, seed=None):\n",
    "    \n",
    "    model = LSHLM(vocab_sz=vocab_sz, d_model=d_model, n_layers=n_layers, n_heads=n_heads, d_ff=d_ff,\n",
    "                  max_seq_len=max_seq_len, bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, \n",
    "                  use_lsh=use_lsh, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                  emb_dropout=emb_dropout, seed=seed)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner for Syhthetic task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_synthetic_learner(dls, model):\n",
    "    \n",
    "    learn = Learner(dls, model, \n",
    "                    loss_func=CrossEntropyLossFlat(ignore_index=-100), \n",
    "                    metrics=[MaskedAccuracy()]).to_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner for enwik8 language modelling task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lm_learner(dls, model, opt_func=adafactor):\n",
    "    learn = Learner(dls, model, \n",
    "                    loss_func=CrossEntropyLossFlat(ignore_index=-100), \n",
    "                    opt_func=opt_func, metrics=[accuracy, perplexity, BPC()]).to_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_wandb(cbs:list=[], wandb_name:str='', wandb_group:str='', wandb_notes:str='', wandb_tags:str='test'):\n",
    "    \n",
    "    wandb_tags_ls = wandb_tags.split(' ')\n",
    "    \n",
    "    try:\n",
    "        import wandb\n",
    "        #!wandb login\n",
    "    except ImportError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Init wandb\n",
    "    try:\n",
    "        wandb_run=wandb.init(reinit=True, project=\"reformer-fastai\", entity=\"fastai_community\", \n",
    "               name=wandb_name, group=wandb_group, notes=wandb_notes, tags=wandb_tags_ls, config={})\n",
    "        print('Weights & Biases initialised ...')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    cbs.append(WandbCallback(log_model=False, log_preds=False))\n",
    "    return wandb_run, cbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "seed = seed if seed!=0 else None \n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def run_exp(task:Param(help=\"Task options: 'synt','lm_base','lm_rev',lm_shared_qk, trans\", type=str),\n",
    "         data_path:Param(help=\"Path to data folder\", type=str, default='./data'),\n",
    "         n_epochs:Param(help=\"Number of epochs\", type=int, default=n_epochs),\n",
    "         lr:Param(help=\"Learning rate\", type=float, default=lr),               \n",
    "         bs:Param(help=\"Batch size\", type=int, default=bs),\n",
    "         sl:Param(help=\"Seqlence length\", type=int, default=sl),\n",
    "#          d_model:Param(help=\"Model dimension\", type=int, default=d_model),\n",
    "#          n_layers:Param(help=\"Number of model layers\", type=int, default=n_layers),\n",
    "#          n_heads:Param(help=\"Number of attention heads\", type=int, default=n_heads),\n",
    "#          vocab_sz:Param(help=\"Vocab size\", type=int, default=vocab_sz),\n",
    "#          d_ff:Param(help=\"Vocab size\", type=int, default=d_ff),\n",
    "#          attn_dropout:Param(help=\"Attention dropout rate\", type=float, default=attn_dropout),\n",
    "#          ff_dropout:Param(help=\"Attention dropout rate\", type=float, default=ff_dropout),\n",
    "#          emb_dropout:Param(help=\"Attention dropout rate\", type=float, default=emb_dropout),\n",
    "#          train_sz:Param(help=\"TwinSequence train size\", type=int, default=train_sz),\n",
    "#          valid_sz:Param(help=\"TwinSequence valid size\", type=int, default=valid_sz),\n",
    "#          n_hashes:Param(help=\"Number of LSH Attention hashes\", type=int, default=n_hashes),\n",
    "#          bucket_size:Param(help=\"LSH Attention bucket size\", type=int, default=bucket_size),\n",
    "#          causal:Param(help=\"Use causal masking\", type=bool_arg, default=causal),\n",
    "#          use_lsh:Param(help=\"Use LSH Attention\", type=bool_arg, default=use_lsh),\n",
    "#          max_seq_len:Param(help=\"Max sequence length for model embedding\", type=int, default=max_seq_len),\n",
    "         do_wandb_logging:Param(help=\"Use weights and biases logging\", type=bool_arg, default=False),\n",
    "         wandb_name:Param(help=\"wandb run name\", type=str, default='my_experiment_name'),\n",
    "         wandb_group:Param(help=\"wandb group\", type=str, default='TEST'),\n",
    "         wandb_notes:Param(help=\"wandb notes\", type=str, default='My experiment notes'),\n",
    "#          wandb_config:Param(help=\"Use wandb logging\", type=bool_arg, default='my_experiment_name'),\n",
    "         wandb_tags:Param(help=\"wandb tags, add tags in a single string, space separated\", type=str, default='test'),\n",
    "         save_model:Param(help=\"Save model locally in /models\", type=bool_arg, default=False),\n",
    "         cuda_id:Param(help=\"Which cuda device to use\", type=int, default=0),\n",
    "         seed:Param(help=\"Set seed for reproducibiltiy, passing anything except 0 will use fastai's set_seed\", type=int, default=0),\n",
    "#          verbose:Param(help=\"Print script logs\", type=bool_arg, default=False)\n",
    "        ):\n",
    "\n",
    "    \"\"\"Task options: 'synt','lm_base','lm_rev',lm_shared_qk, trans\"\"\"\n",
    "    \n",
    "    # Callbacks used for training\n",
    "    cbs = []               \n",
    "    \n",
    "    #random seeds\n",
    "    if seed!=0:\n",
    "        set_seed(seed, reproducible=True)  # this  sets `torch.cudnn.backends ++`\n",
    "    else: \n",
    "        seed = None   # this is passed to LSH and data generator. They expect None or int\n",
    "      \n",
    "    # Set which GPU to run the script on\n",
    "    torch.cuda.set_device(cuda_id)\n",
    "    \n",
    "    if task == 'synt':\n",
    "        print('Getting dataloaders ...')\n",
    "        dls = get_twin_sequence_dataloaders(bs=bs, sl=sl, train_sz=train_sz, valid_sz=valid_sz, seed=seed)\n",
    "        print('done!')\n",
    "        \n",
    "        print('Getting model ...')\n",
    "        config = SyntheticConfig(warn=False, verbose=verbose, **locals())\n",
    "        config.save(wandb_name, add_tstmp=True)\n",
    "        model = LSHLM.from_config(config)\n",
    "        print('done!')\n",
    "        \n",
    "        print('Getting learner ...')\n",
    "        learn = get_synthetic_learner(dls, model)\n",
    "        print('done!')\n",
    "        \n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging: \n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=wandb_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags)\n",
    "        \n",
    "        # Append training callbacks needed\n",
    "        cbs.append(MaskTargCallback())\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        learn.fit_one_cycle(n_epochs, lr, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed, saved in /models relative to where script is run\n",
    "        if save_model:\n",
    "            now = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            learn.save(f'{task}_{wandb_name}_{now}')\n",
    "    \n",
    "    elif 'lm' in task:\n",
    "        \n",
    "        if task == 'lm_base':\n",
    "            config = TransformerLMConfigEnwik8(warn=False, verbose=False, **locals())\n",
    "            print('Getting model ...')\n",
    "            model = TransformerLM.from_config(config)\n",
    "            print('done!')\n",
    "        elif task == 'lm_rev':\n",
    "            config = ReversibleLMConfigEnwik8(warn=False, verbose=False, **locals())\n",
    "            print('Getting model ...')\n",
    "            model = ReversibleLM.from_config(config)\n",
    "            print('done!')\n",
    "        elif task == 'lm_shared_qk':\n",
    "            config = TransformerLMConfigEnwik8(warn=False, verbose=False, **locals())\n",
    "            config['shared_qk'] = True\n",
    "            print('Getting model ...')\n",
    "            model = TransformerLM.from_config(config)\n",
    "            print('done!')\n",
    "        \n",
    "        config.save(wandb_name, add_tstmp=True)\n",
    "\n",
    "        print('Checking data')\n",
    "        download_enwik8_data(dest='./data')\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting dataloaders ...')\n",
    "        dls = get_enwik8_dataloader(data_path=data_path, bs=bs, sl=sl,verbose=True)\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting learner ...')\n",
    "        learn = get_lm_learner(dls, model, opt_func=adafactor)\n",
    "        print('done!')\n",
    "        \n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging: \n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=wandb_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags)\n",
    "        \n",
    "        # Append training callbacks needed\n",
    "#         cbs.append()\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        learn.fit_one_cycle(n_epochs, lr, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed, saved in /models relative to where script is run\n",
    "        if save_model:\n",
    "            now = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            learn.save(f'{task}_{wandb_name}_{now}')\n",
    "        \n",
    "    elif task == 'test_cfg':\n",
    "        print('Locals ', locals())\n",
    "        print()\n",
    "        config = SyntheticConfig(verbouse=True, **locals())\n",
    "        print(config)\n",
    "        config.save('test')\n",
    "        config2 = SyntheticConfig.from_file('test')\n",
    "        print(config2)\n",
    "        \n",
    "    elif task == 'test':\n",
    "        print('testing testing :)')\n",
    "        \n",
    "    else:\n",
    "        print('No task run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 10c_experiment.synthetic-task-analysis.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
