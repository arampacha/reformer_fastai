{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp expscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *\n",
    "from fastai.distributed import *\n",
    "\n",
    "from reformer_fastai.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Script\n",
    "\n",
    "## Run the script\n",
    "Experiments are run with this script by specifying:\n",
    "\n",
    "1) The task to run, i.e. synthetic task, language modelling or translation\n",
    "2) (Optionally) override default parameters for the dataloaders, models, training loop and logging\n",
    "\n",
    "To run the training script run `run_exp` from within the `reformer_fastai` repo. For example:\n",
    "\n",
    "```\n",
    "run_exp 'synth' lr=1e-4 bs=32 \n",
    "```\n",
    "\n",
    "To run experiment script on multiple GPUs use `fastai.launch`:\n",
    "\n",
    "```\n",
    "python -m fastai.launch [--gpus 1,2] expscript.py [args]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configs\n",
    "\n",
    "Model Configs can be found the `21_experiment-config.ipynb` notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enwik8 Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_enwik8_data(data_path='./data'):\n",
    "    dest = Path(data_path)\n",
    "    if not dest.exists(): dest.mkdir()\n",
    "    return untar_data('http://mattmahoney.net/dc/enwik8.zip', dest=dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WMT-14 Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_wmt14_data(data_path='./data'):\n",
    "    dest = Path(data_path)\n",
    "    if not dest.exists(): dest.mkdir()\n",
    "    \n",
    "    if not os.path.isfile(f'{data_path}/wmt14_train') \\\n",
    "        or not os.path.isfile(f'{data_path}/wmt14_valid') \\\n",
    "        or not os.path.isfile(f'{data_path}/swe_wmt_vocab'): \n",
    "        \n",
    "        print('Downloading data')\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "        except ImportError as e:\n",
    "            print(e)\n",
    "        dataset = load_dataset('wmt_t2t', 'de-en')\n",
    "        \n",
    "        train_df = pd.DataFrame(dataset['train']['translation'])\n",
    "        train_df['is_valid'] = False\n",
    "        valid_df = pd.DataFrame(dataset['validation']['translation'])\n",
    "        valid_df['is_valid'] = True\n",
    "        test_df = pd.DataFrame(dataset['test']['translation'])\n",
    "        test_df['is_test'] = True\n",
    "\n",
    "        train_df.to_feather(f'{data_path}/wmt14_train')\n",
    "        valid_df.to_feather(f'{data_path}/wmt14_valid')\n",
    "        test_df.to_feather(f'{data_path}/wmt14_test')\n",
    "        \n",
    "        url=\"https://raw.githubusercontent.com/tensorflow/tensor2tensor/master/tensor2tensor/test_data/vocab.translate_ende_wmt32k.32768.subwords\"\n",
    "        download_url(url, f'{data_path}/swe_wmt_vocab') \n",
    "    else: print('Using saved data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twin Sequence Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_twin_sequence_dataloaders(bs:int=32, sl:int=1024, train_sz:int=500, valid_sz:int=100, seed=None):\n",
    "    dls = DataLoaders.from_dsets(DeterministicTwinSequence(sl, train_sz, seed), \n",
    "                                 DeterministicTwinSequence(sl, valid_sz, seed), \n",
    "                                 bs=bs, shuffle=False, device='cuda')\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enwik8 Dataloader\n",
    "\n",
    "`val_test_chars` sets the the number of tokens in the combined validation and test set. Valdiation and test sets will have `val_test_chars / 2` tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "def get_enwik8_dataloader(data_path='data', bs:int=8, val_bs:int=16, sl:int=1024, n_workers=None, \n",
    "                          val_test_chars:int=10e6, verbose=False, tiny=False, small=False):\n",
    "      \n",
    "    if 'google.colab' in sys.modules:\n",
    "        data_path = '/content' + data_path + '/enwik8'\n",
    "    else:\n",
    "        data_path = data_path + '/enwik8'\n",
    "    \n",
    "    if verbose: print('Reading data into dataframe ...')\n",
    "    df = pd.DataFrame({'text':read_lines(data_path)})\n",
    "    if tiny: \n",
    "        df = df.sample(frac=0.05)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        val_test_chars = 10000\n",
    "    elif small:\n",
    "        df = df[:len(df)//4].copy()\n",
    "        val_test_chars = 4e6\n",
    "        \n",
    "    if verbose: print('done')\n",
    "    \n",
    "    # Do tokenization\n",
    "    btt = ByteTextTokenizer(is_lm=True, add_bos=False, add_eos=False)\n",
    "    if verbose: print('Tokenizing text ...')\n",
    "    df['toks'] = df['text'].apply(btt)\n",
    "    if verbose: print('done')\n",
    "    \n",
    "    # Get length of each sample and cumulative sum of lens\n",
    "    df['lens'] = df['toks'].apply(len)\n",
    "    df['lens_cum_sum'] = df.lens.cumsum()\n",
    "    \n",
    "    # Get splits, split train/valid/test based on count of tokens in each split\n",
    "    train_cutoff = df.lens.sum() - val_test_chars  # keep all but 10M characters for val and test\n",
    "    train_idxs = df.loc[df['lens_cum_sum'] < train_cutoff].index.values\n",
    "    train_idxs = list(range(0, max(train_idxs)))\n",
    "\n",
    "    remaining_idxs = len(df) - max(train_idxs)\n",
    "    validation_idxs = list(range(max(train_idxs), max(train_idxs) + int(remaining_idxs/2)))\n",
    "    test_idxs = list(range(max(validation_idxs), len(df)))\n",
    "\n",
    "    splits = [train_idxs, validation_idxs]\n",
    "     \n",
    "    # Get Datasets\n",
    "    if verbose: print('Setting up Datasets ...')\n",
    "    tfms = [attrgetter(\"text\"), btt]\n",
    "    dsets = Datasets(df, [tfms], splits=splits, dl_type=LMDataLoader)\n",
    "    if verbose: print('done')\n",
    "    \n",
    "    # Get Dataloaders\n",
    "    dl_kwargs = [{'lens':df['lens'].values[train_idxs]},\n",
    "                 {'val_lens':df['lens'].values[validation_idxs]}]\n",
    "    if verbose: print('Setting up Dataloaders ...')\n",
    "    \n",
    "    n_cpus = multiprocessing.cpu_count()\n",
    "    n_workers = n_cpus if n_workers is None else n_workers\n",
    "    \n",
    "    dls = dsets.dataloaders(bs=bs, val_bs=val_bs, seq_len=sl, dl_kwargs=dl_kwargs, shuffle_train=True, n_workers=n_workers)\n",
    "    print('done')\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WMT-14 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_wmt14_dataloader(data_path='data', bs:int=8, val_bs:int=8, sl:int=1024, n_workers=None, \n",
    "                         verbose=False, tiny=False):\n",
    "\n",
    "    if verbose: print('Reading data into dataframe ...')\n",
    "    train_df = pd.read_feather(f'{data_path}/wmt14_train')\n",
    "    valid_df = pd.read_feather(f'{data_path}/wmt14_valid')\n",
    "    #     test_df = pd.read_feather(f'{data_path}/wmt14_test')\n",
    "\n",
    "    if tiny: \n",
    "        train_df = train_df.sample(frac=0.02)\n",
    "        train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Merge Train and Validation datasets\n",
    "    df = pd.concat([train_df, valid_df])\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    if verbose: print('done')\n",
    "\n",
    "    # TOKENIZER + DATASETS\n",
    "    if verbose: print('Setting up Datasets ...')\n",
    "    tok = SubwordTextEncoder(filename=f'{data_path}/swe_wmt_vocab', add_bos=True, seq_len=sl)\n",
    "\n",
    "    train_split = df.loc[df.is_valid == False].index.values\n",
    "    valid_split = df.loc[df.is_valid == True].index.values\n",
    "    splits = train_split, valid_split\n",
    "\n",
    "    # Get text lengths to enable faster init with SortedDL\n",
    "    df['de_lens'] = df['de'].str.len()\n",
    "\n",
    "    # Transforms\n",
    "    add_eos = AddEOSID(tok.EOS_ID)\n",
    "    en_tfms = [ColReader(\"en\"), tok, add_eos]\n",
    "    de_tfms = [ColReader(\"de\"), tok, add_eos]\n",
    "\n",
    "    # Set up datsets\n",
    "    dsets = Datasets(df, [en_tfms, de_tfms], splits=splits)\n",
    "    if verbose: print('done')\n",
    "\n",
    "    # DATALOADERS\n",
    "    if verbose: print('Setting up Dataloaders ...')   \n",
    "    srtd_dl = partial(SortedDL, shuffle=True, res=df['de_lens'].values[splits[0]])\n",
    "    dl_kwargs = [{},{'val_res': df['de_lens'].values[splits[1]]}]\n",
    "\n",
    "    # Define padding\n",
    "    pad_seq2seq = partial(pad_input, pad_idx=tok.PAD_ID, pad_fields=[0,1])\n",
    "    \n",
    "    # Workers\n",
    "    n_cpus = multiprocessing.cpu_count()\n",
    "    n_workers = n_cpus if n_workers is None else n_workers\n",
    "    \n",
    "    dls = dsets.dataloaders(bs=bs, before_batch=pad_seq2seq, dl_type=srtd_dl, dl_kwargs=dl_kwargs, \n",
    "                            shuffle_train=True, n_workers=n_workers)\n",
    "    if verbose: print('done')   \n",
    "    return dls, tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sythetic Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_synthetic_learner(dls, model, precision=0):\n",
    "    learn = Learner(dls, model, \n",
    "                    loss_func=CrossEntropyLossFlat(ignore_index=-100), \n",
    "                    metrics=[MaskedAccuracy()])\n",
    "    if precision==0: learn.to_fp16()\n",
    "    elif precision==1: learn.to_to_non_native_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enwik8 Language Modelling Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lm_learner(dls, model, opt_func=adafactor, precision=0):\n",
    "    learn = Learner(dls, model, \n",
    "                    loss_func=CrossEntropyLossFlat(ignore_index=dls.byte_text_tokenizer.pad_token_id), \n",
    "                    opt_func=opt_func, metrics=[accuracy, perplexity, bpc])\n",
    "    if precision==0: learn.to_fp16()\n",
    "    elif precision==1: learn.to_to_non_native_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReformerLM Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_reformerlm_learner(dls, model, opt_func=adafactor, precision=2):\n",
    "    learn = Learner(dls, model,\n",
    "                    loss_func=CrossEntropyLossFlat(ignore_index=dls.byte_text_tokenizer.pad_token_id), \n",
    "                    opt_func=opt_func, metrics=[accuracy, perplexity, bpc])\n",
    "    if precision==0: learn.to_fp16()\n",
    "    elif precision==1: learn.to_to_non_native_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WMT Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_seq2seq_learner(dls, model, tok, precision=0):\n",
    "    learn = Learner(dls, model, \n",
    "                    loss_func=CrossEntropyLossFlat(ignore_index=tok.PAD_ID), # opt_func=adafactor,\n",
    "                    metrics=[accuracy, Perplexity(), CorpusBLEUMetric()])\n",
    "    if precision==0: learn.to_fp16()\n",
    "    elif precision==1: learn.to_to_non_native_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_wandb(cbs:list=[], wandb_name:str='', wandb_group:str='', wandb_notes:str='', wandb_tags:str='test', save_model=False):\n",
    "    wandb_tags_ls = wandb_tags.split(' ')\n",
    "    \n",
    "    try:\n",
    "        import wandb\n",
    "        #!wandb login\n",
    "    except ImportError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Init wandb\n",
    "    try:\n",
    "        wandb_run=wandb.init(reinit=True, project=\"reformer-fastai\", entity=\"fastai_community\", \n",
    "               name=wandb_name, group=wandb_group, notes=wandb_notes, tags=wandb_tags_ls, config={})\n",
    "        print('Weights & Biases initialised ...')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    cbs.append(WandbCallback(log_model=save_model, log_preds=False))\n",
    "    return wandb_run, cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "### Command line arguments\n",
    "Only arguments used to alternate between different experiment runs  will be passed to the model from the command line, e.g. for the Synthetic experiment, only `n_hashes` and `use_lsh` can be changed from the command line. All other model parameters are fixed from `SyntheticConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def run_exp(task:Param(help=\"Task options: 'synt','lm_base','lm_rev',lm_shared_qk, n_hashes, trans\", type=str),\n",
    "         data_path:Param(help=\"Path to data folder\", type=str, default='./data'),\n",
    "         n_epochs:Param(help=\"Number of epochs\", type=int, default=1),\n",
    "         lr:Param(help=\"Learning rate\", type=float, default=1e-3),               \n",
    "         bs:Param(help=\"Batch size\", type=int, default=64),\n",
    "         train_sz:Param(help=\"TwinSequence train size\", type=int, default=12800),\n",
    "         valid_sz:Param(help=\"TwinSequence valid size\", type=int, default=1280),\n",
    "         n_layers:Param(help=\"Number of layers\", type=int, default=3),\n",
    "         n_hashes:Param(help=\"Number of LSH Attention hashes\", type=int, default=1),\n",
    "         use_lsh:Param(help=\"Use LSH Attention\", type=bool_arg, default=False),\n",
    "         max_seq_len:Param(help=\"Max sequence length for model embedding and dataloader\", type=int, default=2048),\n",
    "         do_wandb_logging:Param(help=\"Use weights and biases logging\", type=bool_arg, default=False),\n",
    "         run_name:Param(help=\"Run name for wandb tracking and model filename\", type=str, default=''),\n",
    "         wandb_group:Param(help=\"wandb group\", type=str, default='TEST'),\n",
    "         wandb_notes:Param(help=\"wandb notes\", type=str, default='My experiment notes'),\n",
    "         wandb_tags:Param(help=\"wandb tags, add tags in a single string, space separated\", type=str, default='test'),\n",
    "         save_model:Param(help=\"Save model locally in /models\", type=bool_arg, default=False),\n",
    "         grad_accum:Param(help=\"Gradient Accumulation, set greater than 1 to implement\", type=int, default=1),\n",
    "         clip:Param(help=\"Gradient Clipping, will be set if > 0.0\", type=float, default=0.0),\n",
    "         cuda_id:Param(help=\"Which cuda device to use\", type=int, default=0),\n",
    "         seed:Param(help=\"Set seed for reproducibiltiy, passing anything except 0 will use fastai's set_seed\", type=int, default=0),\n",
    "         distrib:Param(help=\"Set to True if using distributed training\", type=bool_arg, default=False),\n",
    "         verbose:Param(help=\"Print script logs\", type=bool_arg, default=True),\n",
    "         tiny:Param(help=\"Use 5% of data, for quick iteration and testings\", type=bool_arg, default=False),\n",
    "         precision:Param(help=\"0:fp16, 1:non native fp16, 2:fp32\", type=int, default=0)\n",
    "        ):\n",
    "\n",
    "    \"\"\"Task options: 'synt','lm_base','lm_rev',lm_shared_qk, trans\"\"\"\n",
    "    #Set up distributed training\n",
    "#     _wrapper = rank0_first if distrib else partial\n",
    "#     if distrib: cuda_id = None \n",
    "    torch.cuda.set_device(cuda_id)\n",
    "    \n",
    "    # Callbacks used for training\n",
    "    cbs = []\n",
    "    if save_model: cbs.append(SaveModelCallback(every_epoch=True))\n",
    "    \n",
    "    #random seeds\n",
    "    if seed!=0:\n",
    "        set_seed(seed, reproducible=True)  # this  sets `torch.cudnn.backends ++`\n",
    "    else: \n",
    "        seed = None   # this is passed to LSH and data generator. They expect None or int\n",
    "    \n",
    "    if task == 'synt':\n",
    "        \"Model + Data Args than can be changed from command line: train_sz, valid_sz, n_hashes, use_lsh, seed\"\n",
    "        \n",
    "        \n",
    "        if run_name == '': \n",
    "            if use_lsh: run_name = f'{task}_lsh-{n_hashes}_bs-{bs}_n_eps-{n_epochs}_seed-{seed}'\n",
    "            else: run_name = f'{task}_full-attn_bs-{bs}_n_eps-{n_epochs}'\n",
    "        \n",
    "        print('Getting model ...')\n",
    "        config = SyntheticConfig(warn=False, verbose=verbose, n_hashes=n_hashes, use_lsh=use_lsh)\n",
    "        if verbose: print(config)\n",
    "        config.save(run_name, add_tstmp=True)\n",
    "        model = LSHLM.from_config(config)\n",
    "        print('done!')\n",
    "        \n",
    "        print('Getting dataloaders ...')\n",
    "        if train_sz != 12800: print(f'Note, \"train_sz\" changed from recommended 12800 to {train_sz}')\n",
    "        dls = get_twin_sequence_dataloaders(bs=bs, sl=config['max_seq_len'], train_sz=train_sz, \n",
    "                                            valid_sz=valid_sz, seed=seed)\n",
    "        print('done!')\n",
    "        \n",
    "        print('Getting learner ...')\n",
    "        learn = get_synthetic_learner(dls, model, precision)\n",
    "        print('done!')\n",
    "        \n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging and rank_distrib()==0:\n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=run_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags, save_model=save_model)\n",
    "        \n",
    "        # Append training callbacks needed\n",
    "        cbs.append(MaskTargCallback())\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        with learn.distrib_ctx(cuda_id=cuda_id): learn.fit_one_cycle(n_epochs, lr, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed, saved in /models relative to where script is run\n",
    "        if save_model:\n",
    "            now = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            learn.save(f'{task}_{run_name}_{now}')\n",
    "    \n",
    "    elif 'lm' in task:\n",
    "        \"Model args that can be changed from command line: axial_shape, max_seq_len\"\n",
    "        axial_shape = get_axial_shape(max_seq_len)\n",
    "        if task == 'lm_base':\n",
    "            if run_name == '': run_name = f'{task}_enwik8_sl-{max_seq_len}_bs-{bs}_n_eps-{n_epochs}_seed-{seed}'\n",
    "            config = TransformerLMConfigEnwik8(warn=False, verbose=verbose, \n",
    "                                               axial_shape=axial_shape, max_seq_len=max_seq_len)\n",
    "            print('Getting model ...')\n",
    "            model = TransformerLM.from_config(config)\n",
    "            print('done!')\n",
    "        elif task == 'lm_rev':\n",
    "            if run_name == '': run_name = f'{task}_enwik8_sl-{max_seq_len}_bs-{bs}_n_eps-{n_epochs}_seed-{seed}'\n",
    "            config = ReversibleLMConfigEnwik8(warn=False, verbose=verbose, \n",
    "                                              axial_shape=axial_shape, max_seq_len=max_seq_len)\n",
    "            print('Getting model ...')\n",
    "            model = ReversibleLM.from_config(config)\n",
    "            print('done!')\n",
    "        elif task == 'lm_shared_qk':\n",
    "            if run_name == '': run_name = f'{task}_enwik8_sl-{max_seq_len}_bs-{bs}_n_eps-{n_epochs}_seed-{seed}'\n",
    "            config = TransformerLMConfigEnwik8(warn=False, verbose=verbose, shared_qk=True, \n",
    "                                               axial_shape=axial_shape, max_seq_len=max_seq_len)\n",
    "            print('Getting model ...')\n",
    "            model = TransformerLM.from_config(config)\n",
    "            print('done!')\n",
    "        \n",
    "        if verbose: print(config)\n",
    "        config.save(run_name, add_tstmp=True)\n",
    "\n",
    "        print('Checking data')\n",
    "#         _wrapper(download_enwik8_data, data_path=data_path)\n",
    "#         if distrib: rank0_first(download_enwik8_data, data_path=data_path)\n",
    "        download_enwik8_data(data_path=data_path)\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting dataloaders ...')\n",
    "        dls = get_enwik8_dataloader(data_path=data_path, bs=bs, val_bs=bs, sl=max_seq_len, \n",
    "                                    verbose=verbose, tiny=tiny)\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting learner ...')\n",
    "        learn = get_lm_learner(dls, model, opt_func=adafactor, precision=precision)\n",
    "        print('done!')\n",
    "        \n",
    "        # CALLBACKS\n",
    "        ## Gradient Clipping\n",
    "        if clip != 0.0: cbs.append(GradientClip(max_norm=clip))\n",
    "        \n",
    "        ## Gradient Accumulation\n",
    "        if grad_accum > 1:\n",
    "            print(f'Gradient accumulation on, virtual batch size == {grad_accum}')\n",
    "            cbs.append(GradientAccumulation(n_acc=grad_accum))\n",
    "            run_name = run_name + f'_grad-accum-{grad_accum}'\n",
    "        \n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging and rank_distrib()==0:\n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=run_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags)\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        learn.fit(n_epochs, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed, saved in /models relative to where script is run\n",
    "        if save_model:\n",
    "            now = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            learn.save(f'{task}_{run_name}_{now}')\n",
    "    \n",
    "    elif task == 'n_hashes':\n",
    "        \"Model args that can be changed from command line: n_hashes, seed\"\n",
    "        \n",
    "        if run_name == '': run_name = f'{task}-{n_hashes}_enwik8_sl-{max_seq_len}_bs-{bs}_n_eps-{n_epochs}_seed-{seed}'\n",
    "\n",
    "        print('Checking data')\n",
    "#         _wrapper(download_enwik8_data, data_path=data_path)\n",
    "#         if distrib: rank0_first(download_enwik8_data, data_path=data_path)\n",
    "        download_enwik8_data(data_path=data_path)\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting dataloaders ...')\n",
    "        dls = get_enwik8_dataloader(data_path=data_path, bs=bs, val_bs=bs, sl=max_seq_len, \n",
    "                                    verbose=verbose, tiny=tiny)\n",
    "        print('done')\n",
    "        pad_id = dls.byte_text_tokenizer.pad_token_id\n",
    "        \n",
    "        config = NHashesConfig(warn=False, verbose=verbose, n_hashes=n_hashes,\n",
    "                               seed=seed, pad_idx=pad_id)\n",
    "        print('Getting model ...')\n",
    "        model = LSHLM.from_config(config)\n",
    "        print('done!')\n",
    "        \n",
    "        if verbose: print(config)\n",
    "        config.save(run_name, add_tstmp=True)\n",
    "        \n",
    "        print('Getting learner ...')\n",
    "        learn = get_lm_learner(dls, model, opt_func=adafactor, precision=precision)\n",
    "        print('done!')\n",
    "        \n",
    "        # CALLBACKS\n",
    "        ## Gradient Clipping\n",
    "        if clip != 0.0: cbs.append(GradientClip(max_norm=clip))\n",
    "        \n",
    "        ## Gradient Accumulation\n",
    "        if grad_accum > 1: \n",
    "            print(f'Gradient accumulation on, virtual batch size == {grad_accum}')\n",
    "            cbs.append(GradientAccumulation(n_acc=grad_accum))\n",
    "            run_name = run_name + f'_grad-accum-{grad_accum}'\n",
    "        #LSH-specific callback\n",
    "        if config.use_lsh: cbs.append(PadBatchCallback(bucket_size=config.bucket_size,\n",
    "                                                       val=pad_id, y_val=pad_id))\n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging and rank_distrib()==0:\n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=run_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags)\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        learn.fit(n_epochs, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed, saved in /models relative to where script is run\n",
    "        if save_model:\n",
    "            now = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            learn.save(f'{task}_{run_name}_{now}')\n",
    "    \n",
    "    elif task == 'n_layers':\n",
    "        \"Model args that can be changed from command line: n_hashes, seed\"\n",
    "        \n",
    "        if run_name == '': run_name = f'{task}-{n_layers}_enwik8_sl-{max_seq_len}_bs-{bs}_n_eps-{n_epochs}_seed-{seed}'\n",
    "\n",
    "        print('Checking data')\n",
    "#         _wrapper(download_enwik8_data, data_path=data_path)\n",
    "#         if distrib: rank0_first(download_enwik8_data, data_path=data_path)\n",
    "        download_enwik8_data(data_path=data_path)\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting dataloaders ...')\n",
    "        dls = get_enwik8_dataloader(data_path=data_path, bs=bs, val_bs=bs, sl=max_seq_len, \n",
    "                                    verbose=verbose, tiny=tiny)\n",
    "        print('done')\n",
    "        pad_id = dls.byte_text_tokenizer.pad_token_id\n",
    "        \n",
    "        config = NLayersConfig(warn=False, verbose=verbose, n_layers=n_layers,\n",
    "                               max_seq_len=max_seq_len, seed=seed, pad_idx=pad_id)\n",
    "        print('Getting model ...')\n",
    "        model = ReformerLM.from_config(config)\n",
    "        print('done!')\n",
    "        \n",
    "        if verbose: print(config)\n",
    "        config.save(run_name, add_tstmp=True)\n",
    "        \n",
    "        print('Getting learner ...')\n",
    "        learn = get_reformerlm_learner(dls, model, opt_func=adafactor, precision=precision)\n",
    "        print('done!')\n",
    "        \n",
    "        # CALLBACKS\n",
    "        ## Gradient Clipping\n",
    "        if clip != 0.0: cbs.append(GradientClip(max_norm=clip))\n",
    "        \n",
    "        ## Gradient Accumulation\n",
    "        if grad_accum > 1: \n",
    "            print(f'Gradient accumulation on, virtual batch size == {grad_accum}')\n",
    "            cbs.append(GradientAccumulation(n_acc=grad_accum))\n",
    "            run_name = run_name + f'_grad-accum-{grad_accum}'\n",
    "        #LSH-specific callback\n",
    "        if config.use_lsh: cbs.append(PadBatchCallback(bucket_size=config.bucket_size,\n",
    "                                                       val=pad_id, y_val=pad_id))\n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging and rank_distrib()==0:\n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=run_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags)\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        learn.fit(n_epochs, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed, saved in /models relative to where script is run\n",
    "        if save_model:\n",
    "            now = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            learn.save(f'{task}_{run_name}_{now}')\n",
    "    \n",
    "    \n",
    "    elif task == 'wmt_rev':\n",
    "        \"Model args that can be changed from command line: n_layers, max_seq_len\"\n",
    "        axial_shape = get_axial_shape(max_seq_len)\n",
    "        if run_name == '': run_name = f'{task}_sl-{max_seq_len}_bs-{bs}_n_eps-{n_epochs}_seed-{seed}'\n",
    "            \n",
    "        print('Checking data')\n",
    "        download_wmt14_data(data_path=data_path)\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting dataloaders and tokenizer ...')\n",
    "        dls, tok = get_wmt14_dataloader(data_path=data_path, bs=bs, val_bs=bs, sl=max_seq_len, \n",
    "                                           verbose=verbose, tiny=tiny)\n",
    "        print('done')\n",
    "        \n",
    "        print('Getting model ...')\n",
    "        config = ReversibleTransformerConfigWMT(warn=False, verbose=verbose, \n",
    "                                                enc_vocab_sz=tok.vocab_size, dec_vocab_sz=tok.vocab_size, pad_idx=tok.PAD_ID,\n",
    "                                                n_enc_layers=n_layers, n_dec_layers=n_layers)\n",
    "\n",
    "        model = ReversibleTransformer.from_config(config)\n",
    "        print('done!')\n",
    "        \n",
    "        if verbose: print(config)\n",
    "        config.save(run_name, add_tstmp=True)\n",
    "\n",
    "        print('Getting learner ...')\n",
    "        # Use AdaFactor?       \n",
    "        learn = get_seq2seq_learner(dls, model, tok, precision)\n",
    "        print('done!')\n",
    "        \n",
    "        # CALLBACKS\n",
    "        cbs += [CombineInputOutputCallback(), LossTargetShiftCallback(), RemoveEOSCallback(eos_idx=tok.EOS_ID)]\n",
    "        \n",
    "        ## Gradient Clipping Callback\n",
    "        if clip != 0.0: cbs.append(GradientClip(max_norm=clip))\n",
    "        \n",
    "        ## Gradient Accumulation Callback\n",
    "        if grad_accum > 1:\n",
    "            print(f'Gradient accumulation on, virtual batch size == {grad_accum}')\n",
    "            cbs.append(GradientAccumulation(n_acc=grad_accum))\n",
    "            run_name = run_name + f'_grad-accum-{grad_accum}'\n",
    "        \n",
    "        # Set up Weights & Biases logging, if needed\n",
    "        if do_wandb_logging and rank_distrib()==0:\n",
    "            wandb_run, cbs = init_wandb(cbs, wandb_name=run_name, wandb_group=wandb_group,\n",
    "                                        wandb_notes=wandb_notes, wandb_tags=wandb_tags)\n",
    "        \n",
    "        # Start training\n",
    "        print('Starting training...')\n",
    "        learn.fit_one_cycle(n_epochs, lr, cbs=cbs)\n",
    "        print('done!')\n",
    "        \n",
    "        # Close wandb logging for this run\n",
    "        if do_wandb_logging: wandb_run.finish()\n",
    "        \n",
    "        # Save model weights if needed, saved in /models relative to where script is run\n",
    "        if save_model:\n",
    "            now = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            learn.save(f'{task}_{run_name}_{now}')\n",
    "    \n",
    "    \n",
    "    elif task == 'test_cfg':\n",
    "        print('Locals ', locals())\n",
    "        print()\n",
    "        config = SyntheticConfig(verbouse=True, **locals())\n",
    "        print(config)\n",
    "        config.save('test')\n",
    "        config2 = SyntheticConfig.from_file('test')\n",
    "        print(config2)\n",
    "        \n",
    "    elif task == 'test':\n",
    "        print('testing testing :)')\n",
    "        print(verbose)\n",
    "        \n",
    "    else:\n",
    "        print('No task run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Script\n",
    "\n",
    "Example command to run full scale experiment, note than `run_name` can be passed, but if not passed it will be automatically constructed based on the task and relevant arguments\n",
    "\n",
    "#### Running the Synthetic Experiment:\n",
    "\n",
    "```\n",
    "run_exp 'synt' \\\n",
    "    --n_epochs=750 \\\n",
    "    --lr=1e-4 \\\n",
    "    --bs=128 \\\n",
    "    --use_lsh=True \\\n",
    "    --n_hashes=1 \\\n",
    "    --train_sz=12800 \\\n",
    "    --valid_sz=1280 \\\n",
    "    --seed=1234 \\\n",
    "    --wandb_group='Synthetic' \\\n",
    "    --wandb_tags='synthetic_task lsh lm test' \\\n",
    "    --run_name='synth_lsh_1_hash' \n",
    "```\n",
    "\n",
    "#### Running the Reversible Language Model experiment:\n",
    "\n",
    "> For the full 60k steps with a sequence length of 65536, the number of epochs can be calculated as follows:\n",
    "with sl == 2\\*\\*16, 1 epoch of enwik8 will have 172 batches, therefore; n_epoch == 60000/172 == 349\n",
    "\n",
    "```\n",
    "run_exp 'lm_rev' \\\n",
    "    --n_epochs=3 \\\n",
    "    --lr=1e-4 \\\n",
    "    --bs=8 \\\n",
    "    --max_seq_len=4096 \\\n",
    "    --do_wandb_logging=True \\\n",
    "    --wandb_group='enwik8_lm_rev' \\\n",
    "    --wandb_tags='lm_rev lm exp' \\\n",
    "    --wandb_notes='This is a test'\n",
    "    --grad_accum=4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 10c_experiment.synthetic-task-analysis.ipynb.\n",
      "Converted 11a_experiment.enwik8_baseline.ipynb.\n",
      "Converted 11b_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 11c_experiment.enwik8_reversible.ipynb.\n",
      "Converted 12_experiment.speed-lsh_synthetic-task.ipynb.\n",
      "Converted 13_experiment.enwik8-n_hashes.ipynb.\n",
      "Converted 14_experiment.enwik8-n_layers.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
