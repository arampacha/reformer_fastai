{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of reformer_fastai.reformer failed: Traceback (most recent call last):\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/morgan/anaconda3/envs/fastai_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "RecursionError: maximum recursion depth exceeded\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# default_exp expscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "\n",
    "from reformer_fastai.reformer import LSHLM\n",
    "from reformer_fastai.data import TwinSequence, MaskTargCallback\n",
    "from reformer_fastai.metrics import MaskedAccuracy\n",
    "\n",
    "from reformer_fastai.tracking import *\n",
    "from reformer_fastai.tracking import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "Sequence length of 1024 is used in the paper and training for 150 k iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# Dataloaders\n",
    "bs=128\n",
    "sl=1024\n",
    "train_sz=900\n",
    "valid_sz=100\n",
    "n_epochs=1      # we want 150 k iterations according to the paper. Adjust n_epochs according to train_szs and bs\n",
    "\n",
    "# Model\n",
    "n_hashes=4\n",
    "bucket_size=64  # suggested in trax\n",
    "vocab_sz=128    # specific for the synthetic task\n",
    "d_model=256\n",
    "n_layers=1      # specified in paper\n",
    "n_heads=4\n",
    "max_seq_len=sl\n",
    "causal=True\n",
    "use_lsh=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dataloaders(bs:int=32, sl:int=64, train_sz:int=500, valid_sz:int=100):\n",
    "    dls = DataLoaders.from_dsets(TwinSequence(sl, train_sz), TwinSequence(sl, valid_sz), bs=bs, shuffle=False, device='cuda')\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model(vocab_sz:int=128, d_model:int=256, n_layers:int=1, n_heads:int=4, \n",
    "              max_seq_len:int=64, bucket_size:int=32, n_hashes:int=4, causal:bool=True, use_lsh:bool=True):\n",
    "    model = LSHLM(vocab_sz=vocab_sz, d_model=d_model, n_layers=n_layers, n_heads=n_heads, max_seq_len=max_seq_len,\n",
    "              bucket_size=bucket_size, n_hashes=n_hashes, causal=True, use_lsh=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_learner(dls, model):\n",
    "    learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(ignore_index=-100), \n",
    "                metrics=MaskedAccuracy(), cbs=[MaskTargCallback()]).to_fp16()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def run_exp(task:Param(help=\"Which exeriment task to run\", type=str),\n",
    "         lr:Param(help=\"Learning rate\", type=float, default=1e-3),               \n",
    "         bs:Param(help=\"Batch size\", type=int, default=bs),\n",
    "         sl:Param(help=\"Seqlence length\", type=int, default=sl),\n",
    "         n_epochs:Param(help=\"Number of epochs\", type=int, default=n_epochs),\n",
    "         train_sz:Param(help=\"TwinSequence train size\", type=int, default=train_sz),\n",
    "         valid_sz:Param(help=\"TwinSequence valid size\", type=int, default=valid_sz),\n",
    "         n_hashes:Param(help=\"Number of LSH Attention hashes\", type=int, default=n_hashes),\n",
    "         bucket_size:Param(help=\"LSH Attention bucket size\", type=int, default=bucket_size),\n",
    "         vocab_sz:Param(help=\"Vocab size\", type=int, default=vocab_sz),\n",
    "         d_model:Param(help=\"Model dimension\", type=int, default=d_model),\n",
    "         n_layers:Param(help=\"Number of model layers\", type=int, default=n_layers),\n",
    "         n_heads:Param(help=\"Number of attention heads\", type=int, default=n_heads),\n",
    "         max_seq_len:Param(help=\"Max sequence length for model embedding\", type=int, default=max_seq_len),\n",
    "         causal:Param(help=\"Use causal masking\", type=int, default=causal),\n",
    "         use_lsh:Param(help=\"Use LSH Attention\", type=bool, default=use_lsh),\n",
    "         save_model:Param(help=\"Save model locally in /models\", type=bool, default=True),\n",
    "         do_wandb_logging:Param(help=\"Use wandb logging\", type=bool, default=False),\n",
    "         wandb_name:Param(help=\"wandb run name\", type=str, default='my_experiment_name'),\n",
    "         wandb_group:Param(help=\"wandb group\", type=str, default='TEST'),\n",
    "         wandb_notes:Param(help=\"wandb notes\", type=str, default='My experiment notes'),\n",
    "#          wandb_config:Param(help=\"Use wandb logging\", type=bool, default='my_experiment_name'),\n",
    "         wandb_tags:Param(help=\"wandb tags\", type=list, default=['test']),\n",
    "         cuda_id:Param(help=\"Which cuda device to use\", type=int, default=0)\n",
    "                     ):\n",
    "\n",
    "    \"\"\"tasks: synthetic, lm, translation\"\"\"\n",
    "                      \n",
    "    if task == 'synt':\n",
    "        \n",
    "        torch.cuda.set_device(cuda_id)\n",
    "\n",
    "        dls = get_dataloaders(bs=bs, sl=sl, train_sz=train_sz, valid_sz=valid_sz)\n",
    "\n",
    "        model = get_model(vocab_sz=vocab_sz, d_model=d_model, n_layers=n_layers, n_heads=n_heads, \n",
    "                  max_seq_len=max_seq_len, bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, use_lsh=use_lsh)\n",
    "\n",
    "        learn = get_learner(dls, model)\n",
    "        \n",
    "        cbs=[MaskTargCallback()]\n",
    "        \n",
    "        if do_wandb_logging:\n",
    "            try:\n",
    "                import wandb\n",
    "                #!wandb login\n",
    "            except ImportError as e:\n",
    "                print(e)\n",
    "                \n",
    "            # Init wandb\n",
    "            wandb.init(reinit=True, project=\"reformer-fastai\", entity=\"fastai_community\", \n",
    "                   name=wandb_name, group=wandb_group, notes=wandb_notes, tags=wandb_tags, config={})\n",
    "            \n",
    "            cbs.append(WandbCallback(log_model=False, log_preds=False))\n",
    "        print('starting train')\n",
    "        learn.fit_one_cycle(n_epochs, lr, cbs=cbs)\n",
    "        \n",
    "        if save_model:\n",
    "            import datetime\n",
    "            now = datetime.datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "            learn.save(f'{task}_n_hashes-{n_hashes}_use-lsh-{use_lsh}_epohs-{n_epochs}_{now}')\n",
    "        \n",
    "    else:\n",
    "        print('No task run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline-Copy1.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
