{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Contains basic layers used both in baseline and reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from functools import partial, reduce, wraps\n",
    "from operator import mul\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "try:\n",
    "    from axial_positional_embedding import AxialPositionalEmbedding, AxialPositionalEmbeddingImage\n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "\n",
    "from reformer_fastai.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://github.com/lucidrains/all-normalization-transformer/blob/master/all_normalization_transformer/all_normalization_transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Residual(Module):\n",
    "    \"\"\"Add skip-connection: out = x + sublayer(x)\"\"\"\n",
    "    def __init__(self, sublayer:Module): store_attr()\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x + self.sublayer(x, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PostNorm(Module):\n",
    "    \"\"\"Adds LayerNorm after sublayer\"\"\"\n",
    "    def __init__(self, d_model:int, sublayer:Module):\n",
    "        store_attr('sublayer')\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.sublayer(x, *args, **kwargs)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class PreNorm(Module):\n",
    "    \"\"\"Adds LayerNorm before sublayer\"\"\"\n",
    "    def __init__(self, d_model:int, sublayer:Module):\n",
    "        store_attr('sublayer')\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.sublayer(x, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeedForward(Module):\n",
    "    \"\"\"\n",
    "    Simple positional feed-forward module with GELU activation function.\n",
    "    If d_ff is None defaults to 4*d_model\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, d_ff:int=None, dropout:float=0.):\n",
    "        d_ff = default(d_ff, 4 * d_model)\n",
    "        layers = [nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n",
    "                    nn.Linear(d_ff, d_model), nn.Dropout(dropout)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(p) for p in self.parameters() if p.dim() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "ff  = Residual(PreNorm(d, FeedForward(d)))\n",
    "out = ff(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# axial position helpers (subjected to review)\n",
    "def get_axial_dims(d_emb, n):\n",
    "    res = (d_emb//n, )*(n-1)\n",
    "    res += (d_emb-sum(res), )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AbsolutePositionalEmbedding(Module):\n",
    "    \"\"\"Learnable absolute positional encodings\"\"\"\n",
    "    def __init__(self, d_emb:int, max_seq_len:int):\n",
    "        self.emb = nn.Embedding(max_seq_len, d_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device)\n",
    "        return self.emb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FixedPositionalEmbedding(Module):\n",
    "    \"\"\"Fixed positional encodings\"\"\"\n",
    "    def __init__(self, d_emb:int):\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, d_emb, 2).float() / d_emb))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEmbedding(Module):\n",
    "    \"\"\"\n",
    "    Combines token embedings with positional encodings\n",
    "    pos_enc: str from {'absolute', 'fixed', 'axial'}\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 emb_sz:int, \n",
    "                 d_emb:int, \n",
    "                 max_seq_len:int=512, \n",
    "                 dropout:float=0., \n",
    "                 pos_enc:str='absolute', \n",
    "                 axial_shape:Tuple=None, \n",
    "                 axial_emb_dims:Tuple=None):\n",
    "        store_attr('d_emb')\n",
    "        self.scale = d_emb ** 0.5\n",
    "        #previous default: std = 0.02; fairseq: d_emb ** -0.5; fastai: 0.01\n",
    "        self.emb = Embedding(emb_sz, d_emb, std=d_emb**-0.5)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if pos_enc == 'absolute': self.pos_enc = AbsolutePositionalEmbedding(d_emb, max_seq_len)\n",
    "        elif pos_enc == 'fixed': self.pos_enc = FixedPositionalEmbedding(d_emb)\n",
    "        elif pos_enc == 'axial':\n",
    "            assert axial_shape is not None, 'provide `axial_shape` argument'\n",
    "            assert reduce(mul, axial_shape) == max_seq_len\n",
    "            axial_emb_dims = default(axial_emb_dims, get_axial_dims(d_emb, len(axial_shape)))\n",
    "            self.pos_enc = AxialPositionalEmbedding(d_emb, axial_shape, axial_emb_dims)\n",
    "        #self._init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)  #* self.scale\n",
    "        x *= self.scale\n",
    "        x += self.pos_enc(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    #def _init(self):\n",
    "        #nn.init.trunc_normal_(self.emb.weight, std = self.std)\n",
    "        #if hasattr(self.pos_enc, 'emb'): nn.init.trunc_normal_(self.pos_enc.emb.weight, std=self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters\n",
      "Fixed:    16384\n",
      "Absolute: 49152\n",
      "Axial:    17920\n"
     ]
    }
   ],
   "source": [
    "fix_emb = TransformerEmbedding(256, 64, pos_enc='fixed')\n",
    "abs_emb = TransformerEmbedding(256, 64, pos_enc='absolute')\n",
    "axl_emb = TransformerEmbedding(256, 64, pos_enc='axial', axial_shape=(32,16))\n",
    "print('Total number of parameters')\n",
    "print(f'Fixed:    {total_params(fix_emb)[0]}')\n",
    "print(f'Absolute: {total_params(abs_emb)[0]}')\n",
    "print(f'Axial:    {total_params(axl_emb)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_logging.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
