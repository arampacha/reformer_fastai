{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# from reformer_fastai.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Summary\n",
    "\n",
    "> Summary page from our Reformer Reproducibility Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of Reproducibility\n",
    "\n",
    "*State the main claim(s) of the original paper you are trying to reproduce (typically the main claim(s) of the paper). This is meant to place the work in context, and to tell a reader the objective of the reproduction.*\n",
    "\n",
    "\n",
    "The Reformer paper introduced a model, \"Reformer\", based on LSH Attention and Reversible Residual Layers, that is claimed to perform on par with Transformer models while:\n",
    "- a) being much more memory-efficient\n",
    "- b) much faster on long sequences\n",
    "\n",
    "The scope of this reproducibility effort is to verify these claims of memory efficiency and speed on longer sequences. This verification was carried out via experiments for only NLP datasets used is the paper; a synthetic dataset, enwik8 and WMT. The computer vision experiments were not verified due to a lack of available compute for this community effort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "*Briefly describe what you did and which resources you used.  For example, did you use author’s code?  Did youre-implement parts of the pipeline? You can also use this space to list the hardware used, and the total budget (e.g. GPUhours) for the experiments.*\n",
    "\n",
    "We chose to verify the authors' claims by focusing on the NLP-related experiments outlined in the paper. Our experiments replicated the LSH attention, Reversible Residual Layers, Shared Query-Key Attention NLP experiments with a custom syntheric dataset from the paper as well as the WMT [REF] and enwik8 [REF] datasets.\n",
    "\n",
    "For code development, our approach was to implement the original Transformer model [Vaswani REF](https://arxiv.org/pdf/1706.03762.pdf) from scratch, which them provided a solid foundation from which we could modify individual model components to test the techniques described in the paper. We referenced code from the authors Trax repository [Trax repository REF](https://github.com/google/trax/tree/master/trax/models/reformer), pytorch implementations of the Reformer paper [@lucidrains REF](https://github.com/lucidrains/reformer-pytorch), [@huggingface Transformer REF](https://github.com/huggingface/transformers) as well as other Transformer archtecture implementations [FairSeq REF](https://github.com/pytorch/fairseq/), [@lucidtrains local-attention REF](https://github.com/lucidrains/local-attention) and resources [Illustraed Transformer REF](https://jalammar.github.io/illustrated-transformer/), [Annotated Transformer REF](https://nlp.seas.harvard.edu/2018/04/03/attention.html).  During our experiment design we also referenced the authors' code to replicate the data loading and tokenization methods used and ported them into the fastai framework.\n",
    "\n",
    "The fastai [fastai REF](https://www.mdpi.com/2078-2489/11/2/108/htm) framework was used for dataloading and model traing, experiment tracking was carried out with Weights and Biases [wandb REF](see bibtext reference below) and nbdev literate programming environment [nbdev REF](https://nbdev.fast.ai/) was used for all development work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Weights and Biases BibTeX Reference)**\n",
    "    \n",
    "> @misc{wandb,\n",
    "title = {Experiment Tracking with Weights and Biases},\n",
    "year = {2020},\n",
    "note = {Software available from wandb.com},\n",
    "url={https://www.wandb.com/},\n",
    "author = {Biewald, Lukas},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "*Start with your overall conclusion — where did your results reproduce the original paper, and where did your resultsdiffer? Be specific and use precise language, e.g. \"we reproduced the accuracy to within 1% of reported value, whichsupports the paper’s conclusion that it outperforms the baselines\". Getting exactly the same number is in most casesinfeasible, so you’ll need to use your judgement to decide if your results support the original claim of the paper.*\n",
    "\n",
    "OUR RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What was easy\n",
    "\n",
    "*Describe which parts of your reproduction study were easy. For example, was it easy to run the author’s code, or easy to re-implement their method based on the description in the paper? The goal of this section is to summarize to a reader which parts of the original paper they could easily apply to their problem.*\n",
    "\n",
    "EASY - NOT MUCH :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What was difficult\n",
    "\n",
    "*Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhapsthe data was not available and you couldn’t verify some experiments, or the author’s code was broken and had to bedebugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn’t verify them.The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to re-use, orrequire a significant amount of work and resources to verify*\n",
    "\n",
    "While there were Reformer code resources available, when it came to implementation details this paper was quite challenging due to many design decisions not being fully documented in the paper. Examples include X,Y, Z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication with original authors\n",
    "\n",
    "We emailed the authors a number of times to verify the metric used for language modelling (BPC), the optimizer used for the Machine Translation experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted links.ipynb.\n",
      "Converted report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
