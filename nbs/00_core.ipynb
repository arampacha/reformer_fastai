{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Basic healper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *\n",
    "from fastai.test_utils import *\n",
    "\n",
    "from functools import partial, reduce, wraps\n",
    "from inspect import isfunction\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General purpose utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def expand_dim1(x):\n",
    "    if len(x.shape) == 1:\n",
    "        return x[None, :]\n",
    "    else: return x\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setattr_on(model, attr, val, module_class):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, module_class):\n",
    "            setattr(m, attr, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# generative helpers\n",
    "# credit https://github.com/huggingface/transformers/blob/a0c62d249303a68f5336e3f9a96ecf9241d7abbe/src/transformers/generation_logits_process.py\n",
    "def top_p_filter(logits, top_p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cum_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    # if min_tokens_to_keep > 1:\n",
    "    #         # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "    #         sorted_indices_to_remove[..., : min_tokens_to_keep - 1] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "def top_k_filter(logits, top_k=20):\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "_sampler = {\n",
    "    'top_k':top_k_filter,\n",
    "    'top_p':top_p_filter,\n",
    "    'gready':lambda x: x.argmax(-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH specific helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [lucidrains/reformer-pytorch](https://github.com/lucidrains/reformer-pytorch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n",
    "            namespace_str = str(default(key_namespace, ''))\n",
    "            _cache = getattr(self, cache_attr)\n",
    "            _keyname = f'{cache_namespace}:{namespace_str}'\n",
    "\n",
    "            if fetch:\n",
    "                val = _cache[_keyname]\n",
    "                if reexecute:\n",
    "                    fn(self, *args, **kwargs)\n",
    "            else:\n",
    "                val = fn(self, *args, **kwargs)\n",
    "                if set_cache:\n",
    "                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n",
    "            return val\n",
    "        return wrapper\n",
    "    return inner_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def look_one_back(x):\n",
    "    x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n",
    "    return torch.cat([x, x_extra], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def chunked_sum(tensor, chunks=1):\n",
    "    *orig_size, last_dim = tensor.shape\n",
    "    tensor = tensor.reshape(-1, last_dim)\n",
    "    summed_tensors = [c.sum(dim=-1) for c in tensor.chunk(chunks, dim=0)]\n",
    "    return torch.cat(summed_tensors, dim=0).reshape(orig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def sort_key_val(t1, t2, dim=-1):\n",
    "    values, indices = t1.sort(dim=dim)\n",
    "    t2 = t2.expand_as(t1)\n",
    "    return values, t2.gather(dim, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def batched_index_select(values, indices):\n",
    "    last_dim = values.shape[-1]\n",
    "    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to assess model performance. Test functions with `mod` and input `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = get_text_classifier(AWD_LSTM, vocab_sz=10_000, n_class=10)\n",
    "x = torch.randint(0, 100, (3, 72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def do_cuda_timing(f, inp, context=None, n_loops=100):\n",
    "    '''\n",
    "        Get timings of cuda modules. Note `self_cpu_time_total` is returned, but\n",
    "        from experiments this appears to be similar/same to the total CUDA time\n",
    "        \n",
    "        f :  function to profile, typically an nn.Module\n",
    "        inp : required input to f\n",
    "        context : optional additional input into f, used for Decoder-style modules\n",
    "    '''\n",
    "    f.cuda()\n",
    "    inp = inp.cuda()\n",
    "    if context is not None: context = context.cuda()\n",
    "    with profiler.profile(record_shapes=False, use_cuda=True) as prof:\n",
    "        with profiler.record_function(\"model_inference\"):\n",
    "            with torch.no_grad():\n",
    "                for _ in range(n_loops):\n",
    "                    if context is None: f(inp)\n",
    "                    else: f(inp, context)\n",
    "                    torch.cuda.synchronize()\n",
    "                    \n",
    "    res = round((prof.key_averages().self_cpu_time_total / 1000) / n_loops, 3)\n",
    "    print(f'{res}ms')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def model_performance(n_loops=5, model='arto', dls=None, n_epochs=1, lr=5e-4):\n",
    "    \"\"\"\n",
    "        DEMO CODE ONLY!\n",
    "        Run training loop to measure timings. Note that the models internally\n",
    "        should be changed depending on the model you would like to use. \n",
    "        You should also adjust the metrics you are monitoring\n",
    "    \"\"\"\n",
    "    acc_ls, ppl_ls =[], []\n",
    "    for i in range(n_loops):\n",
    "        # ADD YOUR MODEL(S) INIT HERE\n",
    "#         if model == 'arto': m = artoTransformerLM(vocab_sz, 512)\n",
    "#         elif model == 'pt': m = ptTransformerLM(vocab_sz, 512)\n",
    "#         else: print('model name not correct')\n",
    "        \n",
    "        learn = Learner(dls, m,\n",
    "                    loss_func=CrossEntropyLossFlat(),\n",
    "                    metrics=[accuracy, Perplexity()]).to_native_fp16()\n",
    "\n",
    "        learn.fit_one_cycle(n_epochs, lr, wd=0.05)\n",
    "        \n",
    "        acc_ls.append(learn.recorder.final_record[2])\n",
    "        ppl_ls.append(learn.recorder.final_record[3])\n",
    "    print(f'Avg Accuracy: {round(sum(acc_ls)/len(acc_ls),3)}, std: {np.std(acc_ls)}')\n",
    "    print(f'Avg Perplexity: {round(sum(ppl_ls)/len(ppl_ls),3)}, std: {np.std(ppl_ls)}')\n",
    "    print()\n",
    "    return learn, acc_ls, ppl_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def total_params(m):\n",
    "    \"\"\"\n",
    "    Give the number of parameters of a module and if it's trainable or not\n",
    "    - Taken from Taken from fastai.callback.hook\n",
    "    \"\"\"\n",
    "    params = sum([p.numel() for p in m.parameters()])\n",
    "    trains = [p.requires_grad for p in m.parameters()]\n",
    "    return params, (False if len(trains)==0 else trains[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of params for our test model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24336280, True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Callbacks\n",
    "\n",
    "Callbacks used to ensuring training a translation model works. All 3 are needed\n",
    "\n",
    "See [notebook here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb) for explanation of EOS shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class CombineInputOutputCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback to combine the source (self.xb) and target (self.yb) into self.xb\n",
    "    \"\"\"\n",
    "    def __init__(self): pass\n",
    "    def before_batch(self): \n",
    "        self.learn.xb = (self.xb[0], self.yb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AssertAndCancelFit(Callback):\n",
    "    \"Cancels batch after backward to avoid opt.step()\"\n",
    "    def before_batch(self):\n",
    "        assert len(self.learn.xb) == 2\n",
    "        assert self.learn.xb[1] is self.learn.yb[0]\n",
    "        raise CancelEpochException()\n",
    "\n",
    "learn = synth_learner(cbs=[CombineInputOutputCallback(), AssertAndCancelFit()])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class RemoveEOSCallback(Callback):\n",
    "    \"\"\"\n",
    "        Shift the target presented to the model during training to remove the \"eos\" token as \n",
    "        we don't want the model to learn to translate EOS when it sees EOS.\n",
    "        \n",
    "        In practice we actually mask the EOS token as due to batching the last token will often be a <pad> token,\n",
    "        not EOS\n",
    "    \"\"\"\n",
    "    def __init__(self, eos_idx): self.eos_idx=eos_idx\n",
    "    def before_batch(self):        \n",
    "        eos_mask=(self.learn.xb[1]!=self.eos_idx)\n",
    "        sz=torch.tensor(self.learn.xb[1].size())\n",
    "        sz[1]=sz[1]-1\n",
    "        self.learn.xb = (self.learn.xb[0], self.learn.xb[1][eos_mask].view((sz[0],sz[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class LossTargetShiftCallback(Callback):\n",
    "    \"\"\"\n",
    "        Shift the target shown to the loss to exclude the \"bos\" token as the first token we want predicted\n",
    "        should be an actual word, not the \"bos\" token (as we have already given the model \"bos\" )\n",
    "    \"\"\"\n",
    "    def __init__(self): pass\n",
    "    def after_pred(self): \n",
    "        self.learn.yb = (self.learn.yb[0][:,1:],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TestLossShiftAndCancelFit(Callback):\n",
    "    \"Cancels batch after backward to avoid opt.step()\"\n",
    "    def after_pred(self):      \n",
    "        o = self.learn.dls.one_batch()\n",
    "        assert self.learn.yb[0].size()[1] == o[1].size()[1] - 1\n",
    "        raise CancelEpochException()\n",
    "\n",
    "learn = synth_learner(cbs=[LossTargetShiftCallback(), TestLossShiftAndCancelFit()])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PadBatchCallback(Callback):\n",
    "    \"Pads input and target sequences to multiple of 2*bucket_size\"\n",
    "    def __init__(self, bucket_size:int=64, val:int=0, y_val:int=-100):\n",
    "        self.mult = 2*bucket_size\n",
    "        self.val, self.y_val = val, y_val\n",
    "    def before_batch(self):\n",
    "        bs, sl = self.x.size()\n",
    "        if sl % self.mult != 0:\n",
    "            pad_ = self.mult - sl%self.mult\n",
    "            self.learn.xb = (F.pad(self.x, (0,pad_), 'constant', self.val), )\n",
    "            self.learn.yb = (F.pad(self.y, (0,pad_), 'constant', self.y_val), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LabelSmoothingCrossEntropy(Module):\n",
    "    \"\"\"Label smotthing cross entropy similar to fastai implementation\n",
    "    https://github.com/fastai/fastai/blob/89b1afb59e37e5abf7008888f6e4dd1bf1211e3e/fastai/losses.py#L79\n",
    "    with added option to provide ignore_index\"\"\"\n",
    "    y_int = True\n",
    "    def __init__(self, eps:float=0.1, reduction='mean', ignore_index=-100): store_attr()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        nll_loss = F.nll_loss(log_preds, target.long(), reduction=self.reduction, ignore_index=self.ignore_index)\n",
    "        mask = target.eq(self.ignore_index)\n",
    "        log_preds = log_preds.masked_fill(mask.unsqueeze(-1), 0.)\n",
    "        if self.reduction=='sum': smooth_loss = -log_preds.sum()\n",
    "        else:\n",
    "            smooth_loss = -log_preds.sum(dim=-1) #We divide by that size at the return line so sum and not mean\n",
    "            if self.reduction=='mean':  smooth_loss = smooth_loss.mean()/(1-mask.float().mean())# devide by fraction of accounted values to debias mean\n",
    "        return smooth_loss*self.eps/c + (1-self.eps)*nll_loss\n",
    "\n",
    "    def activation(self, out): return F.softmax(out, dim=-1)\n",
    "    def decodes(self, out):    return out.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates()\n",
    "class LabelSmoothingCrossEntropyFlat(BaseLoss):\n",
    "    \"Same as `LabelSmoothingCrossEntropy`, but flattens input and target.\"\n",
    "    y_int = True\n",
    "    @use_kwargs_dict(keep=True, eps=0.1, reduction='mean')\n",
    "    def __init__(self, *args, axis=-1, **kwargs): super().__init__(LabelSmoothingCrossEntropy, *args, axis=axis, **kwargs)\n",
    "    def activation(self, out): return F.softmax(out, dim=-1)\n",
    "    def decodes(self, out):    return out.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=4\n",
    "sl=10\n",
    "v=32\n",
    "pred = torch.randn(bs, sl, v, requires_grad=True)\n",
    "targ = torch.randint(v, (bs,sl))\n",
    "i, j = torch.triu_indices(bs, sl, offset=(sl-bs+1))\n",
    "targ[i,j] = -1\n",
    "loss_func = LabelSmoothingCrossEntropyFlat(ignore_index=-1)\n",
    "loss = loss_func(pred, targ)\n",
    "loss.backward()\n",
    "assert (torch.all(pred.grad == 0, dim=-1) == (targ==-1)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.distributed import *\n",
    "@patch\n",
    "@contextmanager\n",
    "def distrib_ctx(self: Learner, cuda_id=None,sync_bn=True):\n",
    "    \"A context manager to adapt a learner to train in distributed data parallel mode.\"\n",
    "    # Figure out the GPU to use from rank.  Create a dpg if none exists yet.\n",
    "    if cuda_id is None: cuda_id = int(os.environ.get('DEFAULT_GPU', 0))\n",
    "    if not torch.distributed.is_initialized():\n",
    "        setup_distrib(cuda_id)\n",
    "        cleanup_dpg = torch.distributed.is_initialized()\n",
    "    else: cleanup_dpg = False\n",
    "    # Adapt self to DistributedDataParallel, yield, and cleanup afterwards.\n",
    "    try:\n",
    "        if num_distrib(): self.to_distributed(cuda_id,sync_bn)\n",
    "        yield self\n",
    "    finally:\n",
    "        self.detach_distributed()\n",
    "        if cleanup_dpg: teardown_distrib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 10c_experiment.synthetic-task-analysis.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
