{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Basic healpers and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from fastai.basics import *\n",
    "\n",
    "from functools import partial, reduce\n",
    "from inspect import isfunction\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "try:\n",
    "    from axial_positional_embedding import AxialPositionalEmbedding, AxialPositionalEmbeddingImage\n",
    "except ImportError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General purpose utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def expand_dim1(x):\n",
    "    if len(x.shape) == 1:\n",
    "        return x[None, :]\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# generative helpers\n",
    "# credit https://github.com/huggingface/transformers/blob/a0c62d249303a68f5336e3f9a96ecf9241d7abbe/src/transformers/generation_logits_process.py\n",
    "def top_p_filter(logits, top_p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cum_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    # if min_tokens_to_keep > 1:\n",
    "    #         # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "    #         sorted_indices_to_remove[..., : min_tokens_to_keep - 1] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "def top_k_filter(logits, top_k=20):\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "_sampler = {\n",
    "    'top_k':top_k_filter,\n",
    "    'top_p':top_p_filter,\n",
    "    'gready':lambda x: x.argmax(-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://github.com/lucidrains/all-normalization-transformer/blob/master/all_normalization_transformer/all_normalization_transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Residual(Module):\n",
    "    \"\"\"Add skip-connection: out = x + sublayer(x)\"\"\"\n",
    "    def __init__(self, sublayer:Module): store_attr()\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x + self.sublayer(x, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PostNorm(Module):\n",
    "    \"\"\"Adds LayerNorm after sublayer\"\"\"\n",
    "    def __init__(self, d_model:int, sublayer:Module):\n",
    "        store_attr('sublayer')\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.sublayer(x, *args, **kwargs)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class PreNorm(Module):\n",
    "    \"\"\"Adds LayerNorm before sublayer\"\"\"\n",
    "    def __init__(self, d_model:int, sublayer:Module):\n",
    "        store_attr('sublayer')\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.sublayer(x, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeedForward(Module):\n",
    "    \"\"\"\n",
    "    Simple positional feed-forward module with GELU activation function.\n",
    "    If d_ff is None defaults to 4*d_model\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, d_ff:int=None, dropout:float=0.):\n",
    "        d_ff = default(d_ff, 4 * d_model)\n",
    "        layers = [nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n",
    "                    nn.Linear(d_ff, d_model), nn.Dropout(dropout)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(p) for p in self.parameters() if p.dim() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "ff  = Residual(PreNorm(d, FeedForward(d)))\n",
    "out = ff(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AbsolutePositionalEmbedding(Module):\n",
    "    \"\"\"Learnable absolute positional encodings\"\"\"\n",
    "    def __init__(self, d_emb:int, max_seq_len:int):\n",
    "        self.emb = nn.Embedding(max_seq_len, d_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device)\n",
    "        return self.emb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FixedPositionalEmbedding(Module):\n",
    "    \"\"\"Fixed positional encodings\"\"\"\n",
    "    def __init__(self, d_emb:int):\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, d_emb, 2).float() / d_emb))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEmbedding(Module):\n",
    "    \"\"\"\n",
    "    Combines token embedings with positional encodings\n",
    "    pos_enc: str from {'absolute', 'fixed', 'axial'}\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 emb_sz:int, \n",
    "                 d_emb:int, \n",
    "                 max_seq_len:int=512, \n",
    "                 dropout:float=0., \n",
    "                 pos_enc:str='absolute', \n",
    "                 axial_shape:Tuple=None, \n",
    "                 axial_emb_dims:Tuple=None):\n",
    "        store_attr('d_emb')\n",
    "        self.scale = d_emb ** 0.5\n",
    "        self.std = 0.02    # fairseq: d_emb ** -0.5, fastai: 0.01\n",
    "        self.emb = nn.Embedding(emb_sz, d_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if pos_enc == 'absolute': self.pos_enc = AbsolutePositionalEmbedding(d_emb, max_seq_len)\n",
    "        elif pos_enc == 'fixed': self.pos_enc = FixedPositionalEmbedding(d_emb)\n",
    "        elif pos_enc == 'axial':\n",
    "            assert axial_shape is not None\n",
    "            assert reduce(mul, axial_shape) == max_seq_len\n",
    "            axial_emb_dims = default(axial_emb_dims, get_axial_dims(d_emb, len(axial_shape)))\n",
    "            self.pos_enc = AxialPositionalEmbedding(d_emb, axial_shape, axial_emb_dims)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)  #* self.scale\n",
    "        x *= self.scale \n",
    "        x += self.pos_enc(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    def _init(self):\n",
    "        nn.init.trunc_normal_(self.emb.weight, std = self.std)\n",
    "        if hasattr(self.pos_enc, 'weight'): nn.init.trunc_normal_(self.pos_enc.weight, std = self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_attention.ipynb.\n",
      "Converted 02_transformer.ipynb.\n",
      "Converted 03_reformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
