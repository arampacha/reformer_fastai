{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "> Contains tokenizers used in Reformer paper experiments, converted to fastai transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import six\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ByteTextTokenizer\n",
    "\n",
    "A tokenizer which encodes each byte in a string to an id. For 8-bit strings only. This is the tokenizer used in Language Modelling tasks in the Reformer paper, based off the implementation in the [tensor2tensor library here](https://github.com/tensorflow/tensor2tensor/blob/5f9dd2db6d7797162e53adf152310ed13e9fc711/tensor2tensor/data_generators/text_encoder.py#L176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ByteTextTokenizer(Transform):\n",
    "    \"\"\"\n",
    "        Encodes each byte to an id. For 8-bit strings only.\n",
    "        Credit: https://github.com/tensorflow/tensor2tensor/blob/5f9dd2db6d7797162e53adf152310ed13e9fc711/tensor2tensor/data_generators/text_encoder.py#L176\n",
    "    \"\"\"\n",
    "    def __init__(self, is_lm=True, add_bos=False, add_eos=False):\n",
    "        store_attr('is_lm, add_bos, add_eos')\n",
    "        self.pad_token, self.eos_token, self.bos_token = '<pad>', '<eos>', '<bos>',\n",
    "        self.pad_token_id, self.eos_token_id, self.bos_token_id = 0,1,2\n",
    "        self.reserved_toks = [self.pad_token, self.eos_token, self.bos_token]  ## self.bos_token_id \n",
    "        self.reserved_tokens_bytes = [bytes(rtok, 'ascii') for rtok in self.reserved_toks]\n",
    "        self.numres = len(self.reserved_toks)\n",
    "        self.int2byte = six.int2byte\n",
    "\n",
    "    @typedispatch\n",
    "    def __call__(self, o:list, **kwargs):\n",
    "        out = [c + self.numres for s in o for c in s.encode(\"utf-8\")]\n",
    "        if self.add_bos: out = [self.bos_token_id] + out\n",
    "        if self.add_eos: out =  out + [self.eos_token_id]\n",
    "        if self.is_lm:return LMTensorText(out)\n",
    "        else: return TensorText(out) \n",
    "        \n",
    "    @typedispatch\n",
    "    def __call__(self, o:str, **kwargs):\n",
    "        out = [c + self.numres for c in o.encode(\"utf-8\")]\n",
    "        if self.add_bos: out = [self.bos_token_id] + out\n",
    "        if self.add_eos: out =  out + [self.eos_token_id]\n",
    "        if self.is_lm: return LMTensorText(out)\n",
    "        else: return TensorText(out) \n",
    "    \n",
    "    def encodes(self,o):\n",
    "        return self.__call__(o)\n",
    "    \n",
    "    def decodes(self, o:tuple):\n",
    "        decoded_ids = ()\n",
    "        for i in o:\n",
    "            tmp_ls=[]\n",
    "            for id_ in i:\n",
    "                if 0 <= id_ < self.numres: tmp_ls.append(self.reserved_tokens_bytes[int(id_)])\n",
    "                else: tmp_ls.append(self.int2byte(id_ - self.numres))\n",
    "            decoded_ids = decoded_ids + (b\"\".join(tmp_ls).decode(\"utf-8\", \"replace\"),)\n",
    "        return TitledStr(decoded_ids)\n",
    "    \n",
    "    def decodes(self, o:list):\n",
    "        decoded_ids = []\n",
    "        for id_ in o:\n",
    "            if 0 <= id_ < self.numres: decoded_ids.append(self.reserved_tokens_bytes[int(id_)])\n",
    "            else: decoded_ids.append(self.int2byte(id_ - self.numres))\n",
    "        return TitledStr(b\"\".join(decoded_ids).decode(\"utf-8\", \"replace\"))\n",
    "    \n",
    "    def decodes(self, o:TensorText):\n",
    "        return self.decodes(o.tolist())\n",
    "    \n",
    "    def decodes(self, o:LMTensorText):\n",
    "        return self.decodes(o.tolist())\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self): return 2**8 + self.numres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wonder = \"I wonder how the moon got it's shine?\"\n",
    "tok = ByteTextTokenizer()\n",
    "tok_wonder = tok(wonder)\n",
    "\n",
    "# test string vs list\n",
    "assert (tok(wonder) == tok([wonder])).sum() == len(tok(wonder)) \n",
    "# assert (tok.decode(tok_wonder) == tok.decode([tok_wonder])).sum() == len(wonder) \n",
    "assert type(tok_wonder) == LMTensorText\n",
    "assert len(tok_wonder) == 37\n",
    "assert tok.decode(tok_wonder) == wonder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = ByteTextTokenizer(add_bos=True)\n",
    "tok_wonder2 = tok2(wonder)\n",
    "assert tok_wonder2[0] == 2\n",
    "\n",
    "tok3 = ByteTextTokenizer(add_eos=True)\n",
    "tok_wonder3 = tok3(wonder)\n",
    "assert tok_wonder3[-1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 05_tokenizers_misktak.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
