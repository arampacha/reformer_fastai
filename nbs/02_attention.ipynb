{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from fastai.basics import *\n",
    "\n",
    "from functools import partial, reduce\n",
    "from inspect import isfunction\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from reformer_fastai.core import *\n",
    "from reformer_fastai.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MASK_VAL = -5e4\n",
    "SELF_ATTN_MASK_VAL = -1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttnInProj(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        context = ifnone(context, x)\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_k(context), self.to_v(context)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "proj = AttnInProj(d)\n",
    "q1, k1, v1 = proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2, k2, v2 = proj(x, context)\n",
    "assert (bs, sl, d) == q2.size()\n",
    "assert k2.size() == v2.size() == context.size()\n",
    "assert all_equal(q1, q2)\n",
    "assert not all_equal(k1, k2)\n",
    "q2.shape, k2.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttnInProjV2(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_kv = nn.Linear(d_model, 2*d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        context = ifnone(context, x)\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_kv(context).chunk(2, -1)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "proj = AttnInProj(d)\n",
    "q1, k1, v1 = proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2, k2, v2 = proj(x, context)\n",
    "assert (bs, sl, d) == q2.size()\n",
    "assert k2.size() == v2.size() == context.size()\n",
    "assert all_equal(q1, q2)\n",
    "assert not all_equal(k1, k2)\n",
    "q2.shape, k2.shape, v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Query-Key Attention Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SharedQKAttnInProj(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        self.to_qk = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        context = ifnone(context, x)\n",
    "        qk = self.to_qk(x)\n",
    "        v = self.to_v(context)\n",
    "        return qk, qk, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "shared_proj = SharedQKAttnInProj(d)\n",
    "q1, k1, v1 = shared_proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "assert q1 is k1\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO: Figure out tests for when using SharedQKAttnInProj and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO make sure store_attention works\n",
    "class ScaledDotProdAttention(Module):\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attnetion given q, k, v\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, causal=False, dropout=0., shared_qk=False, store_attention:bool=False):\n",
    "        store_attr()\n",
    "        self.scale = (d_model//n_heads)**-0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, input_mask=None):\n",
    "        device = q.device\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_heads), (q, k, v))\n",
    "        \n",
    "        #TODO: remove after refactor confirmed working\n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "#         input_mask = None\n",
    "#         if any(map(exists, (mask, context_mask))):\n",
    "#             q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "#             k_mask = q_mask if not exists(context) else context_mask\n",
    "#             k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
    "            \n",
    "#             q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "#             k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "#             input_mask = q_mask * k_mask\n",
    "        \n",
    "        # classic dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q*self.scale, k)\n",
    "        \n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "            \n",
    "        if self.shared_qk:\n",
    "            b, h, i, j = dots.shape[:]\n",
    "            m = torch.arange(i)\n",
    "            self_mask = torch.zeros(b, h, i, j).bool()    \n",
    "            self_mask[:, :, m, m] = True\n",
    "            dots.masked_fill_(self_mask, SELF_ATTN_MASK_VAL)\n",
    "            del self_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = dots.shape[-2:]\n",
    "            mask = torch.ones((i, j), device = device).triu_(j - i + 1).bool()\n",
    "            dots.masked_fill_(mask, MASK_VAL)\n",
    "            del mask\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: self.attention = attn.detach().cpu()\n",
    "        \n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot-product attention is calculated as:\n",
    "\n",
    "$$\\textbf {Attention}(Q,K,V) = \\textbf {softmax}({QK^T\\over\\sqrt d_k})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(bs, sl, d)\n",
    "k = torch.randn(bs, sl, d)\n",
    "v = torch.randn(bs, sl, d)\n",
    "attn_func = ScaledDotProdAttention(d, 4)\n",
    "out = attn_func(q, k, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test shared_qk\n",
    "attn_func = ScaledDotProdAttention(d, 4, shared_qk=True)\n",
    "out = attn_func(q, k, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO: add tests with input mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Attention(Module):\n",
    "    \"\"\"\n",
    "    Standard attention module using scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=False,\n",
    "                 shared_qk:bool=False,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, mask, n_heads, bias, shared_qk')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        if shared_qk: self.in_proj = SharedQKAttnInProj(d_model, bias=bias)\n",
    "        else: self.in_proj = AttnInProjV2(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, shared_qk=shared_qk, \n",
    "                                           store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        q, k, v = self.in_proj(x, context)\n",
    "        if self.shared_qk: k = F.normalize(k, 2, dim=-1).type_as(k)\n",
    "                \n",
    "        input_mask = self._make_input_mask(mask, context_mask, x, context)\n",
    "        out = self.attn(q, k, v, input_mask)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]\n",
    "    \n",
    "    def _make_input_mask(self, mask, context_mask, x, context):\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            b, n, _, device = *x.size(), x.device\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "            \n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            return q_mask * k_mask\n",
    "        else: return None #input_mask is None if both mask and context_mask are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "attn = Attention(d)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attn(x, context)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test shared_qk\n",
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "attn = Attention(d, shared_qk=True)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_msg = \"Causal masking error\"\n",
    "attn = Attention(d, causal=True, dropout=0)\n",
    "x1 = torch.randn(bs, sl, d)\n",
    "out1 = attn(x1)\n",
    "x2 = x1.clone()\n",
    "x2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "out2 = attn(x2)\n",
    "# all elements in first half are equal despite second half is defferent\n",
    "assert all_equal(out1[:, :sl//2], out2[:, :sl//2]), e_msg\n",
    "assert not (out1[:, sl//2:] == out2[:, sl//2:]).any(), e_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_msg = \"Masking error\"\n",
    "attn = Attention(d, causal=False, dropout=0)\n",
    "x1 = torch.randn(bs, sl, d)\n",
    "mask = torch.ones(bs, sl)\n",
    "# mask out second half of input\n",
    "mask[:, sl//2:] = 0\n",
    "mask = mask.bool()\n",
    "out1 = attn(x1, mask=mask)\n",
    "x2 = x1.clone()\n",
    "x2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "out2 = attn(x2, mask=mask)\n",
    "# all elements are equal, masked values do not effect result\n",
    "assert all_equal(out1[:, :sl//2], out2[:, :sl//2]), e_msg\n",
    "out1 = attn(x1)\n",
    "out2 = attn(x2)\n",
    "assert not (out1[:, :sl//2] == out2[:, :sl//2]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_msg = \"Context masking error\"\n",
    "attn = Attention(d, causal=False, dropout=0)\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl, d)\n",
    "context_mask = torch.ones(bs, sl)\n",
    "# mask out second half of context\n",
    "context_mask[:, sl//2:] = 0\n",
    "context_mask = context_mask.bool()\n",
    "out1 = attn(x, context, context_mask=context_mask)\n",
    "context2 = context.clone()\n",
    "context2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "out2 = attn(x, context2, context_mask=context_mask)\n",
    "# all elements are equal, masked values do not effect result\n",
    "assert all_equal(out1, out2), e_msg\n",
    "# all output values are different for different context\n",
    "out1 = attn(x, context)\n",
    "out2 = attn(x, context2)\n",
    "assert not (out1 == out2).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check stored attention matrix\n",
    "bs = 4\n",
    "sl = 16\n",
    "csl = sl + 16\n",
    "d = 64\n",
    "x = torch.rand(bs, sl, d)\n",
    "context = torch.rand(bs, csl, d)\n",
    "mask = torch.ones(bs, sl)\n",
    "mask[:, -5:] = 0\n",
    "context_mask = torch.ones(bs, csl)\n",
    "context_mask[:, -10:] = 0\n",
    "mask, context_mask = mask.bool(), context_mask.bool()\n",
    "attn = Attention(d, store_attention=True)\n",
    "out = attn(x, context, mask=mask, context_mask=context_mask)\n",
    "attention = attn.attn.attention\n",
    "assert (bs, sl, d) == out.size()\n",
    "assert attention.size() == (bs, attn.attn.n_heads, sl, csl)\n",
    "# zeros for masked keys and \"don't cares\" for masked queries\n",
    "plt.matshow(attention[0,0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# previous version of additive attention for reference\n",
    "class DecoderAttention(Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 heads = 8, \n",
    "                 causal = False,\n",
    "                 mask = None,\n",
    "                 dropout=0.1, \n",
    "                 bias=False):\n",
    "        self.causal = causal\n",
    "        self.store_attention = False\n",
    "        self.mask = mask #??\n",
    "        self.heads = heads\n",
    "        self.scale = (d_model//heads) ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, d_model, bias = bias)\n",
    "        self.to_kv = nn.Linear(d_model, d_model * 2, bias = bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None, store_attention=False):\n",
    "        b, n, d, h, device = *x.shape, self.heads, x.device\n",
    "        context = default(context, torch.empty(b, 0, d, dtype=x.dtype, device=device))\n",
    "        kv_input = torch.cat([x, context], dim=-2)\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n",
    "\n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            self_mask = q_mask[:, None, :, None] * q_mask[:, None, None, :]\n",
    "            if context.size(-2) != 0:\n",
    "                k_mask = default(context_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "                cross_mask = q_mask[:, None, :, None] * k_mask[:, None, None, :]\n",
    "            else: cross_mask = torch.empty(0, dtype=self_mask.dtype, device=device)\n",
    "            input_mask = torch.cat([self_mask, cross_mask], dim=-1)\n",
    "        \n",
    "        # classic scaled dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q * self.scale, k)\n",
    "        \n",
    "        # might need to tune MASK_VAL for fp16 to work\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = torch.triu_indices(n, n, 1)\n",
    "            dots[:,:,i,j] = MASK_VAL\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: # and not self.training\n",
    "            self.attention = attn.detach().cpu()\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in [self.to_q.weight, self.to_kv.weight, self.to_out.weight]]\n",
    "        if getattr(self.to_q, 'bias', None) is not None: nn.init.constant_(self.to_q.bias, 0)\n",
    "        if getattr(self.to_kv, 'bias', None) is not None: nn.init.constant_(self.to_kv.bias, 0)\n",
    "        nn.init.constant_(self.to_out.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdditiveInProj(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        b, _, d = x.size()\n",
    "        context = ifnone(context, torch.empty(b, 0, d, dtype=x.dtype, device=x.device))\n",
    "        kv_input = torch.cat([x, context], dim=-2)\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_k(kv_input), self.to_v(kv_input)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "proj = AdditiveInProj(d)\n",
    "q1, k1, v1 = proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 240, 64]), torch.Size([4, 240, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2, k2, v2 = proj(x, context)\n",
    "assert (bs, sl, d) == q2.size()\n",
    "assert k2.size() == v2.size() == (bs, x.size(1)+context.size(1), d)\n",
    "assert all_equal(q1, q2)\n",
    "assert not all_equal(k1, k2)\n",
    "q2.shape, k2.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: add support for shared_qk for additive attn\n",
    "class AdditiveAttention(Attention):\n",
    "    \"\"\"\n",
    "    Additive attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=False,\n",
    "                 shared_qk:bool = False,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, n_heads, bias, shared_qk')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        self.in_proj = AdditiveInProj(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "    \n",
    "    def _make_input_mask(self, mask, context_mask, x, context):\n",
    "        b, n, _, device = *x.size(), x.device\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device=device).bool())\n",
    "            self_mask = q_mask[:, None, :, None] * q_mask[:, None, None, :]\n",
    "            if exists(context):\n",
    "                k_mask = default(context_mask, lambda: torch.ones((b, context.shape[-2]), device=device).bool())\n",
    "                cross_mask = q_mask[:, None, :, None] * k_mask[:, None, None, :]\n",
    "            else: cross_mask = torch.empty(0, dtype=self_mask.dtype, device=device)\n",
    "            return torch.cat([self_mask, cross_mask], dim=-1)\n",
    "        else: return None #input_mask is None if both mask and context_mask are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "attn = AdditiveAttention(d)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO: tests for additive attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attn(x, context)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO: add tests with input mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_msg = \"Causal masking error\"\n",
    "attn = AdditiveAttention(d, causal=True, dropout=0)\n",
    "x1 = torch.randn(bs, sl, d)\n",
    "out1 = attn(x1)\n",
    "x2 = x1.clone()\n",
    "x2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "out2 = attn(x2)\n",
    "# all elements in first half are equal despite second half is defferent\n",
    "assert all_equal(out1[:, :sl//2], out2[:, :sl//2]), e_msg\n",
    "assert not (out1[:, sl//2:] == out2[:, sl//2:]).any(), e_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_msg = \"Masking error\"\n",
    "attn = AdditiveAttention(d, causal=False, dropout=0)\n",
    "x1 = torch.randn(bs, sl, d)\n",
    "mask = torch.ones(bs, sl)\n",
    "# mask out second half of input\n",
    "mask[:, sl//2:] = 0\n",
    "mask = mask.bool()\n",
    "out1 = attn(x1, mask=mask)\n",
    "x2 = x1.clone()\n",
    "x2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "out2 = attn(x2, mask=mask)\n",
    "# all elements are equal, masked values do not effect result\n",
    "assert all_equal(out1[:, :sl//2], out2[:, :sl//2]), e_msg\n",
    "out1 = attn(x1)\n",
    "out2 = attn(x2)\n",
    "assert not (out1[:, :sl//2] == out2[:, :sl//2]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_msg = \"Context masking error\"\n",
    "attn = Attention(d, causal=False, dropout=0)\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl, d)\n",
    "context_mask = torch.ones(bs, sl)\n",
    "# mask out second half of context\n",
    "context_mask[:, sl//2:] = 0\n",
    "context_mask = context_mask.bool()\n",
    "out1 = attn(x, context, context_mask=context_mask)\n",
    "context2 = context.clone()\n",
    "context2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "out2 = attn(x, context2, context_mask=context_mask)\n",
    "# all elements are equal, masked values do not effect result\n",
    "assert all_equal(out1, out2), e_msg\n",
    "# all output values are different for different context\n",
    "out1 = attn(x, context)\n",
    "out2 = attn(x, context2)\n",
    "assert not (out1 == out2).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSH attention from Reformer: [The Efficient Transformer](https://arxiv.org/abs/2001.04451). Based on [lucidrains/reformer-pytorch](https://github.com/lucidrains/reformer-pytorch/), but simpliefied and refactored. Uses shared keys and queries, but requires both to be passed as input (even though they are identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSHAttention(Module):\n",
    "    \"\"\"\n",
    "    LSH attention module: \n",
    "    \"\"\"\n",
    "    def __init__( self,\n",
    "                  dropout = 0.,                       # attention matrix dropout\n",
    "                  bucket_size = 64,                   # at least 64 suggested in trax\n",
    "                  n_hashes = 8,                       # papers sugests 8\n",
    "                  causal = False,\n",
    "                  allow_duplicate_attention = False,  # as in the paper\n",
    "                  attend_across_buckets = False,      # as in the paper\n",
    "                  drop_for_hash_rate = 0.0,           # unsure of default, not mentioned in paper\n",
    "                  return_attn = False,\n",
    "                  **kwargs):\n",
    "        \n",
    "        if dropout >= 1.0 or drop_for_hash_rate >=1.0:\n",
    "            raise ValueError('Dropout rates must be lower than 1.')\n",
    "        \n",
    "        store_attr(but=['dropout', 'drop_for_hash_rate'])  # fastcore - store attibutes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n",
    "        self._cache = {} # cache buckets for reversible network, required to make Reformer work at depth\n",
    "\n",
    "    @cache_method_decorator('_cache', 'buckets', reexecute=True)\n",
    "    def hash_vectors(self, n_buckets, vecs):\n",
    "        # 0. We need an even number of buckets: \n",
    "        assert n_buckets % 2 == 0\n",
    "\n",
    "        # 1. account for the input shapes. vecs = [bs, sl, dim]\n",
    "        batch_size, seqlen, dim = vecs.shape\n",
    "        device = vecs.device\n",
    "        rotations_shape = (dim, self.n_hashes, n_buckets // 2)\n",
    "\n",
    "        # 2. Calculate hash bucket id via random rotations, concatenation and argmax \n",
    "        # note: we copy rotations accross batch dimension (see exploration notebook for details). \n",
    "        random_rotations = repeat(torch.randn(rotations_shape,device=device), \n",
    "                                  'd nh nb -> bs d nh nb', bs=batch_size)           \n",
    "        dropped_vecs = self.dropout_for_hash(vecs)\n",
    "                       \n",
    "        rotated_vecs = torch.einsum('bsd,bdhn->bhsn', \n",
    "                                    dropped_vecs,       # [bs, sl, dim]\n",
    "                                    random_rotations)   # [bs, dim, n_hashes, n_buckets//2]\n",
    "                                                        # rotated vecs: [bs, n_hashes, sl, n_buckets//2]\n",
    "\n",
    "        rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1) # [bs, n_hashes, sl, n_buckets]\n",
    "        buckets = torch.argmax(rotated_vecs, dim=-1)                    # [bs, n_hashes, sl] \n",
    "\n",
    "        # 3. Next we add offsets so that bucket numbers from different hashing rounds don't overlap.\n",
    "        # We also reshape the buckets so that each hash round is concatenated along the -1 dim\n",
    "        offsets = torch.arange(self.n_hashes,device=device)                              # list of [0,1,2,..n_hashes-1]\n",
    "        offsets = rearrange(offsets * n_buckets, 'nh -> 1 nh 1')        # [1, n_hashes, 1]\n",
    "        buckets = rearrange(buckets+offsets, 'bs nh sl -> bs (nh sl)')  # [bs, (n_hashes*sl)]\n",
    "        return buckets\n",
    "\n",
    "    def forward(self, q, k, v, input_mask = None, **kwargs):\n",
    "        batch_size, seqlen, dim, device = *q.shape, q.device\n",
    "\n",
    "        # caching\n",
    "        is_reverse = kwargs.pop('_reverse', False)\n",
    "        depth = kwargs.pop('_depth', None)\n",
    "        \n",
    "        # We will have an even number of buckets, and our attention chunks needs to fit completely within a seqlen\n",
    "        assert seqlen % (self.bucket_size * 2) == 0, f'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}'\n",
    "        \n",
    "        # get the hash buckets for our q,k input vectors\n",
    "        n_buckets = seqlen // self.bucket_size\n",
    "        buckets = self.hash_vectors(n_buckets, q, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n",
    "\n",
    "        # We use the same vector as both a query and a key.\n",
    "        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n",
    "        \n",
    "        # Create an index that reflexts both bucket id and sequence id. This let's us sort q, k according \n",
    "        # to both simultaneously. Repeated across the batch dimension.\n",
    "        ticker = repeat(torch.arange((self.n_hashes * seqlen),device=device), 'l -> bs l', bs=batch_size)\n",
    "        buckets_and_t = seqlen * buckets + (ticker % seqlen) \n",
    "        buckets_and_t = buckets_and_t.detach()                # [bs, seqlen*n_hashes]\n",
    "\n",
    "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)  # [bs, seqlen*n_hashes]\n",
    "        _, undo_sort = sticker.sort(dim=-1)                                    # indexes to undo sortings\n",
    "        del ticker\n",
    "\n",
    "        sbuckets_and_t = sbuckets_and_t.detach()   # no need to store gradiens for indexes\n",
    "        sticker = sticker.detach()\n",
    "        undo_sort = undo_sort.detach()\n",
    "\n",
    "        st = (sticker % seqlen)            # index of [0..seqlen-1] for each hash round\n",
    "        sq = batched_index_select(q, st)   # get the sorted q, [bs, seqlen*n_hashes, dim]\n",
    "        sk = batched_index_select(k, st)   # get the sorted k, [bs, seqlen*n_hashes, dim]\n",
    "        sv = batched_index_select(v, st)   # get the sorted v, [bs, seqlen*n_hashes, dim] \n",
    "\n",
    "        # Reshape to include a n_chunks axis.\n",
    "        n_chunks = self.n_hashes * n_buckets\n",
    "        bq_t = bkv_t = rearrange(st, 'bs (n s) -> bs n s', n=n_chunks) # [bs, n_chunks, chunk_size]\n",
    "        bq = rearrange(sq, 'bs (n s) d -> bs n s d', n=n_chunks)       # [bs, n_chunks, chunk_size, dim]\n",
    "        bk = rearrange(sk, 'bs (n s) d -> bs n s d', n=n_chunks)       # [bs, n_chunks, chunk_size, dim]\n",
    "        bv = rearrange(sv, 'bs (n s) d -> bs n s d', n=n_chunks)       # [bs, n_chunks, chunk_size, dim]\n",
    "\n",
    "        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n",
    "        # fine because they effectively provide a learnable temperature for the\n",
    "        # attention softmax, but normalizing keys is needed so that similarity for\n",
    "        # the purposes of attention correctly corresponds to hash locality.\n",
    "        bk = F.normalize(bk, p=2, dim=-1)\n",
    "\n",
    "        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
    "        # boundaries might occur in the middle of a sequence of items from the\n",
    "        # same bucket, so this increases the chances of attending to relevant items.\n",
    "        # Note: no look_back for queries\n",
    "\n",
    "        bk = look_one_back(bk)        # [bs, n_chunks, chunk_size*2, dim]\n",
    "        bv = look_one_back(bv)        # [bs, n_chunks, chunk_size*2, dim]\n",
    "        bkv_t = look_one_back(bkv_t)\n",
    "\n",
    "        # Dot-product attention.\n",
    "        dots = torch.einsum('bnsd,bnzd->bnsz', \n",
    "                    bq,                  # [bs, n_chunks, chunk_size, dim]\n",
    "                    bk                   # [bs, n_chunks, chunk_size*2, dim]\n",
    "                   ) * (dim ** -0.5)     # dots: [bs, n_chunks, chunk_size, chunk_size*2]\n",
    "        masked_value = max_neg_value(dots)\n",
    "\n",
    "        # Input mask for padding in variable lengthed sequences\n",
    "        if input_mask is not None:\n",
    "            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n",
    "            mq = input_mask.gather(1, st).reshape((batch_size, n_chunks, -1))\n",
    "            mkv = look_one_back(mq)\n",
    "            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n",
    "            dots.masked_fill_(~mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Causal masking\n",
    "        if self.causal:\n",
    "            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n",
    "            dots.masked_fill_(mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Mask out attention to self except when no other targets are available.\n",
    "        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n",
    "        dots.masked_fill_(self_mask, SELF_ATTN_MASK_VAL)\n",
    "        del self_mask\n",
    "\n",
    "        # Mask out attention to other hash buckets.\n",
    "        if not self.attend_across_buckets:\n",
    "            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, n_chunks, -1))\n",
    "            bkv_buckets = look_one_back(bkv_buckets)\n",
    "            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n",
    "            dots.masked_fill_(bucket_mask, masked_value)\n",
    "            del bucket_mask\n",
    "\n",
    "        # Don't double-count query-key pairs across multiple rounds of hashing.\n",
    "        # There are two possible strategies here. (1) The default is to count how\n",
    "        # many times a query-key pair is repeated, and to lower its log-prob\n",
    "        # correspondingly at each repetition.\n",
    "        \n",
    "        if not self.allow_duplicate_attention:\n",
    "            locs1 = undo_sort // bq_t.shape[-1]\n",
    "            locs2 = (locs1 + 1) % n_chunks\n",
    "            if not self.attend_across_buckets:\n",
    "                locs1 = buckets * n_chunks + locs1\n",
    "                locs2 = buckets * n_chunks + locs2\n",
    "            locs = torch.cat([\n",
    "                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n",
    "                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n",
    "            ], 1).permute((0, 2, 1))\n",
    "\n",
    "            slocs = batched_index_select(locs, st)\n",
    "            b_locs = torch.reshape(slocs, (batch_size, n_chunks, -1, 2 * self.n_hashes))\n",
    "\n",
    "            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n",
    "\n",
    "            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n",
    "            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n",
    "            bkv_locs = look_one_back(b_locs)\n",
    "\n",
    "            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n",
    "            # for memory considerations, chunk summation of last dimension for counting duplicates\n",
    "            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n",
    "            dup_counts = dup_counts.detach()\n",
    "            assert dup_counts.shape == dots.shape\n",
    "            dots = dots - torch.log(dup_counts + 1e-9)\n",
    "            del dup_counts\n",
    "\n",
    "        # Softmax.\n",
    "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
    "        dots = torch.exp(dots - dots_logsumexp).type_as(dots)\n",
    "        dropped_dots = self.dropout(dots)\n",
    "        \n",
    "        # calculate self-attention (attn * values)\n",
    "        bo = torch.einsum('bnsz,bnzd->bnsd', \n",
    "                          dropped_dots,      # [bs, n_chunks, chunk_size, chunk_size*2]\n",
    "                          bv)                # [bs, n_chunks, chunk_size*2, dim]    \n",
    "                                             # bo: [bs, n_chunks, chunk_size, dim]\n",
    "        \n",
    "        # unchunk, unsort and reshape self-attention\n",
    "        so = rearrange(bo, 'b n s d -> b (n s) d')                     # [bs, seqlen*n_hashes, dim]\n",
    "        o = batched_index_select(so, undo_sort)                        # [bs, seqlen*n_hashes, dim]\n",
    "        o = rearrange(o, 'b (nh sl) d -> b nh sl d', nh=self.n_hashes) # [bs, n_hashes, seqlen, dim]\n",
    "        \n",
    "        # unchunk, unsort and reshape logits\n",
    "        slogits = rearrange(dots_logsumexp, 'bs n s 1 -> bs (n s)')              # [bs, seqlen*n_hashes]\n",
    "        logits = slogits.gather(1, undo_sort)                                    # [bs, seqlen*n_hashes]\n",
    "        logits = rearrange(logits, 'bs (nr sl) -> bs nr sl 1', nr=self.n_hashes) # [bs, n_hashes, seqlen, 1]\n",
    "        \n",
    "        # average probabilites across hash rounds (dim 1) and get weighted attention\n",
    "        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True)) # [bs, n_rounds, seqlen, 1]\n",
    "        out = torch.sum(o * probs, dim=1)                                        # [bs, seqlen, dim]\n",
    "\n",
    "        # return unsorted attention weights - empty otherwise\n",
    "        attn = torch.empty(0, device=device)\n",
    "        if self.return_attn:\n",
    "            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n",
    "            attn_unsort = attn_unsort.view(batch_size * self.n_hashes, -1).long()\n",
    "            unsorted_dots = torch.zeros(batch_size * self.n_hashes, seqlen * seqlen, device=device)\n",
    "            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n",
    "            del attn_unsort\n",
    "            unsorted_dots = unsorted_dots.reshape(batch_size, self.n_hashes, seqlen, seqlen)\n",
    "            attn = torch.sum(unsorted_dots * probs, dim=1)\n",
    "\n",
    "        # return output, attention matrix, and bucket distribution\n",
    "        return out, attn, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test LSH-attention layer. Note: `d_model` is infered from input. Assumes shared key and query, but accepts both as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "shared_proj = SharedQKAttnInProj(d)\n",
    "q, k, v = shared_proj(x)\n",
    "lsh_attn = LSHAttention()\n",
    "out, _, _ = lsh_attn(q, k, v)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH-self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs multihead `LSHAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSHSelfAttention(Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_heads = 8, \n",
    "                 bucket_size = 64,                    # reccomended default from paper/lucid\n",
    "                 n_hashes = 8,                        # reccomended default from paper/lucid\n",
    "                 causal = False, \n",
    "                 bias:bool=False,\n",
    "                 attend_across_buckets = False,      \n",
    "                 allow_duplicate_attention = False,   # Penalize multiple qk-v pairs in same attention chunk or not\n",
    "                 return_attn = False,                 # Not implemented yet\n",
    "                 dropout = 0., \n",
    "                 post_attn_dropout = 0.):             # a final dropout on output (not standard)\n",
    "         \n",
    "        assert (d_model % n_heads) == 0, 'dimensions must be divisible by number of heads'\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.in_proj = SharedQKAttnInProj(d_model, bias=bias)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        \n",
    "        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, \n",
    "                                     attend_across_buckets = attend_across_buckets,  \n",
    "                                     allow_duplicate_attention = allow_duplicate_attention, \n",
    "                                     return_attn = return_attn, dropout = dropout)\n",
    "        self.post_attn_dropout = nn.Dropout(post_attn_dropout)\n",
    "\n",
    "    def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, context_mask = None, **kwargs):\n",
    "        device, dtype = x.device, x.dtype\n",
    "        bs, sl, d_model = x.shape\n",
    "        \n",
    "        # to be refactored\n",
    "        keys = default(keys, torch.empty(bs, 0, d_model, dtype=dtype, device=device))\n",
    "        c = keys.shape[1]\n",
    "\n",
    "        # project keys, queries and values\n",
    "        q, k, v = self.in_proj(x)     # [bs, sl, d_model]\n",
    "        \n",
    "        # split off head dimension for q, k and v. Resulting shapes are: [nh, bs, sl, dim_head]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'bs sl (nh dh) -> nh bs sl dh', nh=self.n_heads), (q, k, v))\n",
    "        \n",
    "        # masks have shape [bs, sl] and are maybe concatenated [bs, sl*2]\n",
    "        mask = None\n",
    "        if input_mask is not None or context_mask is not None:\n",
    "            default_mask = torch.tensor([True], device=device)\n",
    "            i_mask = default(input_mask, default_mask.expand(bs, sl))\n",
    "            c_mask = default(context_mask, default_mask.expand(bs, c))\n",
    "            mask = torch.cat((i_mask, c_mask), dim=1)\n",
    "        \n",
    "        # run lsh per head (iterate through 0th dim i.e. the n_head dim), concatenate and rearrange\n",
    "        # Note: masks are reused per head\n",
    "        lsh_results = L([self.lsh_attn(q_h, k_h, v_h, mask) for q_h, k_h, v_h in zip(q, k, v)])  \n",
    "        out = lsh_results.itemgot(0)                                   # split tuple (output, attn, buckets)\n",
    "        out = torch.cat([head for head in out], dim=0)                 # concatenate [n_heads*bs, sl, dh]\n",
    "        out = rearrange(out, '(nh bs) sl dh -> bs sl (nh dh)', bs=bs)  # [bs, sl, dim_heads] (dim_heads = head_dim * n_heads)\n",
    "        \n",
    "        # pass through final feed forward and maybe dropout\n",
    "        out = self.out_proj(out)                                            # [bs, sl, dim]\n",
    "        return self.post_attn_dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 512\n",
    "x = torch.randn(bs, sl, d)\n",
    "attn = LSHSelfAttention(d)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformer Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformer attention calculates multihead attention with shared keys and queries, and allows switching between full `Attention` or `LSHAttention` at creation, but not during inference or training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReformerAttention(Module):\n",
    "    \"\"\"\n",
    "    Reformer attention container.\n",
    "    \n",
    "    Switch between FullSharedQKAttention and LSHAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=False,\n",
    "                 store_attention:bool=False,\n",
    "                 lsh_attention:bool = True,\n",
    "                 n_hashes:int = 8, \n",
    "                 bucket_size:int = 64):\n",
    "        \n",
    "        store_attr('causal, mask, n_heads, bias, lsh_attention')\n",
    "        \n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        \n",
    "        if lsh_attention: \n",
    "            self.attn = LSHSelfAttention(d_model,\n",
    "                                         n_heads = n_heads, \n",
    "                                         bucket_size=bucket_size,\n",
    "                                         n_hashes=n_hashes,\n",
    "                                         causal=causal, \n",
    "                                         dropout=dropout, \n",
    "                                         return_attn=store_attention)\n",
    "        \n",
    "        else: self.attn = Attention(d_model, \n",
    "                                    n_heads, \n",
    "                                    causal=causal,\n",
    "                                    shared_qk=True,\n",
    "                                    dropout=dropout, \n",
    "                                    store_attention=store_attention)\n",
    "        \n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        \n",
    "        out = self.attn(x, mask, context_mask)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 512\n",
    "x = torch.randn(bs, sl, d)\n",
    "attn_lsh = ReformerAttention(d, lsh_attention=True)\n",
    "out = attn_lsh(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_full = ReformerAttention(d, lsh_attention=False)\n",
    "out = attn_full(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state dicts of full and lsh attention are identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attn.in_proj.to_qk.weight', torch.Size([512, 512])),\n",
       " ('attn.in_proj.to_v.weight', torch.Size([512, 512])),\n",
       " ('attn.out_proj.weight', torch.Size([512, 512]))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.shape) for k, v in attn_lsh.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attn.in_proj.to_qk.weight', torch.Size([512, 512])),\n",
       " ('attn.in_proj.to_v.weight', torch.Size([512, 512])),\n",
       " ('attn.out_proj.weight', torch.Size([512, 512]))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.shape) for k, v in attn_full.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReformerAttentionV2(Module):\n",
    "    \"\"\"\n",
    "    Reformer attention container. Take on making it switchable on the fly.\n",
    "    \n",
    "    Switch between FullSharedQKAttention and LSHAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=False,\n",
    "                 store_attention:bool=False,\n",
    "                 lsh_attention:bool = True,\n",
    "                 n_hashes:int = 8, \n",
    "                 bucket_size:int = 64):\n",
    "        store_attr('causal, mask, n_heads, bias, lsh_attention')\n",
    "        \n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        self.in_proj = SharedQKAttnInProj(d_model, bias=bias)\n",
    "        \n",
    "        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, \n",
    "                                     return_attn=store_attention, dropout=dropout)\n",
    "#         self.lsh_attn = LSHSelfAttention(d_model,\n",
    "#                                          n_heads = n_heads, \n",
    "#                                          bucket_size=bucket_size,\n",
    "#                                          n_hashes=n_hashes,\n",
    "#                                          causal=causal, \n",
    "#                                          dropout=dropout, \n",
    "#                                          return_attn=store_attention)\n",
    "        self.full_attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                                dropout=dropout, shared_qk=True,\n",
    "                                                store_attention=store_attention)\n",
    "        \n",
    "#         self.full_attn = Attention(d_model, \n",
    "#                                     n_heads, \n",
    "#                                     causal=causal,\n",
    "#                                     shared_qk=True,\n",
    "#                                     dropout=dropout, \n",
    "#                                     store_attention=store_attention)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context=None, input_mask=None, context_mask=None):\n",
    "        #doesn't support cross attention for now?\n",
    "        assert context is None, \"sharedQK doesn't support cross attention yet\"\n",
    "        q, k, v = self.in_proj(x)\n",
    "        # use LSH\n",
    "        if self.lsh_attention:\n",
    "            q, k, v = map(lambda t: rearrange(t, 'bs sl (nh dh) -> nh bs sl dh', nh=self.n_heads), (q, k, v))\n",
    "        \n",
    "            # masks have shape [bs, sl] and are maybe concatenated [bs, sl*2]\n",
    "            mask = None\n",
    "            if input_mask is not None or context_mask is not None:\n",
    "                default_mask = torch.tensor([True], device=device)\n",
    "                i_mask = default(input_mask, default_mask.expand(bs, sl))\n",
    "                c_mask = default(context_mask, default_mask.expand(bs, c))\n",
    "                mask = torch.cat((i_mask, c_mask), dim=1)\n",
    "\n",
    "            # run lsh per head (iterate through 0th dim i.e. the n_head dim), concatenate and rearrange\n",
    "            # Note: masks are reused per head\n",
    "            lsh_results = L([self.lsh_attn(q_h, k_h, v_h, mask) for q_h, k_h, v_h in zip(q, k, v)])  \n",
    "            out = lsh_results.itemgot(0)                                   # split tuple (output, attn, buckets)\n",
    "            out = torch.cat([head for head in out], dim=0)                 # concatenate [n_heads*bs, sl, dh]\n",
    "            out = rearrange(out, '(nh bs) sl dh -> bs sl (nh dh)', bs=bs)  # [bs, sl, dim_heads] (dim_heads = head_dim * n_heads)\n",
    "        # use full attention\n",
    "        else:\n",
    "            out = self.full_attn(q, k, v)\n",
    "            \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]\n",
    "    #TODO: add mask generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReformerAttentionV2 containes both `LSHAttention` and `ScaledDotProdAttention` and which one to use is determined by `self.lsh_attention` flag.\n",
    "\n",
    "Proposed TODOs:\n",
    "- [ ] rename `self.lsh_attention` to `self.use_lsh` to avoid confusion with `self.lsh_attn` which is a module\n",
    "- [ ] synchronize mask naming across all Attention modules\n",
    "- [ ] add masking support to ReformerAttentionV2\n",
    "- [ ] synchronize `store_attention` functionality\n",
    "- [ ] test switchable attention module with synthetic task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 512\n",
    "x = torch.randn(bs, sl, d)\n",
    "attn = ReformerAttentionV2(d, lsh_attention=True)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch to full attention\n",
    "attn.lsh_attention = False\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerAttentionV2(\n",
       "  (in_proj): SharedQKAttnInProj(\n",
       "    (to_qk): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (lsh_attn): LSHAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (full_attn): ScaledDotProdAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State dict remanes unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in_proj.to_qk.weight', torch.Size([512, 512])),\n",
       " ('in_proj.to_v.weight', torch.Size([512, 512])),\n",
       " ('out_proj.weight', torch.Size([512, 512]))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.shape) for k, v in attn.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
