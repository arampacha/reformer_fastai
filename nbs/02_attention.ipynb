{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from fastai.basics import *\n",
    "\n",
    "from functools import partial, reduce\n",
    "from inspect import isfunction\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from reformer_fastai.core import *\n",
    "from reformer_fastai.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MASK_VAL = 5e-4\n",
    "SELF_ATTN_MASK_VAL = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttnInProj(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        context = ifnone(context, x)\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_k(context), self.to_v(context)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "proj = AttnInProj(d)\n",
    "q1, k1, v1 = proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2, k2, v2 = proj(x, context)\n",
    "assert (bs, sl, d) == q2.size()\n",
    "assert k2.size() == v2.size() == context.size()\n",
    "assert all_equal(q1, q2)\n",
    "assert not all_equal(k1, k2)\n",
    "q2.shape, k2.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO make sure store_attention works\n",
    "class ScaledDotProdAttention(Module):\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attnetion given q, k, v\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, causal=False, dropout=0., store_attention:bool=False):\n",
    "        store_attr()\n",
    "        self.scale = (d_model//n_heads)**-0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, input_mask=None):\n",
    "        device = q.device\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_heads), (q, k, v))\n",
    "        \n",
    "        #TODO: remove after refactor confirmed working\n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "#         input_mask = None\n",
    "#         if any(map(exists, (mask, context_mask))):\n",
    "#             q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "#             k_mask = q_mask if not exists(context) else context_mask\n",
    "#             k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
    "            \n",
    "#             q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "#             k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "#             input_mask = q_mask * k_mask\n",
    "        \n",
    "        # classic dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q*self.scale, k)\n",
    "        \n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = dots.shape[-2:]\n",
    "            mask = torch.ones((i, j), device = device).triu_(j - i + 1).bool()\n",
    "            dots.masked_fill_(mask, MASK_VAL)\n",
    "            del mask\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: self.attention = attn.detach().cpu()\n",
    "        \n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot-product attention is calculated as:\n",
    "\n",
    "$$\\textbf {Attention}(Q,K,V) = \\textbf {softmax}({QK^T\\over\\sqrt d_k})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(bs, sl, d)\n",
    "k = torch.randn(bs, sl, d)\n",
    "v = torch.randn(bs, sl, d)\n",
    "attn_func = ScaledDotProdAttention(d, 4)\n",
    "out = attn_func(q, k, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO: add tests with input mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Attention(Module):\n",
    "    \"\"\"\n",
    "    Standard attention module using scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=True,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, mask, n_heads, bias')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        self.in_proj = AttnInProj(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        q, k, v = self.in_proj(x, context)\n",
    "        input_mask = self._make_input_mask(mask, context_mask, context, x.device)\n",
    "        out = self.attn(q, k, v, input_mask)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]\n",
    "    \n",
    "    def _make_input_mask(self, mask, context_mask, context, device):\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
    "            \n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            return q_mask * k_mask\n",
    "        else: return None #input_mask is None if both mask and context_mask are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "attn = Attention(d)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO: add tests with input mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# previous version of additive attention for reference\n",
    "class DecoderAttention(Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 heads = 8, \n",
    "                 causal = False,\n",
    "                 mask = None,\n",
    "                 dropout=0.1, \n",
    "                 bias=True):\n",
    "        self.causal = causal\n",
    "        self.store_attention = False\n",
    "        self.mask = mask #??\n",
    "        self.heads = heads\n",
    "        self.scale = (d_model//heads) ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, d_model, bias = bias)\n",
    "        self.to_kv = nn.Linear(d_model, d_model * 2, bias = bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None, store_attention=False):\n",
    "        b, n, d, h, device = *x.shape, self.heads, x.device\n",
    "        context = default(context, torch.empty(b, 0, d, dtype=x.dtype, device=device))\n",
    "        kv_input = torch.cat([x, context], dim=-2)\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n",
    "\n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            self_mask = q_mask[:, None, :, None] * q_mask[:, None, None, :]\n",
    "            if context.size(-2) != 0:\n",
    "                k_mask = default(context_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "                cross_mask = q_mask[:, None, :, None] * k_mask[:, None, None, :]\n",
    "            else: cross_mask = torch.empty(0, dtype=self_mask.dtype, device=device)\n",
    "            input_mask = torch.cat([self_mask, cross_mask], dim=-1)\n",
    "        \n",
    "        # classic scaled dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q * self.scale, k)\n",
    "        \n",
    "        # might need to tune MASK_VAL for fp16 to work\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = torch.triu_indices(n, n, 1)\n",
    "            dots[:,:,i,j] = MASK_VAL\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: # and not self.training\n",
    "            self.attention = attn.detach().cpu()\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in [self.to_q.weight, self.to_kv.weight, self.to_out.weight]]\n",
    "        if getattr(self.to_q, 'bias', None) is not None: nn.init.constant_(self.to_q.bias, 0)\n",
    "        if getattr(self.to_kv, 'bias', None) is not None: nn.init.constant_(self.to_kv.bias, 0)\n",
    "        nn.init.constant_(self.to_out.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdditiveInProj(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        b, _, d = x.size()\n",
    "        context = ifnone(context, torch.empty(b, 0, d, dtype=x.dtype, device=x.device))\n",
    "        kv_input = torch.cat([x, context], dim=-2)\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_k(kv_input), self.to_v(kv_input)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "proj = AdditiveInProj(d)\n",
    "q1, k1, v1 = proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 240, 64]), torch.Size([4, 240, 64]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2, k2, v2 = proj(x, context)\n",
    "assert (bs, sl, d) == q2.size()\n",
    "assert k2.size() == v2.size() == (bs, x.size(1)+context.size(1), d)\n",
    "assert all_equal(q1, q2)\n",
    "assert not all_equal(k1, k2)\n",
    "q2.shape, k2.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdditiveAttention(Attention):\n",
    "    \"\"\"\n",
    "    Additive attention module: \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=True,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, n_heads, bias')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        self.in_proj = AdditiveInProj(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "    \n",
    "    def _make_input_mask(self, mask, context_mask, context, device):\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            self_mask = q_mask[:, None, :, None] * q_mask[:, None, None, :]\n",
    "            if context.size(-2) != 0:\n",
    "                k_mask = default(context_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "                cross_mask = q_mask[:, None, :, None] * k_mask[:, None, None, :]\n",
    "            else: cross_mask = torch.empty(0, dtype=self_mask.dtype, device=device)\n",
    "            return torch.cat([self_mask, cross_mask], dim=-1)\n",
    "        else: return None #input_mask is None if both mask and context_mask are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "attn = AdditiveAttention(d)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared QK Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SharedAttnInProj(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "#         self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_qk = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        context = ifnone(context, x)\n",
    "        qk = self.to_qk(x)\n",
    "#         k = q\n",
    "#         k = self.to_q(x)\n",
    "        v = self.to_v(context)\n",
    "#         return q, k, v\n",
    "        return qk, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedQKAttention(Module):\n",
    "    \"\"\"\n",
    "    Standard attention module using scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=True,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, mask, n_heads, bias')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        self.in_proj = SharedAttnInProj(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "#         q, k, v = self.in_proj(x, context)\n",
    "        q, v = self.in_proj(x, context)\n",
    "        \n",
    "        k = F.normalize(q, 2, dim=-1).type_as(q)\n",
    "        \n",
    "        input_mask = self._make_input_mask(mask, context_mask, context, x.device)\n",
    "        \n",
    "        b, n, d, h = *x.shape, self.n_heads\n",
    "        \n",
    "        mask = torch.zeros(b, h, n, n).bool()    \n",
    "        m = torch.arange(n)\n",
    "        mask[:, :, m, m] = True\n",
    "    \n",
    "        if exists(input_mask): final_mask = input_mask + self_mask \n",
    "        else: final_mask = self_mask\n",
    "        \n",
    "        out = self.attn(q, k, v, input_mask)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]\n",
    "    \n",
    "    def _make_input_mask(self, mask, context_mask, context, device):\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
    "            \n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            return q_mask * k_mask\n",
    "        else: return None #input_mask is None if both mask and context_mask are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "attn = SharedQKAttention(d)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(q, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand((4,8))\n",
    "\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_heads), (q, k, v))\n",
    "\n",
    "# classic dot-product attention\n",
    "dots = torch.einsum('bhid,bhjd->bhij', q*self.scale, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 8]),\n",
       " tensor([[0.6418, 0.9309, 0.0875, 0.4311, 0.7610, 0.5909, 0.5121, 0.8897]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[:, None].size(), q[:, None][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 8]),\n",
       " tensor([[0.6418, 0.9309, 0.0875, 0.4311, 0.7610, 0.5909, 0.5121, 0.8897],\n",
       "         [0.8414, 0.9780, 0.0447, 0.7468, 0.6824, 0.6221, 0.3255, 0.1995],\n",
       "         [0.5803, 0.7266, 0.5935, 0.8492, 0.6448, 0.8659, 0.1144, 0.8912],\n",
       "         [0.5869, 0.2278, 0.8895, 0.7219, 0.8223, 0.7393, 0.4513, 0.1532]]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[None, :].size(), k[None, :][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[:, None][0] == k[None, :][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 12, 12])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.rand((4,4,12))\n",
    "k = q\n",
    "\n",
    "self_mask = q[:, :, :, None] == k[:, :, None, :]\n",
    "self_mask.shape      # [bs, n_chunks, chunk_size, chunk_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 12]), torch.Size([4, 4, 12]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size(), k.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6686, 0.8243, 0.4995, 0.7482, 0.0835, 0.0371, 0.5566, 0.4160, 0.2992,\n",
       "         0.4209]),\n",
       " tensor([0.6686, 0.8243, 0.4995, 0.7482, 0.0835, 0.0371, 0.5566, 0.4160, 0.2992,\n",
       "         0.4209]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[:, :, :, None][0,0,:10,0], k[:, :, None, :][0,0,0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False],\n",
       "        [False,  True, False, False, False],\n",
       "        [False, False,  True, False, False],\n",
       "        [False, False, False,  True, False],\n",
       "        [False, False, False, False,  True]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask[0,0,:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 16])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, n, d, h = 4,5,2,8\n",
    "\n",
    "q = torch.rand((b,n,d*h))\n",
    "k = q\n",
    "v = torch.rand((b,n,d*h))\n",
    "\n",
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 5, 5])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm, km, vm, = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "#q, k, v, = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=8), (q, k, v))\n",
    "\n",
    "# classic dot-product attention\n",
    "# dots = torch.einsum('bhid,bhjd->bhij', q*0.01, k)\n",
    "dots = torch.einsum('bhid,bhjd->bhij', qm*0.01, km)\n",
    "dots.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 5, 5])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm, km, vm, = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "dots = torch.einsum('bhid,bhjd->bhij', qm*0.01, km)\n",
    "dots.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf, 0.0048, 0.0117, 0.0098, 0.0084],\n",
       "        [0.0048,   -inf, 0.0046, 0.0042, 0.0033],\n",
       "        [0.0117, 0.0046,   -inf, 0.0091, 0.0081],\n",
       "        [0.0098, 0.0042, 0.0091,   -inf, 0.0063],\n",
       "        [0.0084, 0.0033, 0.0081, 0.0063,   -inf]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.arange(n)\n",
    "dots[:, :, i, i] = -float('inf')\n",
    "dots[0,0,:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf, 0.0048, 0.0117, 0.0098, 0.0084],\n",
       "        [0.0048,   -inf, 0.0046, 0.0042, 0.0033],\n",
       "        [0.0117, 0.0046,   -inf, 0.0091, 0.0081],\n",
       "        [0.0098, 0.0042, 0.0091,   -inf, 0.0063],\n",
       "        [0.0084, 0.0033, 0.0081, 0.0063,   -inf]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 5, 16]), torch.Size([4, 5, 16]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size(), k.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 5, 16, 16]), tensor([[ True, False, False, False, False],\n",
       "         [False,  True, False, False, False],\n",
       "         [False, False,  True, False, False],\n",
       "         [False, False, False,  True, False],\n",
       "         [False, False, False, False,  True]]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask = q[:, :, :, None] == k[:, :, None, :]\n",
    "self_mask.size(), self_mask[0,0,:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 1, 5, 2]), torch.Size([4, 1, 8, 5, 2]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm[:, :, None].size(), km[:, None, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 8, 5, 2]), tensor([[[ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True]],\n",
       " \n",
       "         [[False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False]],\n",
       " \n",
       "         [[False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False]],\n",
       " \n",
       "         [[False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False]],\n",
       " \n",
       "         [[False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False]]]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mself_mask = qm[:, :, None] == km[:, None, :]\n",
    "mself_mask.size(), mself_mask[0,0,:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 5, 16]), torch.Size([4, 5, 16, 1]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size(), q[:, :, :, None].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mself_mask = rearrange(mself_mask, 'b n (h d) -> b h n d', h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 4, 4]), torch.Size([4, 8, 4, 1, 4]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size(), q[:, :, :, None].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 4, 4]), torch.Size([4, 8, 1, 4, 4]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.size(), k[:, :, None, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 4, 1, 4]),\n",
       " torch.Size([4, 8, 1, 4, 4]),\n",
       " torch.Size([4, 8, 4, 4, 4]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask = q[:, :, :, None] == k[:, :, None, :]\n",
    "self_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8, 4, 4])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask = q[:, :, None] == k[:, None, :]\n",
    "self_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 4, 4, 4])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask = torch.ne(q.unsqueeze(-1), k.unsqueeze(-2))\n",
    "self_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 4, 4, 4]), tensor([[[ True,  True,  True,  True],\n",
       "          [False, False, False, False],\n",
       "          [False, False, False, False],\n",
       "          [False, False, False, False]],\n",
       " \n",
       "         [[False, False, False, False],\n",
       "          [ True,  True,  True,  True],\n",
       "          [False, False, False, False],\n",
       "          [False, False, False, False]],\n",
       " \n",
       "         [[False, False, False, False],\n",
       "          [False, False, False, False],\n",
       "          [ True,  True,  True,  True],\n",
       "          [False, False, False, False]],\n",
       " \n",
       "         [[False, False, False, False],\n",
       "          [False, False, False, False],\n",
       "          [False, False, False, False],\n",
       "          [ True,  True,  True,  True]]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask[0,0,:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]),\n",
       " torch.Size([4, 8]),\n",
       " tensor([[0.8887, 0.3320, 0.5010, 0.7032, 0.8009, 0.7979, 0.6450, 0.5553],\n",
       "         [0.0773, 0.2908, 0.9349, 0.5278, 0.2663, 0.2752, 0.1811, 0.1575],\n",
       "         [0.0947, 0.6816, 0.0076, 0.3070, 0.6063, 0.3572, 0.3927, 0.9506],\n",
       "         [0.6972, 0.4863, 0.6777, 0.8588, 0.3077, 0.9452, 0.4496, 0.4202]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size(), k.size(), q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]), tensor([[3.6479, 1.6419, 2.0818, 3.2484],\n",
       "         [1.6419, 1.4474, 0.8552, 1.7718],\n",
       "         [2.0818, 0.8552, 2.1208, 1.7664],\n",
       "         [3.2484, 1.7718, 1.7664, 3.2862]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dots = torch.matmul(q, k.transpose(1,0))\n",
    "dots.size(), dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]),\n",
       " tensor([[0.8887, 0.3320, 0.5010, 0.7032, 0.8009, 0.7979, 0.6450, 0.5553],\n",
       "         [0.0773, 0.2908, 0.9349, 0.5278, 0.2663, 0.2752, 0.1811, 0.1575],\n",
       "         [0.0947, 0.6816, 0.0076, 0.3070, 0.6063, 0.3572, 0.3927, 0.9506],\n",
       "         [0.6972, 0.4863, 0.6777, 0.8588, 0.3077, 0.9452, 0.4496, 0.4202]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size(), q #, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 8]),\n",
       " tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask.size(), self_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 8]),\n",
       " tensor([[[ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "          [False, False, False, False, False, False, False, False]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True]]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_mask.squeeze(0).size(), self_mask.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSH attention from Reformer: [The Efficient Transformer](https://arxiv.org/abs/2001.04451). Based on [lucidrains/reformer-pytorch](https://github.com/lucidrains/reformer-pytorch/), but simpliefied and refactored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSHAttention(Module):\n",
    "    \"\"\"\n",
    "    Additive attention module: \n",
    "    \"\"\"\n",
    "    def __init__( self,\n",
    "                  dropout = 0.,                       # attention matrix dropout\n",
    "                  bucket_size = 64,                   # at least 64 suggested in trax\n",
    "                  n_hashes = 8,                       # papers sugests 8\n",
    "                  causal = False,\n",
    "                  allow_duplicate_attention = False,  # as in the paper\n",
    "                  attend_across_buckets = False,      # as in the paper\n",
    "                  drop_for_hash_rate = 0.0,           # unsure of default, not mentioned in paper\n",
    "                  return_attn = False,\n",
    "                  **kwargs):\n",
    "        \n",
    "        if dropout >= 1.0 or drop_for_hash_rate >=1.0:\n",
    "            raise ValueError('Dropout rates must be lower than 1.')\n",
    "        \n",
    "        store_attr(but=['dropout', 'drop_for_hash_rate'])  # fastcore - store attibutes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n",
    "        self._cache = {} # cache buckets for reversible network, required to make Reformer work at depth\n",
    "\n",
    "    @cache_method_decorator('_cache', 'buckets', reexecute=True)\n",
    "    def hash_vectors(self, n_buckets, vecs):\n",
    "        # 0. We need an even number of buckets: \n",
    "        assert n_buckets % 2 == 0\n",
    "\n",
    "        # 1. account for the input shapes. vecs = [bs, sl, dim]\n",
    "        batch_size, seqlen, dim = vecs.shape\n",
    "        device = vecs.device\n",
    "        #print(device)\n",
    "        rotations_shape = (dim, self.n_hashes, n_buckets // 2)\n",
    "\n",
    "        # 2. Calculate hash bucket id via random rotations, concatenation and argmax \n",
    "        # note: we copy rotations accross batch dimension (see exploration notebook for details). \n",
    "        random_rotations = repeat(torch.randn(rotations_shape,device=device), \n",
    "                                  'd nh nb -> bs d nh nb', bs=batch_size)           \n",
    "        dropped_vecs = self.dropout_for_hash(vecs)\n",
    "                       \n",
    "        rotated_vecs = torch.einsum('bsd,bdhn->bhsn', \n",
    "                                    dropped_vecs,       # [bs, sl, dim]\n",
    "                                    random_rotations)   # [bs, dim, n_hashes, n_buckets//2]\n",
    "                                                        # rotated vecs: [bs, n_hashes, sl, n_buckets//2]\n",
    "\n",
    "        rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1) # [bs, n_hashes, sl, n_buckets]\n",
    "        buckets = torch.argmax(rotated_vecs, dim=-1)                    # [bs, n_hashes, sl] \n",
    "\n",
    "        # 3. Next we add offsets so that bucket numbers from different hashing rounds don't overlap.\n",
    "        # We also reshape the buckets so that each hash round is concatenated along the -1 dim\n",
    "        offsets = torch.arange(self.n_hashes,device=device)                              # list of [0,1,2,..n_hashes-1]\n",
    "        offsets = rearrange(offsets * n_buckets, 'nh -> 1 nh 1')        # [1, n_hashes, 1]\n",
    "        buckets = rearrange(buckets+offsets, 'bs nh sl -> bs (nh sl)')  # [bs, (n_hashes*sl)]\n",
    "        return buckets\n",
    "\n",
    "    def forward(self, qk, v, input_mask = None, **kwargs):\n",
    "        batch_size, seqlen, dim, device = *qk.shape, qk.device\n",
    "        #print(qk.device)\n",
    "\n",
    "        # caching\n",
    "        is_reverse = kwargs.pop('_reverse', False)\n",
    "        depth = kwargs.pop('_depth', None)\n",
    "        \n",
    "        # We will have an even number of buckets, and our attention chunks needs to fit completely within a seqlen\n",
    "        assert seqlen % (self.bucket_size * 2) == 0, f'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}'\n",
    "        \n",
    "        # get the hash buckets for our qk input vectors\n",
    "        n_buckets = seqlen // self.bucket_size\n",
    "        buckets = self.hash_vectors(n_buckets, qk, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n",
    "\n",
    "        # We use the same vector as both a query and a key.\n",
    "        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n",
    "        \n",
    "        # Create an index that reflexts both bucket id and sequence id. This let's us sort qk according \n",
    "        # to both simultaneously. Repeated across the batch dimension.\n",
    "        ticker = repeat(torch.arange((self.n_hashes * seqlen),device=device), 'l -> bs l', bs=batch_size)\n",
    "        buckets_and_t = seqlen * buckets + (ticker % seqlen) \n",
    "        buckets_and_t = buckets_and_t.detach()                # [bs, seqlen*n_hashes]\n",
    "\n",
    "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)  # [bs, seqlen*n_hashes]\n",
    "        _, undo_sort = sticker.sort(dim=-1)                                    # indexes to undo sortings\n",
    "        del ticker\n",
    "\n",
    "        sbuckets_and_t = sbuckets_and_t.detach()   # no need to store gradiens for indexes\n",
    "        sticker = sticker.detach()\n",
    "        undo_sort = undo_sort.detach()\n",
    "\n",
    "        st = (sticker % seqlen)             # index of [0..seqlen-1] for each hash round\n",
    "        sqk = batched_index_select(qk, st)  # get the sorted qk, [bs, seqlen*n_hashes, dim]\n",
    "        sv = batched_index_select(v, st)    # get the sorted v, [bs, seqlen*n_hashes, dim] \n",
    "\n",
    "        # Reshape to include a n_chunks axis.\n",
    "        n_chunks = self.n_hashes * n_buckets\n",
    "        bq_t = bkv_t = rearrange(st, 'bs (n s) -> bs n s', n=n_chunks) # [bs, n_chunks, chunk_size]\n",
    "        bqk = rearrange(sqk, 'bs (n s) d -> bs n s d', n=n_chunks)     # [bs, n_chunks, chunk_size, dim]\n",
    "        bv = rearrange(sv, 'bs (n s) d -> bs n s d', n=n_chunks)       # [bs, n_chunks, chunk_size, dim]\n",
    "\n",
    "        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n",
    "        # fine because they effectively provide a learnable temperature for the\n",
    "        # attention softmax, but normalizing keys is needed so that similarity for\n",
    "        # the purposes of attention correctly corresponds to hash locality.\n",
    "        bq = bqk\n",
    "        bk = F.normalize(bqk, p=2, dim=-1).type_as(bq)\n",
    "\n",
    "        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
    "        # boundaries might occur in the middle of a sequence of items from the\n",
    "        # same bucket, so this increases the chances of attending to relevant items.\n",
    "        # Note: no look_back for queries\n",
    "\n",
    "        bk = look_one_back(bk)        # [bs, n_chunks, chunk_size*2, dim]\n",
    "        bv = look_one_back(bv)        # [bs, n_chunks, chunk_size*2, dim]\n",
    "        bkv_t = look_one_back(bkv_t)\n",
    "\n",
    "        # Dot-product attention.\n",
    "        dots = torch.einsum('bnsd,bnzd->bnsz', \n",
    "                    bq,                  # [bs, n_chunks, chunk_size, dim]\n",
    "                    bk                   # [bs, n_chunks, chunk_size*2, dim]\n",
    "                   ) * (dim ** -0.5)     # dots: [bs, n_chunks, chunk_size, chunk_size*2]\n",
    "        masked_value = max_neg_value(dots)\n",
    "\n",
    "        # Input mask for padding in variable lengthed sequences\n",
    "        if input_mask is not None:\n",
    "            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n",
    "            mq = input_mask.gather(1, st).reshape((batch_size, n_chunks, -1))\n",
    "            mkv = look_one_back(mq)\n",
    "            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n",
    "            dots.masked_fill_(~mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Causal masking\n",
    "        if self.causal:\n",
    "            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n",
    "            dots.masked_fill_(mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Mask out attention to self except when no other targets are available.\n",
    "        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n",
    "        dots.masked_fill_(self_mask, SELF_ATTN_MASK_VAL)\n",
    "        del self_mask\n",
    "\n",
    "        # Mask out attention to other hash buckets.\n",
    "        if not self.attend_across_buckets:\n",
    "            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, n_chunks, -1))\n",
    "            bkv_buckets = look_one_back(bkv_buckets)\n",
    "            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n",
    "            dots.masked_fill_(bucket_mask, masked_value)\n",
    "            del bucket_mask\n",
    "\n",
    "        # Don't double-count query-key pairs across multiple rounds of hashing.\n",
    "        # There are two possible strategies here. (1) The default is to count how\n",
    "        # many times a query-key pair is repeated, and to lower its log-prob\n",
    "        # correspondingly at each repetition.\n",
    "        \n",
    "        if not self.allow_duplicate_attention:\n",
    "            locs1 = undo_sort // bq_t.shape[-1]\n",
    "            locs2 = (locs1 + 1) % n_chunks\n",
    "            if not self.attend_across_buckets:\n",
    "                locs1 = buckets * n_chunks + locs1\n",
    "                locs2 = buckets * n_chunks + locs2\n",
    "            locs = torch.cat([\n",
    "                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n",
    "                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n",
    "            ], 1).permute((0, 2, 1))\n",
    "\n",
    "            slocs = batched_index_select(locs, st)\n",
    "            b_locs = torch.reshape(slocs, (batch_size, n_chunks, -1, 2 * self.n_hashes))\n",
    "\n",
    "            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n",
    "\n",
    "            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n",
    "            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n",
    "            bkv_locs = look_one_back(b_locs)\n",
    "\n",
    "            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n",
    "            # for memory considerations, chunk summation of last dimension for counting duplicates\n",
    "            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n",
    "            dup_counts = dup_counts.detach()\n",
    "            assert dup_counts.shape == dots.shape\n",
    "            dots = dots - torch.log(dup_counts + 1e-9)\n",
    "            del dup_counts\n",
    "\n",
    "        # Softmax.\n",
    "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
    "        dots = torch.exp(dots - dots_logsumexp).type_as(dots)\n",
    "        dropped_dots = self.dropout(dots)\n",
    "        \n",
    "        # calculate self-attention (attn * values)\n",
    "        bo = torch.einsum('bnsz,bnzd->bnsd', \n",
    "                          dropped_dots,      # [bs, n_chunks, chunk_size, chunk_size*2]\n",
    "                          bv)                # [bs, n_chunks, chunk_size*2, dim]    \n",
    "                                             # bo: [bs, n_chunks, chunk_size, dim]\n",
    "        \n",
    "        # unchunk, unsort and reshape self-attention\n",
    "        so = rearrange(bo, 'b n s d -> b (n s) d')                     # [bs, seqlen*n_hashes, dim]\n",
    "        o = batched_index_select(so, undo_sort)                        # [bs, seqlen*n_hashes, dim]\n",
    "        o = rearrange(o, 'b (nh sl) d -> b nh sl d', nh=self.n_hashes) # [bs, n_hashes, seqlen, dim]\n",
    "        \n",
    "        # unchunk, unsort and reshape logits\n",
    "        slogits = rearrange(dots_logsumexp, 'bs n s 1 -> bs (n s)')              # [bs, seqlen*n_hashes]\n",
    "        logits = slogits.gather(1, undo_sort)                                    # [bs, seqlen*n_hashes]\n",
    "        logits = rearrange(logits, 'bs (nr sl) -> bs nr sl 1', nr=self.n_hashes) # [bs, n_hashes, seqlen, 1]\n",
    "        \n",
    "        # average probabilites across hash rounds (dim 1) and get weighted attention\n",
    "        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True)) # [bs, n_rounds, seqlen, 1]\n",
    "        out = torch.sum(o * probs, dim=1)                                        # [bs, seqlen, dim]\n",
    "\n",
    "        # return unsorted attention weights - empty otherwise\n",
    "        attn = torch.empty(0, device=device)\n",
    "        if self.return_attn:\n",
    "            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n",
    "            attn_unsort = attn_unsort.view(batch_size * self.n_hashes, -1).long()\n",
    "            unsorted_dots = torch.zeros(batch_size * self.n_hashes, seqlen * seqlen, device=device)\n",
    "            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n",
    "            del attn_unsort\n",
    "            unsorted_dots = unsorted_dots.reshape(batch_size, self.n_hashes, seqlen, seqlen)\n",
    "            attn = torch.sum(unsorted_dots * probs, dim=1)\n",
    "\n",
    "        # return output, attention matrix, and bucket distribution\n",
    "        return out, attn, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test attention layer. Note: `d_model` is infered from input. Assumes shared key and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk, v = torch.randn(bs, sl, d), torch.randn(bs, sl, d)\n",
    "lsh_attn = LSHAttention()\n",
    "out, _, _ = lsh_attn(qk, v)\n",
    "assert (bs, sl, d) == out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformer Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: proto to be implemented...\n",
    "class ReformerAttention(Module):\n",
    "    \"\"\"\n",
    "    Reformer attnetion container\n",
    "    \n",
    "    Switch between FullSharedQKAttention and LSHAttention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=True,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, mask, n_heads, bias')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "#         self.in_proj = AttnInProj(d_model, bias=bias)\n",
    "#         self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "#                                            dropout=dropout, store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        q, k, v = self.in_proj(x, context)\n",
    "        \n",
    "        out = self.attn(q, k, v, mask, context_mask)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
