{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "\n",
    "from reformer_fastai.core import *\n",
    "from reformer_fastai.transformer import *\n",
    "from reformer_fastai.reformer import *\n",
    "\n",
    "import json\n",
    "from inspect import signature, Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _dummy(): return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_sig(d):\n",
    "    \"Update signature of `f` from dict `d`\"\n",
    "    d = {k:Parameter(k, Parameter.KEYWORD_ONLY, default=v) for k,v in d.items()}\n",
    "    def _f(f):\n",
    "        sig = signature(f)\n",
    "        sigd = dict(sig.parameters)\n",
    "        sigd.pop('kwargs')\n",
    "        sigd.update(d)\n",
    "        f.__signature__ = sig.replace(parameters=sigd.values())\n",
    "        return f\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConfigBase:\n",
    "    \"Base class for Configs\"\n",
    "    _d:dict = None\n",
    "    _model = _dummy\n",
    "    \n",
    "    def __init__(self, *, verbose=False, warn=True, **kwargs):\n",
    "        self.validate()\n",
    "        for k,v in kwargs.items():\n",
    "            if k in self._d:\n",
    "                self._d[k]=v\n",
    "                if verbose: print(f'Setting `{k}` = {v}')\n",
    "            elif warn: print(f'Parameter `{k}` is not accepted by {self._model.__name__}. Skipped')\n",
    "    \n",
    "    def validate(self):\n",
    "        assert exists(self._d), \"_d missing. You might want to provide defaults for config\"\n",
    "        assert self._model is not _dummy, \"_model missing. Provide a model class\"\n",
    "    \n",
    "    def validate_arg(self, k):\n",
    "        assert k in self._d.keys(), f\"{self._model.__name__} does not accept `{k}` argument\"\n",
    "        \n",
    "    def __getattr__(self, k):\n",
    "        try:\n",
    "            res = self._d[k]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"{type(self).__name__} does not have attribute `{k}`\")\n",
    "        return res\n",
    "    \n",
    "    def __setattr__(self, k, v):\n",
    "        self.validate_arg(k)\n",
    "        self._d[k] = v\n",
    "    \n",
    "    def __getitem__(self, k):\n",
    "        return self._d[k]\n",
    "    \n",
    "    def __setitem__(self, k, v):\n",
    "        self.validate_arg(k)\n",
    "        self._d[k] = v\n",
    "        \n",
    "    def __repr__(self):\n",
    "        s = f\"{self._model.__name__} config \\n\" + '-'*20\n",
    "        s += ''.join(f'\\n{k:16}{v}' for k,v in self._d.items())\n",
    "        return s\n",
    "    \n",
    "    def dict(self): return self._d\n",
    "    \n",
    "    def save(self, fn, add_tstmp=False):\n",
    "        os.makedirs('exp_configs', exist_ok=True)\n",
    "        if add_tstmp:\n",
    "            tstmp = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            fn += tstmp\n",
    "        with open(f'exp_configs/{fn}.json', 'w') as f:\n",
    "            json.dump(self.dict(), f)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, fn):\n",
    "        with open(f'exp_configs/{fn}.json') as f:\n",
    "            d = json.load(f)\n",
    "        return cls(**d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SyntheticConfig(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for Synthetic Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.synthetic-task.html for details\n",
    "    \"\"\"\n",
    "    _model = LSHLM\n",
    "    _d = {\n",
    "        'vocab_sz':128,\n",
    "        'd_model':256,\n",
    "        'n_layers':1,\n",
    "        'n_heads':4,\n",
    "        'd_ff':256,\n",
    "        'attn_dropout':0.0,\n",
    "        'ff_dropout':0.0,\n",
    "        'emb_dropout':0.0,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'absolute',\n",
    "        'max_seq_len':1024,\n",
    "        'axial_shape':None,\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'bucket_size':64,\n",
    "        'use_lsh':True,\n",
    "        'n_hashes':4,\n",
    "        'seed':123,\n",
    "    }\n",
    "    @update_sig(_d)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting `n_hashes` = 8\n",
      "Setting `seed` = 1\n",
      "Parameter `foo` is not accepted by LSHLM. Skipped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSHLM config \n",
       "--------------------\n",
       "vocab_sz        128\n",
       "d_model         256\n",
       "n_layers        1\n",
       "n_heads         4\n",
       "d_ff            256\n",
       "attn_dropout    0.0\n",
       "ff_dropout      0.0\n",
       "emb_dropout     0.0\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         absolute\n",
       "max_seq_len     1024\n",
       "axial_shape     None\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "bucket_size     64\n",
       "use_lsh         True\n",
       "n_hashes        8\n",
       "seed            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synt_config = SyntheticConfig(n_hashes=8, seed=1, foo=1, verbose=True)\n",
    "synt_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "synt_config.save('_tmp_synt')\n",
    "synt_config2 = SyntheticConfig.from_file('_tmp_synt')\n",
    "assert synt_config.dict()==synt_config2.dict(), 'Loading saved config failed' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accesing config params as attributes\n",
    "synt_config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSHLM does not accept `foo` argument\n"
     ]
    }
   ],
   "source": [
    "#setting config params as attributes\n",
    "synt_config.n_hashes = 4\n",
    "assert synt_config._d['n_hashes'] == 4\n",
    "try: synt_config.foo = 1\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accesing config params as items\n",
    "synt_config['n_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSHLM does not accept `foo` argument\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#setting config params as items\n",
    "synt_config['n_hashes'] = 1\n",
    "assert synt_config._d['n_hashes'] == 1\n",
    "try: synt_config['foo'] = 1\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticConfig does not have attribute `foo`\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#skip\n",
    "try: synt_config.foo\n",
    "except AttributeError as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Config for envik8 needs to be updated when we diside on sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerLMConfigEnwik8(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for enwik8 Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-baseline.html for details\n",
    "    \"\"\"\n",
    "    _model = TransformerLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':1024,\n",
    "        'n_layers':3,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'axial',\n",
    "        'max_seq_len':2048,\n",
    "        'axial_shape':(64,32),\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'shared_qk':False,\n",
    "    }\n",
    "    @update_sig(_d)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM config \n",
       "--------------------\n",
       "vocab_sz        256\n",
       "d_model         1024\n",
       "n_layers        3\n",
       "n_heads         8\n",
       "d_ff            4096\n",
       "attn_dropout    0.1\n",
       "ff_dropout      0.1\n",
       "emb_dropout     0.1\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         axial\n",
       "max_seq_len     2048\n",
       "axial_shape     (64, 32)\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "shared_qk       False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerLMConfigEnwik8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM(\n",
       "  (emb): TransformerEmbedding(\n",
       "    (emb): Embedding(256, 1024)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (pos_enc): AxialPositionalEmbedding()\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderBlock(\n",
       "        (attn): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerEncoderBlock(\n",
       "        (attn): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerEncoderBlock(\n",
       "        (attn): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "#hide\n",
    "TransformerLM.from_config(TransformerLMConfigEnwik8())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReversibleLMConfigEnwik8(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for enwik8 Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-reversible.html for details\n",
    "    \"\"\"\n",
    "    _model = ReversibleLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':1024,\n",
    "        'n_layers':3,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'axial',\n",
    "        'max_seq_len':2048,\n",
    "        'axial_shape':(64,32),\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'rev_thres':0,\n",
    "    }\n",
    "    @update_sig(_d)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReversibleLM config \n",
       "--------------------\n",
       "vocab_sz        256\n",
       "d_model         1024\n",
       "n_layers        3\n",
       "n_heads         8\n",
       "d_ff            4096\n",
       "attn_dropout    0.1\n",
       "ff_dropout      0.1\n",
       "emb_dropout     0.1\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         axial\n",
       "max_seq_len     2048\n",
       "axial_shape     (64, 32)\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "rev_thres       128"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReversibleLMConfigEnwik8(rev_thres=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReversibleLM(\n",
       "  (emb): TransformerEmbedding(\n",
       "    (emb): Embedding(256, 1024)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (pos_enc): AxialPositionalEmbedding()\n",
       "  )\n",
       "  (encoder): ReversibleEncoder(\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ReversibleSequence(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): Attention(\n",
       "                (in_proj): AttnInProjV2(\n",
       "                  (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                )\n",
       "                (attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): Attention(\n",
       "                (in_proj): AttnInProjV2(\n",
       "                  (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                )\n",
       "                (attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): Attention(\n",
       "                (in_proj): AttnInProjV2(\n",
       "                  (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                )\n",
       "                (attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (irrev_blocks): ModuleList(\n",
       "        (0): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "#hide\n",
    "ReversibleLM.from_config(ReversibleLMConfigEnwik8())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NHashesConfig(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for evaluating performance as function of `n_hashes`.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-n_hashes.html for details\n",
    "    \"\"\"\n",
    "    _model = LSHLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':1024,\n",
    "        'n_layers':3,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'axial',\n",
    "        'max_seq_len':4096,\n",
    "        'axial_shape':None,\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'bucket_size':64,\n",
    "        'use_lsh':True,\n",
    "        'n_hashes':2,\n",
    "        'seed':842,\n",
    "    }\n",
    "    @update_sig(_d)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSHLM config \n",
       "--------------------\n",
       "vocab_sz        256\n",
       "d_model         1024\n",
       "n_layers        3\n",
       "n_heads         8\n",
       "d_ff            4096\n",
       "attn_dropout    0.1\n",
       "ff_dropout      0.1\n",
       "emb_dropout     0.1\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         axial\n",
       "max_seq_len     4096\n",
       "axial_shape     None\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "bucket_size     64\n",
       "use_lsh         True\n",
       "n_hashes        2\n",
       "seed            842"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = NHashesConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NLayersConfig(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for evaluating performance as function of `n_layers`.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-n_layers.html for details\n",
    "    \"\"\"\n",
    "    _model = ReformerLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':1024,\n",
    "        'n_layers':3,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'ff_chunks':64,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'axial',\n",
    "        'max_seq_len':2**14,\n",
    "        'axial_shape':None,\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'bucket_size':64,\n",
    "        'use_lsh':True,\n",
    "        'n_hashes':8,\n",
    "        'rev_thres':0,\n",
    "        'seed':842,\n",
    "    }\n",
    "    @update_sig(_d)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerLM config \n",
       "--------------------\n",
       "vocab_sz        256\n",
       "d_model         1024\n",
       "n_layers        3\n",
       "n_heads         8\n",
       "d_ff            4096\n",
       "ff_chunks       64\n",
       "attn_dropout    0.1\n",
       "ff_dropout      0.1\n",
       "emb_dropout     0.1\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         axial\n",
       "max_seq_len     32768\n",
       "axial_shape     None\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "bucket_size     64\n",
       "use_lsh         True\n",
       "n_hashes        8\n",
       "rev_thres       0\n",
       "seed            842"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = NLayersConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerLM(\n",
       "  (emb): TransformerEmbedding(\n",
       "    (emb): Embedding(256, 1024)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (pos_enc): AxialPositionalEmbedding()\n",
       "  )\n",
       "  (encoder): ReformerEncoder(\n",
       "    (layers): ReversibleSequence(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ReformerAttentionV2(\n",
       "                (in_proj): SharedQKAttnInProj(\n",
       "                  (to_qk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ReformerAttentionV2(\n",
       "                (in_proj): SharedQKAttnInProj(\n",
       "                  (to_qk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ReformerAttentionV2(\n",
       "                (in_proj): SharedQKAttnInProj(\n",
       "                  (to_qk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (irrev_blocks): ModuleList(\n",
       "        (0): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): ReformerAttentionV2(\n",
       "              (in_proj): SharedQKAttnInProj(\n",
       "                (to_qk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lsh_attn): LSHAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (full_attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): ReformerAttentionV2(\n",
       "              (in_proj): SharedQKAttnInProj(\n",
       "                (to_qk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lsh_attn): LSHAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (full_attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): ReformerAttentionV2(\n",
       "              (in_proj): SharedQKAttnInProj(\n",
       "                (to_qk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lsh_attn): LSHAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (full_attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "ReformerLM.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (*, vocab_sz=128, d_model=256, n_layers=1, n_heads=4, d_ff=256, attn_dropout=0.0, ff_dropout=0.0, emb_dropout=0.0, tie_weights=True, causal=True, pos_enc='absolute', max_seq_len=1024, axial_shape=None, axial_emb_dims=None, pad_idx=None, prenorm=False, attn_bias=False, bucket_size=64, use_lsh=True, n_hashes=4, seed=123)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "#hide\n",
    "signature(SyntheticConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 10c_experiment.synthetic-task-analysis.ipynb.\n",
      "Converted 11a_experiment.enwik8_baseline.ipynb.\n",
      "Converted 11b_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 11c_experiment.enwik8_reversible.ipynb.\n",
      "Converted 12_experiment.speed-lsh_synthetic-task.ipynb.\n",
      "Converted 13_experiment.enwik8-n_hashes.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
