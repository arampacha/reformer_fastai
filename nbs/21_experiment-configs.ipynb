{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of reformer_fastai.reformer failed: Traceback (most recent call last):\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "RecursionError: maximum recursion depth exceeded while calling a Python object\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# default_exp configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "\n",
    "from reformer_fastai.all import *\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _dummy(): return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConfigBase:\n",
    "    \"Base class for Configs\"\n",
    "    _d:dict = None\n",
    "    _model = _dummy\n",
    "    \n",
    "    @delegates(_model)\n",
    "    def __init__(self, verbouse=False, warn=True, **kwargs):\n",
    "        self.validate()\n",
    "        for k,v in kwargs.items():\n",
    "            if k in self._d:\n",
    "                self._d[k]=v\n",
    "                if verbouse: print(f'Setting {k} = {v}')\n",
    "            elif warn: print(f'Parameter {k} is not accepted by LSHLM. Skipped')\n",
    "    \n",
    "    def validate(self):\n",
    "        assert exists(self._d), \"_d missing. You might want to provide defaults for config\"\n",
    "        assert self._model is not _dummy, \"_model missing. Provide a model class\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f\"{self._model.__name__} config \\n\" + '-'*20\n",
    "        s += ''.join(f'\\n{k:16}{v}' for k,v in self._d.items())\n",
    "        return s\n",
    "    \n",
    "    def dict(self): return self._d\n",
    "    \n",
    "    def save(self, fn, add_tstmp=False):\n",
    "        os.makedirs('exp_configs', exist_ok=True)\n",
    "        if add_tstmp:\n",
    "            tstmp = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            fn += tstmp\n",
    "        with open(f'exp_configs/{fn}.json', 'w') as f:\n",
    "            json.dump(self.dict(), f)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, fn):\n",
    "        with open(f'exp_configs/{fn}.json') as f:\n",
    "            d = json.load(f)\n",
    "        return cls(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SyntheticConfig(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for Synthetic Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.synthetic-task.html for details\n",
    "    \"\"\"\n",
    "    _model = LSHLM\n",
    "    _d = {\n",
    "        'vocab_sz':128,\n",
    "        'd_model':256,\n",
    "        'n_layers':1,\n",
    "        'n_heads':4,\n",
    "        'd_ff':256,\n",
    "        'attn_dropout':0.0,\n",
    "        'ff_dropout':0.0,\n",
    "        'emb_dropout':0.0,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'absolute',\n",
    "        'max_seq_len':1024,\n",
    "        'axial_shape':None,\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'bucket_size':64,\n",
    "        'use_lsh':True,\n",
    "        'n_hashes':4,\n",
    "        'seed':123,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting n_hashes = 8\n",
      "Setting seed = 1\n",
      "Parameter foo is not accepted by LSHLM. Skipped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSHLM config \n",
       "--------------------\n",
       "vocab_sz        128\n",
       "d_model         256\n",
       "n_layers        1\n",
       "n_heads         4\n",
       "d_ff            256\n",
       "attn_dropout    0.0\n",
       "ff_dropout      0.0\n",
       "emb_dropout     0.0\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         absolute\n",
       "max_seq_len     1024\n",
       "axial_shape     None\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "bucket_size     64\n",
       "use_lsh         True\n",
       "n_hashes        8\n",
       "seed            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synt_config = SyntheticConfig(n_hashes=8, seed=1, foo=1, verbouse=True)\n",
    "synt_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "synt_config.save('_tmp_synt')\n",
    "synt_config2 = SyntheticConfig.from_file('_tmp_synt')\n",
    "assert synt_config.dict()==synt_config2.dict(), 'Loading saved config failed' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Config for envik8 needs to be updated when we diside on sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerLMConfigEnwik8(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for enwik8 Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-baseline.html for details\n",
    "    \"\"\"\n",
    "    _model = TransformerLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':1024,\n",
    "        'n_layers':3,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'axial',\n",
    "        'max_seq_len':2048,\n",
    "        'axial_shape':(64,32),\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'shared_qk':False,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM config \n",
       "--------------------\n",
       "vocab_sz        256\n",
       "d_model         1024\n",
       "n_layers        3\n",
       "n_heads         8\n",
       "d_ff            4096\n",
       "attn_dropout    0.1\n",
       "ff_dropout      0.1\n",
       "emb_dropout     0.1\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         axial\n",
       "max_seq_len     2048\n",
       "axial_shape     (64, 32)\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "shared_qk       False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerLMConfigEnwik8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM(\n",
       "  (emb): TransformerEmbedding(\n",
       "    (emb): Embedding(256, 1024)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (pos_enc): AxialPositionalEmbedding()\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderBlock(\n",
       "        (attn): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerEncoderBlock(\n",
       "        (attn): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerEncoderBlock(\n",
       "        (attn): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "#hide\n",
    "TransformerLM.from_config(TransformerLMConfigEnwik8())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReversibleLMConfigEnwik8(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for enwik8 Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-reversible.html for details\n",
    "    \"\"\"\n",
    "    _model = TransformerLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':1024,\n",
    "        'n_layers':3,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'axial',\n",
    "        'max_seq_len':2048,\n",
    "        'axial_shape':(64,32),\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'rev_thres':0,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM config \n",
       "--------------------\n",
       "vocab_sz        256\n",
       "d_model         1024\n",
       "n_layers        3\n",
       "n_heads         8\n",
       "d_ff            4096\n",
       "attn_dropout    0.1\n",
       "ff_dropout      0.1\n",
       "emb_dropout     0.1\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         axial\n",
       "max_seq_len     2048\n",
       "axial_shape     (64, 32)\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "rev_thres       128"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReversibleLMConfigEnwik8(rev_thres=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReversibleLM(\n",
       "  (emb): TransformerEmbedding(\n",
       "    (emb): Embedding(256, 1024)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (pos_enc): AxialPositionalEmbedding()\n",
       "  )\n",
       "  (encoder): ReversibleEncoder(\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ReversibleSequence(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): Attention(\n",
       "                (in_proj): AttnInProjV2(\n",
       "                  (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                )\n",
       "                (attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): Attention(\n",
       "                (in_proj): AttnInProjV2(\n",
       "                  (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                )\n",
       "                (attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): Attention(\n",
       "                (in_proj): AttnInProjV2(\n",
       "                  (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                )\n",
       "                (attn): ScaledDotProdAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PostNorm(\n",
       "              (sublayer): ChunkedFeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (1): GELU()\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (irrev_blocks): ModuleList(\n",
       "        (0): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): IrreversibleBlock(\n",
       "          (f): PostNorm(\n",
       "            (sublayer): Attention(\n",
       "              (in_proj): AttnInProjV2(\n",
       "                (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              )\n",
       "              (attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (g): PostNorm(\n",
       "            (sublayer): ChunkedFeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (4): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "#hide\n",
    "ReversibleLM.from_config(ReversibleLMConfigEnwik8())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 10c_experiment.synthetic-task-analysis.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n",
      "Converted tmp-22_experiment-generator.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
