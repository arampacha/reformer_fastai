{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "\n",
    "from reformer_fastai.all import *\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _dummy(): return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConfigBase:\n",
    "    \"Base class for Configs\"\n",
    "    _d:dict = None\n",
    "    _model = _dummy\n",
    "    \n",
    "    @delegates(_model)\n",
    "    def __init__(self, verbouse=False, warn=True, **kwargs):\n",
    "        self.validate()\n",
    "        for k,v in kwargs.items():\n",
    "            if k in self._d:\n",
    "                self._d[k]=v\n",
    "                if verbouse: print(f'Setting {k} = {v}')\n",
    "            elif warn: print(f'Parameter {k} is not accepted by LSHLM. Skipped')\n",
    "    \n",
    "    def validate(self):\n",
    "        assert exists(self._d), \"_d missing. You might want to provide defaults for config\"\n",
    "        assert self._model is not _dummy, \"_model missing. Provide a model class\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f\"{self._model.__name__} config \\n\" + '-'*20\n",
    "        s += ''.join(f'\\n{k:16}{v}' for k,v in self._d.items())\n",
    "        return s\n",
    "    \n",
    "    def dict(self): return self._d\n",
    "    \n",
    "    def save(self, fn, add_tstmp=False):\n",
    "        if add_tstmp:\n",
    "            tstmp = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            fn += tstmp\n",
    "        with open(f'{fn}.json', 'w') as f:\n",
    "            json.dump(self.dict(), f)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, fn):\n",
    "        with open(f'{fn}.json') as f:\n",
    "            d = json.load(f)\n",
    "        return cls(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SyntheticConfig(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for Synthetic Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.synthetic-task.html for details\n",
    "    \"\"\"\n",
    "    _model = LSHLM\n",
    "    _d = {\n",
    "        'vocab_sz':128,\n",
    "        'd_model':256,\n",
    "        'n_layers':1,\n",
    "        'n_heads':4,\n",
    "        'd_ff':256,\n",
    "        'attn_dropout':0.0,\n",
    "        'ff_dropout':0.0,\n",
    "        'emb_dropout':0.0,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'absolute',\n",
    "        'max_seq_len':1024,\n",
    "        'axial_shape':None,\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'bucket_size':64,\n",
    "        'use_lsh':True,\n",
    "        'n_hashes':4,\n",
    "        'seed':123,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting n_hashes = 8\n",
      "Setting seed = 1\n",
      "Parameter foo is not accepted by LSHLM. Skipped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSHLM config \n",
       "--------------------\n",
       "vocab_sz        128\n",
       "d_model         256\n",
       "n_layers        1\n",
       "n_heads         4\n",
       "d_ff            256\n",
       "attn_dropout    0.0\n",
       "ff_dropout      0.0\n",
       "emb_dropout     0.0\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         absolute\n",
       "max_seq_len     1024\n",
       "axial_shape     None\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "bucket_size     64\n",
       "use_lsh         True\n",
       "n_hashes        8\n",
       "seed            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synt_config = SyntheticConfig(n_hashes=8, seed=1, foo=1, verbouse=True)\n",
    "synt_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "synt_config.save('_tmp_synt')\n",
    "synt_config2 = SyntheticConfig.from_file('_tmp_synt')\n",
    "assert synt_config.dict()==synt_config2.dict(), 'Loading saved config failed' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Config for envik8 needs to be updated when we diside on sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerLMConfigEnwik8(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for enwik8 Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-baseline.html for details\n",
    "    \"\"\"\n",
    "    _model = TransformerLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':1024,\n",
    "        'n_layers':3,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'axial',\n",
    "        'max_seq_len':2048,\n",
    "        'axial_shape':(64,32),\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'shared_qk':False,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM config \n",
       "--------------------\n",
       "vocab_sz        256\n",
       "d_model         1024\n",
       "n_layers        3\n",
       "n_heads         8\n",
       "d_ff            4096\n",
       "attn_dropout    0.1\n",
       "ff_dropout      0.1\n",
       "emb_dropout     0.1\n",
       "tie_weights     True\n",
       "causal          True\n",
       "pos_enc         axial\n",
       "max_seq_len     2048\n",
       "axial_shape     (64, 32)\n",
       "axial_emb_dims  None\n",
       "pad_idx         None\n",
       "prenorm         False\n",
       "attn_bias       False\n",
       "shared_qk       False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerLMConfigEnwik8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_experiment.synthetic-task.ipynb.\n",
      "Converted 10a_experiment.synthetic-task-comparison.ipynb.\n",
      "Converted 10b_experiment.synthetic-task-minimal.ipynb.\n",
      "Converted 10c_experiment.synthetic-task-analysis.ipynb.\n",
      "Converted 11_experiment.enwik8_baseline.ipynb.\n",
      "Converted 12_experiment.enwik8_sharedQK.ipynb.\n",
      "Converted 13_experiment.enwik8_reversible.ipynb.\n",
      "Converted 20_experiment-script.ipynb.\n",
      "Converted 21_experiment-configs.ipynb.\n",
      "Converted 50_exploration.LSH.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted reproducibility.report_1_reproducibility_summary.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
