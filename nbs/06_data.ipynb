{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Imports for data functions\n",
    "import pandas as pd\n",
    "import os\n",
    "from reformer_fastai.tokenizers import ByteTextTokenizer\n",
    "from fastai.text.all import *\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def read_lines(path):\n",
    "    \"\"\"\n",
    "    Tokenizes a text file.\n",
    "    \"\"\"\n",
    "    assert os.path.exists(path)\n",
    "    lines = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(line)  # + ['<eos>'])\n",
    "    return lines\n",
    "\n",
    "\n",
    "def convert_data_to_seq_length(df, seq_length=2**16):\n",
    "    \"\"\"\n",
    "    Take a dataframe text data and convert it to a dataframe with the same columns where\n",
    "    every data sample is of numericalized token length of seq_length, except for the last example which is the remainder.\n",
    "    (less than but closest to the value given)\n",
    "    :param df: a pandas dataframe with columns [tokenized, lens] consisting of the numericalized tokens of text and their respective lengths\n",
    "    :param seq_length: the numericalized token sequence length to split the data into\n",
    "    :return: the new dataframe with split data samples\n",
    "    \"\"\"\n",
    "    concat_data = to_concat(list(df['tokenized']))\n",
    "    result = pd.DataFrame(columns=['tokenized', 'lens'])\n",
    "    n_seqs = len(concat_data)//seq_length\n",
    "    for i in tqdm(range(n_seqs), desc=\"Splitting data\", total=n_seqs):\n",
    "        sample = concat_data[i*seq_length:(i+1)*seq_length]\n",
    "        result = result.append(\n",
    "            {\n",
    "                'tokenized': sample,\n",
    "                'lens': len(sample),\n",
    "            },\n",
    "            ignore_index=True)\n",
    "    # Add last data sample which is the remainder\n",
    "    sample = concat_data[n_seqs*seq_length:]\n",
    "    if len(sample) > 0:\n",
    "        result = result.append(\n",
    "        {\n",
    "            'tokenized': sample,\n",
    "            'lens': len(sample),\n",
    "        },\n",
    "        ignore_index=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_and_prepare_data(data_path, seq_length=0):\n",
    "    \"\"\"\n",
    "    Read the data from file, and prepare the dataframe.\n",
    "    This does not include splitting into train and validation sets.\n",
    "    :param data_path: relative path to the raw data\n",
    "    :param seq_length: sequence length to split data into, default is don't change data sample length\n",
    "    :return: the dataframe after preparations\n",
    "    \"\"\"\n",
    "    print(\"Reading data from path...\")\n",
    "    # Read the data from file\n",
    "    enwik8 = read_lines(data_path)\n",
    "    df = pd.DataFrame({'text': enwik8})\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    time.sleep(0.5)  # this is so the printing of the progress bar is not weird\n",
    "    # Initialize the BTT\n",
    "    btt = ByteTextTokenizer(is_lm=True, add_bos=True, add_eos=True)\n",
    "\n",
    "    # Modify dataset for training\n",
    "    tqdm.pandas(desc=\"Tokenizing data\")\n",
    "    df['tokenized'] = df['text'].progress_map(lambda x: btt(x))\n",
    "    \n",
    "    # By default we won't change the data sample length\n",
    "    if seq_length != 0:\n",
    "        print(\"Sequence length has been added, splitting data to samples with sequence length \" + str(seq_length))\n",
    "        # Convert data samples according to sequence length\n",
    "        df = convert_data_to_seq_length(df, seq_length)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        df['lens'] = df['text'].map(lambda x: len(x))\n",
    "\n",
    "    df['lens_cum_sum'] = df.lens.cumsum()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/NTU/hgi/miniconda3/envs/fastai/lib/python3.8/site-packages/tqdm/std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "tokenizing data: 100%|██████████| 1/1 [00:00<00:00, 1217.86it/s]\n",
      "Splitting data: 100%|██████████| 7/7 [00:00<00:00, 376.76it/s]\n",
      "Splitting data: 100%|██████████| 2/2 [00:00<00:00, 363.95it/s]\n"
     ]
    }
   ],
   "source": [
    "test_text = 'hello world!'\n",
    "test_df = pd.DataFrame({'text': [test_text]})\n",
    "btt = ByteTextTokenizer(is_lm=True, add_bos=True, add_eos=True)\n",
    "tokenized_test_text = btt(test_text)\n",
    "assert len(test_df) == 1\n",
    "assert len(test_df['text'][0]) == len(test_text)\n",
    "\n",
    "tqdm.pandas(desc=\"tokenizing data\")\n",
    "test_df['tokenized'] = test_df['text'].progress_map(lambda x: btt(x))\n",
    "\n",
    "# Split the df into a divisable length (2)\n",
    "converted_test_df = convert_data_to_seq_length(test_df, 2)\n",
    "assert len(converted_test_df) == len(tokenized_test_text)//2\n",
    "\n",
    "# Split the df into a non-divisable length (5)\n",
    "converted_test_df = convert_data_to_seq_length(test_df, 5)\n",
    "assert len(converted_test_df) != len(tokenized_test_text)//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMTensorText([  2, 107, 104, 111, 111, 114,  35, 122, 114, 117, 111, 103,  36,   1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['tokenized'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data utils for the synthetic task of the reformer paper. We want to create sequences of the form 0w0w, where w is a sequence of integeres between 1-127 of some lenght: eg. 08470847.\n",
    "We create items on the fly instead of all items up front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TwinSequence(Dataset):\n",
    "    def __init__(self, sl=1024, len=100):\n",
    "        assert sl%2 == 0\n",
    "        self.sl = sl\n",
    "        self.len = len\n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.randint(1,128,(self.sl//2,))             # w: [1-127] of len sl//2\n",
    "        seq[0] = 0                                           # seq = 0w\n",
    "        seq = torch.cat((seq,seq), -1)                       # seq = 0w0w\n",
    "        target = torch.cat((seq[1:],torch.tensor([0])), -1)  # return offset target x:[0123], y:[1230]\n",
    "        return (seq, target)                     \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "#export\n",
    "def get_twinseq_dls(sl, train_sz=500, valid_sz=100, bs=64, shuffle=False, device='cuda'):\n",
    "    return DataLoaders.from_dsets(TwinSequence(sl, train_sz), \n",
    "                                  TwinSequence(sl, valid_sz), \n",
    "                                  bs=bs, shuffle=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]), torch.Size([64, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls=get_twinseq_dls(sl=10, device='cpu')\n",
    "xb, yb = dls.one_batch()\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 24, 52, 68, 118, 0, 24, 52, 68, 118],\n",
       " [24, 52, 68, 118, 0, 24, 52, 68, 118, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp, targ = xb[0].tolist(), yb[0].tolist()\n",
    "inp, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_equal(inp[1:], targ[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the synthetic task we also have to mask the **first half** of the targets. The first part is just random integers, so it's impossible to learn anything from it. We set the tokens in the first part to a special index, -100, and later tell our lossfunction to ignore items with this value. This means that the only task the model can learn is to copy the first part of the input sequence. If we didn't mask the first part, it would be penalized for poor performance in the first part, and would try to find a compromise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskTargCallback(Callback):\n",
    "    def before_batch(self):\n",
    "        self.y[:, :self.dls.train_ds.sl//2] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
