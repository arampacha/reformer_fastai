{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Imports for data functions\n",
    "import pandas as pd\n",
    "import os\n",
    "from reformer_fastai.tokenizers import ByteTextTokenizer\n",
    "from fastai.text.all import to_concat\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def read_lines(path):\n",
    "    \"\"\"\n",
    "    Tokenizes a text file.\n",
    "    \"\"\"\n",
    "    assert os.path.exists(path)\n",
    "    lines = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(line)  # + ['<eos>'])\n",
    "    return lines\n",
    "\n",
    "\n",
    "def convert_data_to_seq_length(df, seq_length=2**16):\n",
    "    \"\"\"\n",
    "    Take a dataframe text data and convert it to a dataframe with the same columns where\n",
    "    every data sample is of numericalized token length of seq_length, except for the last example which is the remainder.\n",
    "    (less than but closest to the value given)\n",
    "    :param df: a pandas dataframe with columns [tokenized, lens] consisting of the numericalized tokens of text and their respective lengths\n",
    "    :param seq_length: the numericalized token sequence length to split the data into\n",
    "    :return: the new dataframe with split data samples\n",
    "    \"\"\"\n",
    "    concat_data = to_concat(list(df['tokenized']))\n",
    "    result = pd.DataFrame(columns=['tokenized', 'lens'])\n",
    "    n_seqs = len(concat_data)//seq_length\n",
    "    for i in tqdm(range(n_seqs), desc=\"Splitting data\", total=n_seqs):\n",
    "        sample = concat_data[i*seq_length:(i+1)*seq_length]\n",
    "        result = result.append(\n",
    "            {\n",
    "                'tokenized': sample,\n",
    "                'lens': len(sample),\n",
    "            },\n",
    "            ignore_index=True)\n",
    "    # Add last data sample which is the remainder\n",
    "    sample = concat_data[n_seqs*seq_length:]\n",
    "    if len(sample) > 0:\n",
    "        result = result.append(\n",
    "        {\n",
    "            'tokenized': sample,\n",
    "            'lens': len(sample),\n",
    "        },\n",
    "        ignore_index=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_and_prepare_data(data_path, seq_length=0):\n",
    "    \"\"\"\n",
    "    Read the data from file, and prepare the dataframe.\n",
    "    This does not include splitting into train and validation sets.\n",
    "    :param data_path: relative path to the raw data\n",
    "    :param seq_length: sequence length to split data into, default is don't change data sample length\n",
    "    :return: the dataframe after preparations\n",
    "    \"\"\"\n",
    "    print(\"Reading data from path...\")\n",
    "    # Read the data from file\n",
    "    enwik8 = read_lines(data_path)\n",
    "    df = pd.DataFrame({'text': enwik8})\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    time.sleep(0.5)  # this is so the printing of the progress bar is not weird\n",
    "    # Initialize the BTT\n",
    "    btt = ByteTextTokenizer(is_lm=True, add_bos=True, add_eos=True)\n",
    "\n",
    "    # Modify dataset for training\n",
    "    tqdm.pandas(desc=\"Tokenizing data\")\n",
    "    df['tokenized'] = df['text'].progress_map(lambda x: btt(x))\n",
    "    \n",
    "    # By default we won't change the data sample length\n",
    "    if seq_length != 0:\n",
    "        print(\"Sequence length has been added, splitting data to samples with sequence length \" + str(seq_length))\n",
    "        # Convert data samples according to sequence length\n",
    "        df = convert_data_to_seq_length(df, seq_length)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        df['lens'] = df['text'].map(lambda x: len(x))\n",
    "\n",
    "    df['lens_cum_sum'] = df.lens.cumsum()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing data: 100%|██████████| 1/1 [00:00<00:00, 1047.01it/s]\n",
      "Splitting data: 100%|██████████| 7/7 [00:00<00:00, 245.79it/s]\n",
      "Splitting data: 100%|██████████| 2/2 [00:00<00:00, 206.42it/s]\n"
     ]
    }
   ],
   "source": [
    "test_text = 'hello world!'\n",
    "test_df = pd.DataFrame({'text': [test_text]})\n",
    "btt = ByteTextTokenizer(is_lm=True, add_bos=True, add_eos=True)\n",
    "tokenized_test_text = btt(test_text)\n",
    "assert len(test_df) == 1\n",
    "assert len(test_df['text'][0]) == len(test_text)\n",
    "\n",
    "tqdm.pandas(desc=\"tokenizing data\")\n",
    "test_df['tokenized'] = test_df['text'].progress_map(lambda x: btt(x))\n",
    "\n",
    "# Split the df into a divisable length (2)\n",
    "converted_test_df = convert_data_to_seq_length(test_df, 2)\n",
    "assert len(converted_test_df) == len(tokenized_test_text)//2\n",
    "\n",
    "# Split the df into a non-divisable length (5)\n",
    "converted_test_df = convert_data_to_seq_length(test_df, 5)\n",
    "assert len(converted_test_df) != len(tokenized_test_text)//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMTensorText([  2, 107, 104, 111, 111, 114,  35, 122, 114, 117, 111, 103,  36,   1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['tokenized'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
