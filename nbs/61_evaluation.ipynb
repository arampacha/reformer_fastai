{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from reformer_fastai.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def print_eval(learn, res):\n",
    "    print('Evaluation results: ', '; '.join(f'{m.name}: {v:.2f}' for m,v in zip(learn.metrics, res[1:])), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-activation",
   "metadata": {},
   "source": [
    "# Evaluation of saved models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-compromise",
   "metadata": {},
   "source": [
    "## Evaluation on synthetic task\n",
    "\n",
    "See https://arampacha.github.io/reformer_fastai/experiment.synthetic-task.html for dataset and training details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reformer_fastai.expscript import get_twin_sequence_dataloaders, get_synthetic_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('fastai_community/reformer-fastai/run-wvb5g2re-model:v0', type='model')\n",
    "artifact_dir = artifact.download(root='./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_twin_sequence_dataloaders()\n",
    "config = SyntheticConfig(use_lsh=False)\n",
    "model = LSHLM.from_config(config)\n",
    "learn = get_synthetic_learner(dls, model, precision=0)\n",
    "learn.add_cb(MaskTargCallback());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = learn.dls.test_dl(DeterministicTwinSequence(1024, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-houston",
   "metadata": {},
   "source": [
    "Score of randomly initialized model as sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-magnitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: \n",
      "masked_accuracy: 0.01\n"
     ]
    }
   ],
   "source": [
    "res = learn.validate(dl=test_dl)\n",
    "print_eval(learn, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-stereo",
   "metadata": {},
   "source": [
    "After loading pretrained weights models achieves perfect accuracy on unseen data, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-agriculture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: \n",
      "masked_accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "learn.load('run-wvb5g2re-model', with_opt=False);\n",
    "res = learn.validate(dl=test_dl)\n",
    "print_eval(learn, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-suspension",
   "metadata": {},
   "source": [
    "## Evaluation on enwik8 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reformer_fastai.expscript import get_lm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data('http://mattmahoney.net/dc/enwik8.zip', dest='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':read_lines('data/enwik8')})\n",
    "\n",
    "btt = ByteTextTokenizer(is_lm=True, add_bos=False, add_eos=False)\n",
    "df['toks'] = df['text'].apply(btt)\n",
    "df['lens'] = df['toks'].apply(len)\n",
    "df['lens_cum_sum'] = df.lens.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = df.lens.sum() - 10_000_000  # 10M characters for val and test\n",
    "train_idxs = df.loc[df['lens_cum_sum'] < train_cutoff].index.values\n",
    "train_idxs = list(range(0, max(train_idxs)))\n",
    "\n",
    "remaining_idxs = len(df) - max(train_idxs)\n",
    "validation_idxs = list(range(max(train_idxs), max(train_idxs) + int(remaining_idxs/2)))\n",
    "test_idxs = list(range(max(validation_idxs), len(df)))\n",
    "\n",
    "splits = [train_idxs, validation_idxs]\n",
    "\n",
    "tfms = [attrgetter(\"text\"), btt]\n",
    "dsets = Datasets(df, [tfms], splits=splits, dl_type=LMDataLoader)\n",
    "dl_kwargs = [{'lens':df['lens'].values[train_idxs]},\n",
    "             {'val_lens':df['lens'].values[validation_idxs]}]\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "dls = dsets.dataloaders(bs=8, val_bs=24, seq_len=4096, dl_kwargs=dl_kwargs, shuffle_train=True, n_workers=n_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NHashesConfig(n_hashes=2, pad_idx=dls.byte_text_tokenizer.pad_token_id)\n",
    "model = LSHLM.from_config(config)\n",
    "learn = get_lm_learner(dls, model, opt_func=adafactor, precision=2)\n",
    "learn.add_cb(PadBatchCallback(bucket_size=config.bucket_size, val=config.pad_idx, y_val=config.pad_idx));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = learn.dls.test_dl(df.iloc[test_idxs, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-bloom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indiana Jones, &amp;quot;Obtainer of Rare [[Antiquities]],&amp;quot; is modeled after the strong-jawed heroes of the matinee [[serial]]s and [[pulp magazine]]s that Lucas and Spielberg enjoyed in their childhoods, such as the [[Republic Pictures]] [[serial]]s, and [[Doc Savage]].   The two friends first discussed the project while in [[Hawaii]] during the time of release of the first ''[[Star Wars]]'' film.  Spielberg told Lucas how he wanted to direct a [[James Bond]] film. Lucas responded that he had something better than that.\\n\\nSpielberg wanted Indiana to be a James Bond-like figure that got into difficult situations and worked his way out. Upon requests by Spielberg and Lucas the costume designer was given the task to make the character have a distinctive recognizable silhouette through the style of the hat (much like [[Dick Tracy]]). After examining many hats, the designers chose an urban version of the classic [[Australia]]n [[fedora (hat)|fedora]], the [[Akubra]]. The</td>\n",
       "      <td>ndiana Jones, &amp;quot;Obtainer of Rare [[Antiquities]],&amp;quot; is modeled after the strong-jawed heroes of the matinee [[serial]]s and [[pulp magazine]]s that Lucas and Spielberg enjoyed in their childhoods, such as the [[Republic Pictures]] [[serial]]s, and [[Doc Savage]].   The two friends first discussed the project while in [[Hawaii]] during the time of release of the first ''[[Star Wars]]'' film.  Spielberg told Lucas how he wanted to direct a [[James Bond]] film. Lucas responded that he had something better than that.\\n\\nSpielberg wanted Indiana to be a James Bond-like figure that got into difficult situations and worked his way out. Upon requests by Spielberg and Lucas the costume designer was given the task to make the character have a distinctive recognizable silhouette through the style of the hat (much like [[Dick Tracy]]). After examining many hats, the designers chose an urban version of the classic [[Australia]]n [[fedora (hat)|fedora]], the [[Akubra]]. The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eaders at the state level continue to emphasize the state's past economic base of manufacturing and farming.\\n\\n== Military installations ==\\nIndiana was formerly home to two major military installations, [[Grissom Air Force Base]] near Peru (reduced to reservist operations in 1994) and [[Fort Benjamin Harrison]] near Indianapolis, now largely reduced to reservist operations, though the [[Department of Defense]] continues to operate a large financial operation there.\\n\\nCurrent active installations include [[Air National Guard]] fighter units at [[Fort Wayne, Indiana|Fort Wayne]] and [[Terre Haute]] airports (to be consolidated at Fort Wayne under the 2005 BRAC proposal, with the Terre Haute facility remaining open as a non-flying installation), the [[Crane Naval Weapons Center]] in the southwest of the state and the Army's [[Newport Chemical Depot]], which is currently heavily involved in neutralizing dangerous chemical weapons stored there.\\n\\n== Demographics ==\\n{| class=&amp;quot;toccolours&amp;quot; align=&amp;quot;right&amp;quot; cellpadding=&amp;quot;4&amp;quot; cellspacing=&amp;quot;0&amp;quot; style=&amp;quot;margin:0 0 1em 1em; font-size: 95%;&amp;quot;\\n|-\\n! colspan=2 bgcolor=&amp;quot;#ccccff&amp;quot; align=&amp;quot;center&amp;quot;| Historical populations\\n|-\\n!</td>\n",
       "      <td>aders at the state level continue to emphasize the state's past economic base of manufacturing and farming.\\n\\n== Military installations ==\\nIndiana was formerly home to two major military installations, [[Grissom Air Force Base]] near Peru (reduced to reservist operations in 1994) and [[Fort Benjamin Harrison]] near Indianapolis, now largely reduced to reservist operations, though the [[Department of Defense]] continues to operate a large financial operation there.\\n\\nCurrent active installations include [[Air National Guard]] fighter units at [[Fort Wayne, Indiana|Fort Wayne]] and [[Terre Haute]] airports (to be consolidated at Fort Wayne under the 2005 BRAC proposal, with the Terre Haute facility remaining open as a non-flying installation), the [[Crane Naval Weapons Center]] in the southwest of the state and the Army's [[Newport Chemical Depot]], which is currently heavily involved in neutralizing dangerous chemical weapons stored there.\\n\\n== Demographics ==\\n{| class=&amp;quot;toccolours&amp;quot; align=&amp;quot;right&amp;quot; cellpadding=&amp;quot;4&amp;quot; cellspacing=&amp;quot;0&amp;quot; style=&amp;quot;margin:0 0 1em 1em; font-size: 95%;&amp;quot;\\n|-\\n! colspan=2 bgcolor=&amp;quot;#ccccff&amp;quot; align=&amp;quot;center&amp;quot;| Historical populations\\n|-\\n!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dl.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-winner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: \n",
      "accuracy: 0.05; perplexity: 22361.88; bpc: 14.45\n"
     ]
    }
   ],
   "source": [
    "res = learn.validate(dl=test_dl)\n",
    "print_eval(learn, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# run = wandb.init()\n",
    "artifact = run.use_artifact('fastai_community/reformer-fastai/3tbfvs77:v0', type='model')\n",
    "artifact_dir = artifact.download(root='./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('n_hashes_n_hashes-4_enwik8_sl-4096_bs-4_n_eps-10_seed-42_grad-accum-8__24_01_2021_15:12', with_opt=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-colombia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: \n",
      "accuracy: 0.70; perplexity: 2.76; bpc: 1.46\n"
     ]
    }
   ],
   "source": [
    "res = learn.validate(dl=test_dl)\n",
    "print_eval(learn, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-porcelain",
   "metadata": {},
   "source": [
    "It's possible to improve performance by increasing number of hashing rounds. In this case one can observe small improvement in perplaxity and BPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-selling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: \n",
      "accuracy: 0.70; perplexity: 2.73; bpc: 1.45\n"
     ]
    }
   ],
   "source": [
    "learn.model.n_hashes = 8\n",
    "res = learn.validate(dl=test_dl)\n",
    "print_eval(learn, res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
