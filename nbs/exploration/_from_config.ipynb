{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from fastcore.all import *\n",
    "from reformer_fastai.all import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigBase:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntConfig:\n",
    "    _d = {\n",
    "        'vocab_sz':128,\n",
    "        'd_model':256,\n",
    "        'n_layers':1,\n",
    "        'n_heads':4,\n",
    "        'd_ff':256,\n",
    "        'attn_dropout':0.0,\n",
    "        'ff_dropout':0.0,\n",
    "        'emb_dropout':0.0,\n",
    "        'bucket_size':64,\n",
    "        'max_seq_len':1024,\n",
    "        'random_state':123,\n",
    "        'use_lsh':True,\n",
    "        'n_hashes':4\n",
    "    }\n",
    "    \n",
    "    @delegates(LSHLM)\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            if k in self._d: self._d[k]=v\n",
    "            else: print(f'Parameter {key} is not accepted by LSHLM. Skipped')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = \"LSHLM config \\n\" + '-'*20\n",
    "        s += ''.join(f'\\n{k:16}{v}' for k,v in self._d.items())\n",
    "        return s\n",
    "    \n",
    "    def dict(self): return self._d\n",
    "    \n",
    "    def save(self, fn, add_tstmp=False):\n",
    "        if add_tstmp:\n",
    "            tstmp = time.strftime(\"_%d_%m_%Y_%H:%M\", time.gmtime())\n",
    "            fn += tstmp\n",
    "        save_pickle(fn, self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, fn):\n",
    "        return load_pickle(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSHLM config \n",
       "--------------------\n",
       "vocab_sz        128\n",
       "d_model         256\n",
       "n_layers        1\n",
       "n_heads         4\n",
       "d_ff            256\n",
       "attn_dropout    0.0\n",
       "ff_dropout      0.0\n",
       "emb_dropout     0.0\n",
       "bucket_size     64\n",
       "max_seq_len     1024\n",
       "random_state    123\n",
       "use_lsh         True\n",
       "n_hashes        4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = SyntConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_sz': 128,\n",
       " 'd_model': 256,\n",
       " 'n_layers': 1,\n",
       " 'n_heads': 4,\n",
       " 'd_ff': 256,\n",
       " 'attn_dropout': 0.0,\n",
       " 'ff_dropout': 0.0,\n",
       " 'emb_dropout': 0.0,\n",
       " 'bucket_size': 64,\n",
       " 'max_seq_len': 1024,\n",
       " 'random_state': 123,\n",
       " 'use_lsh': True,\n",
       " 'n_hashes': 4}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSHLM config \n",
       "--------------------\n",
       "vocab_sz        128\n",
       "d_model         256\n",
       "n_layers        1\n",
       "n_heads         4\n",
       "d_ff            256\n",
       "attn_dropout    0.0\n",
       "ff_dropout      0.0\n",
       "emb_dropout     0.0\n",
       "bucket_size     64\n",
       "max_seq_len     1024\n",
       "random_state    123\n",
       "use_lsh         True\n",
       "n_hashes        4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = SyntConfig.from_file('test')\n",
    "config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SyntheticConfig = {\n",
    "    'vocab_sz':128,\n",
    "    'd_model':256,\n",
    "    'n_layers':1,\n",
    "    'n_heads':4,\n",
    "    'd_ff':256,\n",
    "    'attn_dropout':0.0,\n",
    "    'ff_dropout':0.0,\n",
    "    'emb_dropout':0.0,\n",
    "    'bucket_size':64,\n",
    "    'max_seq_len':1024,\n",
    "    'random_state':123,\n",
    "    'use_lsh':True,\n",
    "    'n_hashes':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)\n",
    "def from_config(cls:LSHLM, config):\n",
    "    return cls(**config.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSHLM(\n",
       "  (emb): TransformerEmbedding(\n",
       "    (emb): Embedding(128, 256)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (pos_enc): AbsolutePositionalEmbedding(\n",
       "      (emb): Embedding(1024, 256)\n",
       "    )\n",
       "  )\n",
       "  (encoder): LSHEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSHEncoderBlock(\n",
       "        (attn): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): ReformerAttentionV2(\n",
       "              (in_proj): SharedQKAttnInProj(\n",
       "                (to_qk): Linear(in_features=256, out_features=256, bias=False)\n",
       "                (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              )\n",
       "              (lsh_attn): LSHAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (full_attn): ScaledDotProdAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): PostNorm(\n",
       "          (sublayer): Residual(\n",
       "            (sublayer): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSHLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
