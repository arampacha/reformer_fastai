## LM experiments
### ReversibleLM
* [ ] Compare prenorm vs postnorm
* [ ] Attention input projection may be done in multiple ways. The one used now worked best for standard transformer on en-ga dataset. Might be good idea to try out more configs here
* [ ] ...

## Translation experiments