{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.autograd.function import Function\n",
    "from torch.utils.checkpoint import get_device_states, set_device_states\n",
    "from functools import wraps\n",
    "\n",
    "from fastai.basics import *\n",
    "from reformer_fastai.core import *\n",
    "from reformer_fastai.attention import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits to @lucidrains https://github.com/lucidrains\n",
    "\n",
    "raw version to be added LSH attention and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Chunk(nn.Module):\n",
    "    def __init__(self, chunks, fn, dim = -1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.chunks = chunks\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.chunks == 1:\n",
    "            return self.fn(x, **kwargs)\n",
    "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
    "        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)\n",
    "\n",
    "class ChunkedFeedForward(nn.Module):\n",
    "    def __init__(self, d, d_ff=None, chunks=1, dropout=0., dim=-1):\n",
    "        super().__init__()\n",
    "        d_ff = default(d_ff, 4*d)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d),\n",
    "            nn.Dropout(dropout)\n",
    "            )\n",
    "        self.chunks = chunks\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        if self.chunks == 1:\n",
    "            return self.net(x)\n",
    "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
    "        return torch.cat([self.net(c) for c in chunks], dim = self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Deterministic(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper module to ensure determinism for backward pass\n",
    "    following example for saving and setting rng here \n",
    "    https://pytorch.org/docs/stable/_modules/torch/utils/checkpoint.html\n",
    "    \"\"\"\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.cpu_state = None\n",
    "        self.cuda_in_fwd = None\n",
    "        self.gpu_devices = None\n",
    "        self.gpu_states = None\n",
    "\n",
    "    def record_rng(self, *args):\n",
    "        self.cpu_state = torch.get_rng_state()\n",
    "        if torch.cuda._initialized:\n",
    "            self.cuda_in_fwd = True\n",
    "            self.gpu_devices, self.gpu_states = get_device_states(*args)\n",
    "\n",
    "    def forward(self, *args, record_rng = False, set_rng = False, **kwargs):\n",
    "        if record_rng:\n",
    "            self.record_rng(*args)\n",
    "\n",
    "        if not set_rng:\n",
    "            return self.net(*args, **kwargs)\n",
    "\n",
    "        rng_devices = []\n",
    "        if self.cuda_in_fwd:\n",
    "            rng_devices = self.gpu_devices\n",
    "\n",
    "        with torch.random.fork_rng(devices=rng_devices, enabled=True):\n",
    "            torch.set_rng_state(self.cpu_state)\n",
    "            if self.cuda_in_fwd:\n",
    "                set_device_states(self.gpu_devices, self.gpu_states)\n",
    "            return self.net(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# heavily inspired by https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py\n",
    "# once multi-GPU is confirmed working, refactor and send PR back to source\n",
    "class ReversibleBlock(nn.Module):\n",
    "    def __init__(self, f, g, depth=None, send_signal = False):\n",
    "        super().__init__()\n",
    "        self.f = Deterministic(f)\n",
    "        self.g = Deterministic(g)\n",
    "\n",
    "        self.depth = depth\n",
    "        self.send_signal = send_signal\n",
    "\n",
    "    def forward(self, x, f_args = {}, g_args = {}):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
    "        y1, y2 = None, None\n",
    "\n",
    "        if self.send_signal:\n",
    "            f_args['_reverse'] = g_args['_reverse'] = False\n",
    "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)\n",
    "            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)\n",
    "\n",
    "        return torch.cat([y1, y2], dim=2)\n",
    "\n",
    "    def backward_pass(self, y, dy, f_args = {}, g_args = {}):\n",
    "        y1, y2 = torch.chunk(y, 2, dim=2)\n",
    "        del y\n",
    "\n",
    "        dy1, dy2 = torch.chunk(dy, 2, dim=2)\n",
    "        del dy\n",
    "\n",
    "        if self.send_signal:\n",
    "            f_args['_reverse'] = g_args['_reverse'] = True\n",
    "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            y1.requires_grad = True\n",
    "            gy1 = self.g(y1, set_rng=True, **g_args)\n",
    "            torch.autograd.backward(gy1, dy2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x2 = y2 - gy1\n",
    "            del y2, gy1\n",
    "\n",
    "            dx1 = dy1 + y1.grad\n",
    "            del dy1\n",
    "            y1.grad = None\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            x2.requires_grad = True\n",
    "            fx2 = self.f(x2, set_rng=True, **f_args)\n",
    "            torch.autograd.backward(fx2, dx1, retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x1 = y1 - fx2\n",
    "            del y1, fx2\n",
    "\n",
    "            dx2 = dy2 + x2.grad\n",
    "            del dy2\n",
    "            x2.grad = None\n",
    "\n",
    "            x = torch.cat([x1, x2.detach()], dim=2)\n",
    "            dx = torch.cat([dx1, dx2], dim=2)\n",
    "        \n",
    "        return x, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IrreversibleBlock(nn.Module):\n",
    "    def __init__(self, f, g):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "    def forward(self, x, f_args={}, g_args={}):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
    "        y1 = x1 + self.f(x2, **f_args)\n",
    "        y2 = x2 + self.g(y1, **g_args)\n",
    "        return torch.cat([y1, y2], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _ReversibleFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, blocks, kwargs):\n",
    "        ctx.kwargs = kwargs\n",
    "        for block in blocks:\n",
    "            x = block(x, **kwargs)\n",
    "        ctx.y = x.detach()\n",
    "        ctx.blocks = blocks\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy):\n",
    "        y = ctx.y\n",
    "        kwargs = ctx.kwargs\n",
    "        for block in ctx.blocks[::-1]:\n",
    "            y, dy = block.backward_pass(y, dy, **kwargs)\n",
    "        return dy, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReversibleSequence(nn.Module):\n",
    "    def __init__(self, blocks, layer_dropout = 0., reverse_thres = 0, send_signal = False):\n",
    "        super().__init__()\n",
    "        self.layer_dropout = layer_dropout\n",
    "        self.reverse_thres = reverse_thres # uses revblocks if seq_len else irrev_blocks\n",
    "\n",
    "        self.blocks = nn.ModuleList([ReversibleBlock(f, g, depth, send_signal) for depth, (f, g) in enumerate(blocks)])\n",
    "        self.irrev_blocks = nn.ModuleList([IrreversibleBlock(f=f, g=g) for f, g in blocks])\n",
    "\n",
    "    def forward(self, x, arg_route = (True, True), **kwargs):\n",
    "        reverse = x.shape[1] > self.reverse_thres\n",
    "        blocks = self.blocks if reverse else self.irrev_blocks\n",
    "\n",
    "        if self.training and self.layer_dropout > 0:\n",
    "            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) < self.layer_dropout\n",
    "            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]\n",
    "            blocks = self.blocks[:1] if len(blocks) == 0 else blocks\n",
    "\n",
    "        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)\n",
    "        block_kwargs = {'f_args': f_args, 'g_args': g_args}\n",
    "\n",
    "        if not reverse:\n",
    "            for block in blocks:\n",
    "                x = block(x, **block_kwargs)\n",
    "            return x\n",
    "\n",
    "        return _ReversibleFunction.apply(x, blocks, block_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# mess for now; will clean up after LSHAttention args finalized\n",
    "class ReformerEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 depth, \n",
    "                 heads = 8, \n",
    "                 max_seq_len = 512,              \n",
    "                 d_head = None, \n",
    "                 bucket_size = 64, \n",
    "                 n_hashes = 8, \n",
    "                 ff_chunks = 100, \n",
    "                 attn_chunks = None, # ??\n",
    "                 causal = False, \n",
    "                 weight_tie = False, # ??\n",
    "                 attn_dropout = 0.,\n",
    "                 post_attn_dropout = 0.,\n",
    "                 lsh_dropout = 0., \n",
    "                 ff_dropout = 0.,  \n",
    "                 d_ff = None, \n",
    "                 layer_dropout = 0., \n",
    "                 lsh_attend_across_buckets = True, \n",
    "                 lsh_allow_duplicate_attention = True, \n",
    "                 random_rotations_per_head = False,                  \n",
    "                 use_full_attn = False, \n",
    "                 full_attn_thres = 0, \n",
    "                 reverse_thres = 0,  \n",
    "                 one_value_head = False, \n",
    "                 n_local_attn_heads = 0,\n",
    "                 prenorm=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.depth = depth\n",
    "\n",
    "        self.bucket_size = bucket_size\n",
    "        # self.full_attn_thres = full_attn_thres\n",
    "        \n",
    "        # use regular attention for now\n",
    "        get_attn = lambda: Attention(d_model, heads, causal=causal, dropout=attn_dropout)\n",
    "        # get_attn = lambda: LSHSelfAttention(d_model, heads, bucket_size, n_hashes, causal = causal, d_head = d_head, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)\n",
    "        # get_ff = lambda: Chunk(ff_chunks, FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout), dim = -2)\n",
    "        get_ff = lambda: ChunkedFeedForward(d_model, d_ff, chunks=ff_chunks, dropout=ff_dropout, dim=1)\n",
    "\n",
    "        blocks = []\n",
    "        #residual_fn_wrapper = ReZero if use_rezero else partial(PreNorm, norm_type, d_model)\n",
    "        norm_wrapper = PreNorm if prenorm else PostNorm\n",
    "        \n",
    "        for ind in range(depth):\n",
    "            layer_num = ind + 1\n",
    "            \n",
    "            attn = get_attn()\n",
    "            ff = get_ff()\n",
    "\n",
    "            f = norm_wrapper(d_model, attn)\n",
    "            g = norm_wrapper(d_model, ff)\n",
    "\n",
    "            blocks.append(nn.ModuleList([f, g]))\n",
    "        # send_signal is not implemented for now\n",
    "        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout=layer_dropout, reverse_thres=reverse_thres, send_signal=False)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.cat([x, x], dim = -1)\n",
    "        arg_route = (True, False)\n",
    "        # pdb.set_trace()\n",
    "        x = self.layers(x, arg_route = arg_route, **kwargs)\n",
    "        return torch.stack(x.chunk(2, dim=-1)).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReformerDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 depth = 6, \n",
    "                 heads = 8,  \n",
    "                 max_seq_len = 512,\n",
    "                 d_head = None, \n",
    "                 bucket_size = 64, \n",
    "                 n_hashes = 8, \n",
    "                 ff_chunks = 100, \n",
    "                 attn_chunks = None, # ??\n",
    "                 causal = False, \n",
    "                 weight_tie = False, # weight sharing option do we need to keep this?\n",
    "                 attn_dropout = 0.,\n",
    "                 post_attn_dropout = 0.,\n",
    "                 ff_dropout = 0.,  \n",
    "                 d_ff = None, \n",
    "                 layer_dropout = 0.,\n",
    "                 prenorm=True,\n",
    "                 reverse_thres = 0,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.depth = depth\n",
    "        \n",
    "        # use regular attention for now\n",
    "        get_attn = lambda: DecoderAttention(d_model, heads, causal=causal, dropout=attn_dropout)\n",
    "        get_ff = lambda: ChunkedFeedForward(d_model, d_ff, chunks=ff_chunks, dropout=ff_dropout, dim=1)\n",
    "        norm_wrapper = PreNorm if prenorm else PostNorm\n",
    "        blocks = []\n",
    "        for ind in range(depth):\n",
    "            layer_num = ind + 1\n",
    "            \n",
    "            f = norm_wrapper(d_model, get_attn())\n",
    "            g = norm_wrapper(d_model, get_ff())\n",
    "\n",
    "            blocks.append(nn.ModuleList([f, g]))\n",
    "        # send_signal is not implemented for now\n",
    "        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout=layer_dropout, reverse_thres=reverse_thres, send_signal=False)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.cat([x, x], dim = -1)\n",
    "        arg_route = (True, False)\n",
    "        # pdb.set_trace()\n",
    "        x = self.layers(x, arg_route = arg_route, **kwargs)\n",
    "        return torch.stack(x.chunk(2, dim=-1)).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReformerLM(nn.Module):#, TransformerLM):\n",
    "    \"\"\"\n",
    "    Reformer for language modelling\n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_sz,\n",
    "                 d_model, \n",
    "                 depth = 6,\n",
    "                 tie_weights = True,\n",
    "                 max_seq_len = 512, \n",
    "                 heads = 8, \n",
    "                 d_head = None, \n",
    "                 bucket_size = 64, \n",
    "                 n_hashes = 8, \n",
    "                 ff_chunks = 100, \n",
    "                 attn_chunks = None, # ??\n",
    "                 causal = True, \n",
    "                 weight_tie = False, # ??\n",
    "                 attn_dropout = 0.,\n",
    "                 post_attn_dropout = 0.,\n",
    "                 lsh_dropout = 0., \n",
    "                 ff_dropout = 0.,  \n",
    "                 d_ff = None, \n",
    "                 layer_dropout = 0., \n",
    "                 lsh_attend_across_buckets = True, \n",
    "                 lsh_allow_duplicate_attention = True, \n",
    "                 random_rotations_per_head = False,                  \n",
    "                 use_full_attn = False, \n",
    "                 full_attn_thres = 0, \n",
    "                 reverse_thres = 0,  \n",
    "                 one_value_head = False, \n",
    "                 n_local_attn_heads = 0,\n",
    "                 prenorm=True):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len=max_seq_len)\n",
    "        #temp line to mark we need to pass more args to encoder\n",
    "        kwargs = {}\n",
    "        self.encoder = ReformerEncoder(d_model, depth, max_seq_len=max_seq_len, causal=causal, reverse_thres=reverse_thres,\n",
    "                                        **kwargs)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReformerEncDec(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Transformer Encoder-Decoder model\n",
    "    Parameters:\n",
    "        * enc_vocab_sz: int - source vocab size\n",
    "        * dec_vocab_sz: int - target vocab size\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_enc_layers: int (default: 6) \n",
    "        * n_dec_layers: int (default: 6)\n",
    "        * heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * prenorm: bool - whether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - whether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, if pad_idx is provided, and no mask/context_mask are passed to \n",
    "                forward method will be used to generate padding masks\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * shared_emb: bool - if True encoder and decoder will use shared embedding layer\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * src - source input ids, shape [bs, src_sl]\n",
    "        * tgt - target input ids, shape [bs, tgt_sl]\n",
    "        * src_mask - optional boolean source mask, shape [bs, src_sl]\n",
    "        * tgt_mask - optional boolean target mask, shape [bs, tgt_sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, tgt_sl, tgt_vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 enc_vocab_sz, \n",
    "                 dec_vocab_sz, \n",
    "                 d_model, \n",
    "                 depth=6, \n",
    "                 heads=8, \n",
    "                 max_seq_len=512, \n",
    "                 pad_idx=None, \n",
    "                 tie_weights=True,                  \n",
    "                 emb_dropout=0.1,\n",
    "                 attn_dropout=0.1, \n",
    "                 ff_dropout=0.1,\n",
    "                 pos_enc='absolute', \n",
    "                 d_ff=None, \n",
    "                 prenorm=False, \n",
    "                 axial_shape=None, \n",
    "                 axial_emb_dims=None,\n",
    "                 comb_attn=False,\n",
    "                 reverse_thres=0):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.depth = depth\n",
    "        self.pad_idx = pad_idx\n",
    "        self.enc_emb = TransformerEmbedding(enc_vocab_sz, d_model, max_seq_len, dropout=emb_dropout,\n",
    "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        self.dec_emb = TransformerEmbedding(dec_vocab_sz, d_model, max_seq_len, dropout=emb_dropout,\n",
    "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        self.encoder = ReformerEncoder(d_model, depth, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm, reverse_thres=reverse_thres)\n",
    "        self.decoder = ReformerDecoder(d_model, depth, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm, reverse_thres=reverse_thres)\n",
    "        self.proj = nn.Linear(d_model, dec_vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.dec_emb.emb.weight\n",
    "\n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None):\n",
    "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "        tgt_mask = default(tgt_mask, self.get_padding_mask(tgt))\n",
    "        enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
    "        out = self.decoder(self.dec_emb(tgt), context=enc, mask=tgt_mask, context_mask=src_mask)\n",
    "        return self.proj(out)\n",
    "    def get_padding_mask(self, x):\n",
    "        if self.pad_idx is None: return None\n",
    "        return (x != self.pad_idx)\n",
    "    #TODO add beam search and refactor\n",
    "    @torch.no_grad()\n",
    "    def generate(self, src,\n",
    "                src_mask=None,\n",
    "                max_len=50,\n",
    "                temperature=1.,\n",
    "                method = 'top_k',\n",
    "                top_k = 20,\n",
    "                top_p = 0.9,\n",
    "                early_stopping=False,\n",
    "                bos_idx=2, # TODO change to match future usecases\n",
    "                eos_idx=None):\n",
    "        self.to(src.device) #TODO test for potential problems\n",
    "        self.eval()\n",
    "        thresh = top_k if method=='top_k' else top_p\n",
    "        sampler = _sampler[method]\n",
    "        src = expand_dim1(src)\n",
    "        bs = src.size(0)\n",
    "        inp = src.new_full((bs, 1), bos_idx) #start with bos tokens\n",
    "        pdb.set_trace()\n",
    "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "        enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
    "        out = inp\n",
    "        for _ in range(max_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "            dec = self.decoder(self.dec_emb(out), context=enc)\n",
    "            logits = self.proj(dec)[:, -1, :]\n",
    "            if method == 'greedy':\n",
    "                sample = sampler(logits)\n",
    "            else:\n",
    "                filtered_logits = sampler(logits)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "                sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "\n",
    "            if (early_stopping and \n",
    "                ((sample == eos_idx).all() or \n",
    "                (sample == self.pad_idx).all())):\n",
    "                break\n",
    "        #TODO mb output cleanup\n",
    "        return out\n",
    "    \n",
    "    def store_attention(self, layer_ids=None, store_encoder=False, store_decoder=True):\n",
    "        #defaults to storing attention for all layers\n",
    "        layer_ids = default(layer_ids, list(range(self.depth)))\n",
    "        for module in self.children():\n",
    "            if issubclass(type(module), TransformerEncoder) and store_encoder:\n",
    "                for i, l in enumerate(module.layers):\n",
    "                    if i in layer_ids:\n",
    "                        for m in l.modules():\n",
    "                            if issubclass(type(m), (Attention)):\n",
    "                                m.store_attention = True\n",
    "            elif issubclass(type(module), TransformerDecoder) and store_decoder:\n",
    "                for i, l in enumerate(module.layers):\n",
    "                    if i in layer_ids:\n",
    "                        for m in l.modules():\n",
    "                            if issubclass(type(m), (Attention)):\n",
    "                                m.store_attention = True\n",
    "    #TODO mb separate encoder and decoder attention\n",
    "    def get_attention_matrix(self, get_encoder=False, get_decoder=True):\n",
    "        res = []\n",
    "        if get_encoder:\n",
    "            for m in self.encoder.modules():\n",
    "                if issubclass(type(m), (Attention)):\n",
    "                    attention = getattr(m, 'attention', None)\n",
    "                    if attention is not None:\n",
    "                        res.append(attention)\n",
    "                    # reset stored attention\n",
    "                    m.attention = None\n",
    "                    m.store_attention = False\n",
    "        if get_decoder:\n",
    "            for m in self.decoder.modules():\n",
    "                if issubclass(type(m), (Attention)):\n",
    "                    attention = getattr(m, 'attention', None)\n",
    "                    if attention is not None:\n",
    "                        res.append(attention)\n",
    "                    # reset stored attention\n",
    "                    m.attention = None\n",
    "                    m.store_attention = False\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_attention.ipynb.\n",
      "Converted 02_transformer.ipynb.\n",
      "Converted 03_reformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv] *",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
